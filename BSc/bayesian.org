#+TITLE:     Bayesian Inference and Hypothesis Testing
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \input{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:2
#+OPTIONS: toc:nil

* Introduction
** Probability refresher
*** Col A                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
  \begin{tikzpicture}
    \node<2->[someset, minimum size=2cm, fill=green, opacity=0.5] at (0,0) (Recovery) [label=Recovery] {$A_1$};
    \node<3->[someset, minimum size=3cm, fill=red, opacity=0.5] at (2,0) (Side effects) [label=Side effects] {$A_2$};
    \node[someset, minimum size=5cm] at (1,0) (Everything) [label=below:Everything ($\Outcomes$)] {};
    \node<4-> at (1.5,0.5) (omega) {$\outcome$};
    \node<4-> at (4,3) (patient) {Patient state};
    \draw<4->[->, bend left=45] (patient) -- (omega);
  \end{tikzpicture}
*** Col B                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
**** Notation
#+ATTR_BEAMER: :overlay <+->
- $\omega \in \Omega$ random outcome
- $P(A)$: Probability that $\omega$ is in $A$.
- $P(A_2) > P(A_1)$ means $A_2$ is more likely than $A_1$ according to $P$.
#+BEAMER: \pause
**** Axioms
#+ATTR_BEAMER: :overlay <+->
- $P(A \geq 0)$ for any $A \subset \Omega$.
- $P(\Omega) = 1$.
- If $A_1 \cap A_2 = \emptyset$ then $P(A_1 \cup A_2) = P(A_1) + P(A_2)$.
  
** Mutually exclusive events
*** Col A                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

    \begin{tikzpicture}
      \node[someset, minimum size=1cm, fill=green, opacity=0.5] at (0,0) (Recovery) [label=Recovery] {$A_1$};
      \node[someset, minimum size=2cm, fill=red, opacity=0.5] at (2,0) (Side effects) [label=Side effects] {$A_2$};
      \node[someset, minimum size=4cm] at (1,0) (Everything) [label=below:Everything ($\Outcomes$)] {};
    \end{tikzpicture}
*** Col B                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
- A_1 \cap A_2 = \emptyset
- Consequently, $P(A_1 \cup A_2) = P(A_1) + P(A_2)$
- So the probability of $A_1$ *or* $A_2$ equals the probability of $A_1$  *plus* the probability of $A_2$.
#+BEAMER: \pause
**** Which of the following is true?
1. If $A_2$ happens then $A_1$ happens.
2. If $A_1$ happens then $A_2$ happens.
3. $A_1$ and $A_2$ can happen at the same time.
4. $A_1$ and $A_2$ cannot happen at the same time.
#+BEAMER: \pause
- (4) is correct.


** Implication
*** Col A                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

    \begin{tikzpicture}
      \node[someset, minimum size=1cm, fill=green, opacity=0.5] at (1.5,-0.5) (Recovery) [label=Recovery] {$A_1$};
      \node[someset, minimum size=3cm, fill=red, opacity=0.5] at (1,0) (Side effects) [label=Side effects] {$A_2$};
      \node[someset, minimum size=4cm] at (1,0) (Everything) [label=below:Everything ($\Outcomes$)] {};
    \end{tikzpicture}
*** Col B                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
- $A_1 \subset A_2$.
**** Which of the following is true?
1. If $A_2$ happens then $A_1$ happens.
2. If $A_1$ happens then $A_2$ happens.
3. $A_1$ and $A_2$ can happen at the same time.
4. $A_1$ and $A_2$ cannot happen at the same time.
#+BEAMER: \pause
- (1,3) are correct

** Marginalisation
*** Col A                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
  \begin{tikzpicture}
    \begin{scope}
      \draw[opacity=0.5, fill=gray, preaction={draw, thick, double
        distance=0pt}] plot[smooth cycle] coordinates{ (0,2) (2.5,2.5)
        (3,4) (4,3) (4,-1) (0,0) };
      
      \draw[thick, draw=black, fill=red, opacity=0.5] (2,1) -- (2,3)
      -- (-2,1) -- cycle; \draw[thick, draw=black, fill=blue,
      opacity=0.5] (2,1) -- (2,-2) -- (-2,1) -- cycle; \draw[thick,
      draw=black, fill=yellow, opacity=0.5] (2,1) -- (2,-2) -- (5,1)
      -- cycle; \draw[thick, draw=black, fill=green, opacity=0.5]
      (2,1) -- (2,3) -- (5,1) -- cycle;
    \end{scope}
    \node at (1,2) {$B_1$}; \node at (3,2) {$B_2$}; \node at (3,0)
    {$B_3$}; \node at (1,0) {$B_4$};
  \end{tikzpicture}
*** Col B                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
\begin{align*}
  P(B) &= P\left(\bigcup_i B_i\right) = \sum_i P(B_i) \\
  P(A \cap B) &= P\left(\bigcup_i (A \cap B_i)\right)\\
& = \sum_i P(A \cap B_i),
\end{align*}

* Conditional Probability and the Theorem of Bayes
#+TOC: headlines
** Bayes theorem                                         :theory:probability:
#+ATTR_BEAMER: :overlay <+->
- Recall the definition of Conditional probability:
 \[
 P(A | B) = P(A \cap B) / P(B)
 \]
 i.e. the probability of A happening if B happens.
- It is also true that:
 \[
 P(B | A) = P(A \cap B) / P(A)
 \]
- Combining the two equations, reverse the conditioning:
 \[
 P(A | B) = P(B | A) P (A) / P(B)
 \]

- So we can reverse the order of conditioning, i.e. relate to the probability of A given B to that of B given A.

** The cards problem
 1. Print out a number of cards, with either [A|A], [A|B] or [B|B] on their sides.
 2. If you have an A, what is the probability of an A on the other side?
 3. Have the students perform the experiment with:
    1. Draw a random card.
    2. Count the number of people with A.
    3. What is the probability that somebody with an A on one side will have an A on the other?
    4. Half of the people should have an A?
#+BEAMER: \pause

*** The prior and posterior probabilities
	| A | A | 2/6 | A observed | 2/3
	| A | B | 1/6 | A observed | 1/3
	| B | A | 1/6 |            |
	| B | B | 2/6 |            |

* Simple Bayesian hypothesis testing
#+TOC: headlines
** The murder problem
#+ATTR_BEAMER: :overlay <+->
-  A murder occurred in a house over Christmas. There were $n$ people inside, plus the victim. Person X, the victim's son, is accused of a murder. 

- There are two possibilities:
       - $H_0$: They are innocent.
       - $H_1$: They are guilty.
#+BEAMER: \pause	
       What is your belief that they have committed the crime? 
#+BEAMER: \pause	
*** Prior elicitation
#+ATTR_BEAMER: :overlay <+->
- All those that think the accused is guilty, raise their hand.
- Divide by the number of people in class
- Let us call this $P(H_1)$.
- This is a purely subjective measure!

** DNA test

 - Let us now do a DNA test on the suspect
#+BEAMER: \pause

*** DNA test properties
 #+ATTR_BEAMER: :overlay <+->
 - $D$: Test is positive
 - $P(D | H_0) = 10\%$: False positive rate
 - $P(D | H_1) = 100\%$: True positive rate

#+BEAMER: \pause

*** Run the test
#+ATTR_BEAMER: :overlay <+->
- The result is either positive or negative ($\neg D)$.
- What is your belief *now* that the suspect is guilty?

** Everybody is a suspect
       #+ATTR_BEAMER: :overlay <+->
- Run a DNA test on everybody in the house.
- What is different from before?
- Who has a positive test?
- What is your belief that the people with the positive test are guilty?

** Explanation
- *Prior*: $P(H_i)$. How much do we believe in $H_i$.
- *Likelihood* $P(D | H_i)$. How likely is $D$ if $H_i$ is true.
#+BEAMER: \pause
- *Posterior* $P(H_i | D)$: How likely is $H_i$ given the data
  \[
  P(H_i | D)
  =
  \frac{P(D \cap H_i)}{P(D)}
  = \frac{P(D | H_i) P(H_i)}{P(D)}
  \]
#+BEAMER: \pause
- Obtaining the *marginal* probability: \[ P(D)
 = P(D \cap H_0) + P(D \cap H_1) & = P(D | H_0) P(H_0) + P(D | H_1) P(H_1) \]

- Posterior: \[P(H_0 | D) = \frac{P(D | H_0) P(H_0)}{P(D | H_0) P(H_0) + P(D | H_1) P(H_1)}\]
- Assuming $P(D | H_1) = 1$, and setting $P(H_0) = q$, this gives
       \[
       P(H_0 | D) = \frac{0.1 q}{0.1 q + 1 - q} =  \frac{q}{10 - 9q}
       \]
- The posterior can always be updated with more data!
** Python example

#+BEGIN_SRC python
  def get_posterior(prior, data, likelihood):
	  marginal = prior*likelihood[data][0]
	   + (1 - prior)*likelihood[data][1]
	  posterior = prior*likelihood[data][0] / marginal
	  return posterior

  import numpy as np
  prior = 0.9 # Pr(H1)
  likelihood = np.zeros([2, 2])
  likelihood[0][0] = 0.9 # Pr(F|H0)
  likelihood[1][0] = 0.1 # Pr(T|H0)
  likelihood[0][1] = 0 # Pr(F|H1)
  likelihood[1][1] = 1 # Pr(T|H1)
  data = 1
  return get_posterior(prior, data, likelihood)
#+END_SRC

#+RESULTS:
: 0.4736842105263158


** Types of hypothesis testing problems
#+ATTR_BEAMER: :overlay <+->
*** Simple Hypothesis Test
#+ATTR_BEAMER: :overlay <+->
Example: DNA evidence, Covid tests
- Two hypothesese $H_0, H_1$
- $P(D | H_i)$ is defined for all $i$

*** Multiple Hypotheses Test
#+ATTR_BEAMER: :overlay <+->
Example: Model selection
- $H_i$: One of many mutually exclusive models
- $P(D | H_i)$ is defined for all $i$

*** Null Hypothesis Test
#+ATTR_BEAMER: :overlay <+->
Example: Are men's and women's heights the same?
- $H_0$: The 'null' hypothesis
- $P(D | H_0)$ is defined
- The alternative is *undefined*

** Pitfalls
#+ATTR_BEAMER: :overlay <+->

*** Problem definition
#+ATTR_BEAMER: :overlay <+->
- Defining the models $P(D | H_i)$ incorrectly.
- Using an "unreasonable" prior $P(H_i)$
#+BEAMER: \pause
*** The garden of many paths
#+ATTR_BEAMER: :overlay <+->
- Having a huge hypothesis space
- Selecting the relevant hypothesis after seeing the data

* Bayesian Inference
** Probabilistic models
#+ATTR_BEAMER: :overlay <+->
- Model family $\{P_\param |  \param \in \Param\}$
- Data $x \sim P_{\param^*}$ for some $\param^* \in \Param$.
- How can we estimate the correct $\param$?
- How can we predict a new data point?
#+BEAMER: \pause
*** Bernoulli model :B_example:
    :PROPERTIES:
    :BEAMER_env: example
    :END:
**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
#+ATTR_BEAMER: :overlay <+->
- $x \in \{0,1\}$, $\param \in [0,1]$
- $x \mid \param \sim \Ber(\param)$
- $P_\param(1) = \param$
- $P_\param(0) = 1 - \param$.
**** Col B                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
\begin{tikzpicture}
\node[RV] at (1,0) (x) {$x_t$};
\node[RV,hidden] at (0,0) (mean) {$\theta$};
\draw[->] (mean) to (x);
\end{tikzpicture}
#+BEAMER: \pause
*** Gaussian model                                                :B_example:
    :PROPERTIES:
    :BEAMER_env: example
    :END:
**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
#+ATTR_BEAMER: :overlay <+->
- $x \in \Reals$, $\mu \in \Reals, \sigma \in \Reals_+$
- $x \mid \mu, \sigma \sim \Normal(\mu, \sigma)$
- $p_\param(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)$
**** Col B                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
\begin{tikzpicture}
\node[RV] at (1,0) (x) {$x_t$};
\node[RV,hidden] at (0,0) (mean) {$\mu$};
\node[RV,hidden] at (0,1) (std) {$\sigma$};
\draw[->] (mean) to (x);
\draw[->] (std) to (x);
\end{tikzpicture}


** Maximum likelihood (ML) inference
- Family $\{P_\param |  \param \in \Param\}$
- Data $x$ with *likelihood* $P_\param(x)$ for each parameter value $\param$.
- $\param_{\textrm{ML}}(x) = \argmax_\param P_\param(x)$
*** Bernoulli model :B_example:
    :PROPERTIES:
    :BEAMER_env: example
    :END:
#+ATTR_BEAMER: :overlay <+->
- $x_t \in \{0,1\}$, for $t \in [T]$, $\param \in [0,1]$
- $x_t \mid \param \sim \Ber(\param)$
- $P_\param(x_1, \ldots, x_T)  = \prod_{t=1}^T P_\param(x_t)$
- What maximises the likelihood?
- Define $s_T = \sum_{t=1}^T x_t$.
- Show that $\param_{\textrm{ML}}(x) = s_T / T$.
- What is the problem with this estimate?


** Maximum a posteriori (MAP) inference
- Family $\{P_\param |  \param \in \Param\}$
- Data $x$ with *likelihood* $P_\param(x)$ for each parameter value $\param$.
- *Prior* $\bel(\param)$.
- $\param_{\textrm{MAP}}(x) = \argmax_\param P_\param(x) \bel(\param)$
- Experiment with the prior for the Bernoulli model.


** Bayesian Inference
#+ATTR_BEAMER: :overlay <+->
- Model family $\{P_\param |  \param \in \Param\}$
- Each model $\param$ assigns probabilities $P_\param(x)$  to possible $x \in X$.
- We also have a (subjective) prior distribution $\bel$ over the parameters.
- Given $x$, we calculate the posterior distribution
#+BEAMER: \pause
\begin{align}
\bel(\param | x)
& = \frac{P_\param(x) \bel(\param)}{\sum_{\param' \in \Param} P_{\param'}(x) \bel(\param')},
\tag{finite $\Param$, $\bel$ is a probability}
\\
\dbel(\param | x)
& = \frac{P_\param(x) \dbel(\param)}{\int_{\Param} P_{\param'}(x) \dbel(\param') d\param'},
\tag{continuous $\Param$, $\dbel$ is a density}
\\
\bel(B | x)
& = \frac{\int_{B} P_{\param'}(x) d\bel(\param)}
{\int_{\Param}P_{\param'}(x) d\bel(\param)},
&& B \subset \Param
\tag{arbitrary $\Param$, $\bel$ is a measure}
\end{align}
#+BEAMER: \pause
*** Alternative notation for different probability spaces
- The *prior* $\bel(\param) = \Pr(\param)$ and *posterior* $\bel(\param \mid x) = \Pr(\param \mid x)$ belief.
- The *likelihood* $P_\param(x) = \Pr(x \mid \param)$
- The *marginal* $\Pr_\bel(x) = \sum_\param P_\param(x) \bel(\param)$.
** Probabilistic machine learning
#+ATTR_BEAMER: :overlay <+->
- Model family $\{P_\param |  \param \in \Param\}$
- Prior $\bel$ on $\Param$
- Observations $x = x_1, \ldots, x_t$.
#+BEAMER: \pause
*** Maximum likelihood approach
- Model selection: $\param^*_{ML}(x) = \argmax_\param P_\param(x)$.
- Model prediction: $P_{\param^*_{ML}(x)}(x_{t+1})$ 
#+BEAMER: \pause
*** Maximum a posteriori approach
- Model selection: $\param^*_{MAP}(x) = \argmax_\param P_\param(x) \bel(\param)$.
- Model prediction: $P_{\param^*_{MAP}(x)}(x_{t+1})$ 
#+BEAMER: \pause
*** Bayesian approach
- Posterior calculation: $\bel(\param | x) = P_\param(x) \bel(\param) / \Pr_\bel(x)$
- Model prediction: $\Pr_\bel(x_{t+1} | x) = \sum_\param P_\param(x_{t+1}) \bel(\param | x)$ 
** Differences between approaches
*** Maximum likelihood approach
- Ignores model complexity
- Is an optimisation problem
*** Maximum a posteriori approach
- Regularises model selection using the prior
- Can be seen as solving the optimisation problem
  \[
  \max_\param \ln P_\param(x) + \ln \bel(\param),
  \]
  where the prior term $\ln \bel(\param)$ acts as a regulariser.
*** Bayesian approach
- Does not select a single model
- Averages over all models according to their fit *and* the prior
- Does *not* result in an optimisation problem.


** The n-meteorologists problem  
- Consider $n$ meteorological stations $\{\mu\}$ predicting rainfall.
- $x_t \in \{0,1\}$ with $x_t = 1$ if it rains on day $t$.
- We have a prior distribution $\bel(\mu)$ for each station.
- At time $t$, station $\mu$ makes as a prediction $P_\mu(x_{t+1} | x_1, \ldots, x_t)$
- We observe $x_{t+1}$ and calculate the posterior  $\bel(\mu | x_1, \ldots, x_t, x_{t+1})$.
*** The marginal distribution 
To take into account all stations, we can marginalise:
\[
\Pr_\bel(x_{t+1} \mid x_1, \ldots x_t) = 
\sum_\mu P_\mu(x_{t+1} | x_t) \bel(\mu)
\]
*** The posterior :exercise:
- Show that
\[
\bel(\mu \mid x_1, \ldots, x_{t+1}) = 
\frac{P_\mu(x_t \mid x_1, \ldots, x_t) \bel(\mu|x_1, \ldots, x_t)}
{\sum_{\mu'} P_{\mu'}(x_t \mid x_1, \ldots, x_t) \bel(\mu'|x_1, \ldots, x_t)}
\]
- How would you implement an ML or a MAP solution to this problem?

** Sufficient statistics
*** A statistic $f$
This is any function $f : X \to S$ where
- $X$ is the data space
- $S$ is an arbitrary space
*** Example statistics for $X = \Reals^*$ (the set of all real-valued sequences)
- The sample mean of a sequence $1/T \sum_{t=1}^T x_t$
- The total number of samples $T$
*** Sufficient statistic
$f$ is sufficient for a family $\{P_\param : \param \in \Param\}$ when
\[
f(x) = f(x') \Rightarrow P_\param(x) = P_\param(x') \forall \param \in \Param.
\]
If there exists a finite-dimensional sufficient statistic, Bayesian and ML learning can be done in closed form within the family.
** Conjugate priors
Consider a parametrised family of priors $\Bel$ on $\Param$ and a distribution family $\{P_\param\}$
The pair is conjugate if, for any prior $\bel \in \Bel$, and any observation $x$, there exists $\bel' \in \Bel$ such that $\bel'(\param) = \bel(\param | x)$
*** Standard Parametric conjugate families
|---------------+------------+---------------------------------+-----------------------|
| Prior         | Likelihood | Parameters $\param$             | Observations $x$      |
|---------------+------------+---------------------------------+-----------------------|
| Beta         | Bernoulli  | $[0,1]$                         | $\{0,1\}^T$           |
| Multinomial  | Dirichlet  | $\Simplex^n$                    | $\{1, \ldots, n\}^T$  |
| Gamma        | Normal     | $\Reals, \Reals$                | $\Reals^T$            |
| Wishart      | Normal     | $\Reals^n, \Reals^{n \times n}$ | $\Reals^{n \times T}$ |
|---------------+------------+---------------------------------+-----------------------|

The Simplex $\Simplex^n = \{\vparam \in [0,1]^n : \|\vparam\|_1\}$ is the set of all \(n\)-dimensional probability vectors.

*** Extensions
- Discrete Bayesian Networks.
- Linear-Gaussian Models (i.e. Bayesian linear regression)
- Gaussian Processes.

** Beta-Bernoulli
\begin{tikzpicture}
\node[RV] at (1,0) (x) {$x_t$};
\node[RV,hidden] at (0,0) (mean) {$\theta$};
\node[RV] at (-1,0) (prior) {$\vectorsym{\alpha}$};
\draw[->] (prior) to (mean);
\draw[->] (mean) to (x);
\end{tikzpicture}

*** Definition of the Bernoulli distribution
If $x_t \mid \param \sim \Ber(\param)$. $\param \in [0,1]$, $x_t \in \{0, 1\}$ and:
\[
P_\param(x_t = 1) = \param
\]
*** Definition of the Beta density 
If $\param \sim \Beta(\alpha_1, \alpha_0)$, $\alpha_0, \alpha_1 > 0$ and
\[
p(\param | \alpha_1, \alpha_0) \propto \param^{\alpha_1 - 1} (1 - \param)^{\alpha_0 - 1}
\]
*** Beta-Bernoulli conjugate pair
- $\param \sim \Beta(\alpha_1, \alpha_0)$.
- $x_t \mid \param \sim \Ber(\param)$.
Then, for any $x = x_1, \ldots, x_T$, the posterior distribution is
- $\param \mid x \sim \Beta(\alpha_1 + \sum_t x_t , \alpha_0 + T - \sum_t x_t)$.
** Dirichlet-Multinomial
\begin{tikzpicture}
\node[RV] at (1,0) (x) {$x_t$};
\node[RV,hidden] at (0,0) (mean) {$\vparam$};
\draw[->] (mean) to (x);
\node[RV] at (-1,0) (prior) {$\vectorsym{\alpha}$};
\draw[->] (prior) to (mean);
\end{tikzpicture}
*** Definition of the Multinomial distribution
If $x_t \mid \vparam \sim \Mult(\vparam)$,
with $\param \in \Simplex^n$ and $x_t \in \{1, \ldots, n\}$ and:
\[
P_\vparam(x_t = i) = \param_i
\]
*** Definition of the Dirichlet density 
If $\vparam \sim \Dir(\vectorsym{\alpha})$, with $\vectorsym{\alpha} \in \Reals^n_+$ then
\[
p(\param | \vectorsym{\alpha}) \propto \prod_i \param_i^{\alpha_i - 1}
\]
*** Dirichlet-Multinomial conjugate pair
- $\param \sim \Dir(\vectorsym{\alpha})$.
- $x_t \mid \param \sim \Ber(\vparam)$.
Then, for any $x = x_1, \ldots, x_T$, the posterior distribution is
- $\param \mid x \sim \Dir(\vectorsym{\alpha + \vectorsym{s}_T})$, where $s_{T,i} = \sum_{t=1}^T \ind{x_t = i}$,

** Discrete Bayesian Networks
\begin{tikzpicture}
\node[RV] at (0,0) (x1) {$x_1$};
\node[RV] at (0,1) (x2) {$x_2$};
\node[RV] at (1,0) (x3) {$x_3$};
\node[RV] at (1,1) (x4) {$x_4$};
\node[RV,hidden] at (-1,0) (m1) {$\vparam_1$};
\node[RV,hidden] at (-1,1) (m2) {$\vparam_2$};
\node[RV,hidden] at (2,0) (m3) {$\vparam_3$};
\node[RV,hidden] at (2,1) (m4) {$\vparam_4$};
\draw[->] (x1) to (x2);
\draw[->] (x2) to (x3);
\draw[->] (x4) to (x3);
\draw[->] (x2) to (x4);
\draw[->] (m1) to (x1);
\draw[->] (m2) to (x2);
\draw[->] (m3) to (x3);
\draw[->] (m4) to (x4);
\end{tikzpicture}

- A directed acyclic graph (DAG) defined on variables $x_1, \ldots, x_n$ with each $x_n$ taking a finite number of values,
- Let $S_i$ be the indices corresponding to parent variables of $x_i$.
- $x_i \mid \vparam_i, x_{S_i} = k \sim \Mult(\vparam_{i,k})$.

*** Example: Lung cancer, smoking and asbestos
**** LSA DAG
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:

\begin{tikzpicture}
\node[RV] at (0,0) (x1) {$x_S$};
\node[RV] at (0,1) (x2) {$x_C$};
\node[RV] at (1,0) (x3) {$x_A$};
\node[RV,hidden] at (-1,0) (m1) {$\param_A$};
\node[RV,hidden] at (-1,1) (m2) {$\vparam_C$};
\node[RV,hidden] at (2,0) (m3) {$\param_S$};
\draw[->] (x1) to (x2);
\draw[->] (x3) to (x2);
\draw[->] (m1) to (x1);
\draw[->] (m2) to (x2);
\draw[->] (m3) to (x3);
\end{tikzpicture}
**** LSA Equations
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
\begin{align}
P_{\param_A}(x_A = 1) &= \param_A\\
P_{\param_S}(x_S = 1) &= \param_S\\
P_{\vparam_C}(x_C = 1 \mid X_A= j, X_S = k) &= \param_{C,j,k}
\end{align}

** Markov model
\begin{tikzpicture}
\node[RV] at (-1,0) (x0) {$x_{t-1}$};
\node[RV] at (0,0) (x1) {$x_t$};
\node[RV] at (1,0) (x2) {$x_{t+1}$};
\node[RV,hidden] at (1,1) (m1) {$\vparam$};
\node[RV] at (0,1) (prior) {$\vectorsym{\alpha}$};
\draw[->] (prior) to (m1);
\draw[->] (m1) to (x0);
\draw[->] (m1) to (x1);
\draw[->] (m1) to (x2);
\draw[->] (x0) to (x1);
\draw[->] (x1) to (x2);
\end{tikzpicture}

A *Markov model* obeys
\[
\Pr_\vparam(x_{k+1} | x_k, \ldots, x_1) = \Pr_\vparam(x_{k+1} | x_k)
\]
i.e. the graphical model is a chain. We are usually interested in *homogeneous* models, where
\[
\Pr_\vparam(x_{k+1} = i \mid x_k = j) = \param_{i,j} \qquad \forall k
\]
*** Inference for finite Markov models
- If $x_t \in [n]$ then $x_{t+1} \mid \vparam, x_t = i \sim \Mult(\vparam_i)$, $\vparam_i \in \Simplex^n$
- Prior $\vparam_i \mid \vectorsym{\alpha} \sim \Dir(\vectorsym{\alpha})$ for all $i \in [n]$.
- Posterior $\vparam_i \mid x_1, \ldots, x_t, \vectorsym{\alpha} \sim \Dir(\vectorsym{\alpha}^{(t)})$ with
  \[
  \alpha^{t}_{i,j} = \alpha_{i,j} + \sum_{k=1}^t \ind{x_k = i \wedge x_{k+1} = j},
  \qquad
  \vectorsym{\alpha}^0 =   \vectorsym{\alpha}.
  \]

