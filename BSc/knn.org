#+TITLE: Nearest Neighbour Algorithms
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_CLASS_OPTIONS: [10pt]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}
* Introduction
** The hidden secret of machine learning
*** Supervised learning

- Given labelled training examples $(x_1, y_1), \ldots (x_T, y_T)$ where
- $x_t \in X$ are *features* 
- $y_t \in Y$ are *labels*..
**** Feature space $\CX$
- Usually $\CX = \Reals^n$: the n-dimensional Euclidean space
- How do we use your class data?
**** Classification
- $Y = \{1, \ldots, m\}$ are *discrete* labels
**** Regression
- $Y = \Reals^m$ are *continuous* values

*** The kNN algorithm idea

- Assume an unknown example is similar to its neighbours
- Smoothness allows us to make predictions

/Discriminatory analysis-nonparametric discrimination: consistency properties/, Evelyn Fix and Joseph L.  Hodges Jr, 1951.

**** Evelyn :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+CAPTION: Evelyn Fix
#+ATTR_LATEX: :width 0.5\textwidth
[[../fig/fix_evelyn2.jpg]]
**** Joseph :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+CAPTION: Joseph Hodges
#+ATTR_LATEX: :width 0.5\textwidth
[[../fig/Hodges.jpg]]





*** Performance of KNN on image classification
[[../fig/knn-image-performance.png]]

- Really simple!
- Can outperform really complex models!
  
* The algorithm
** $k$ Nearest Neighbours

*** The Nearest Neighbour algorithm
**** Pseudocode
#+ATTR_BEAMER: :overlay <+->
- Input: Data $(x_t, y_t)_{t=1}^T$, test point $x$, distance $d$ 
- $t^* = \argmin_t d(x_t, x)$ /// How do we implement this?
- Return $\hat{y}_t = y_{t^*}$

#+BEAMER: \pause
**** Classification
     $\hat{y}_t  \in [m] \equiv \{1, \ldots, m\}$
     
#+BEAMER: \pause
**** Regression
$\hat{y}_t  \in \Reals^m$

*** The k-Nearest Neighbour algorithm

**** Pseudocode
#+ATTR_BEAMER: :overlay <+->
- Input: Data $(x_t, y_t)_{t=1}^T$, test point $x$, distance $d$, neighbours \(k\)
- Calculate $h_t = d(x_t, x)$ for all $t$.
- Get sorted indices $s = \texttt{argsort}(h)$ so that $d(x_{s_i}, x) \leq d(x_{s_{i+1}}, x)$ for all $i$. 
- Return $\sum_{i=1}^k y_{s_i} / k$.

#+BEAMER: \pause
**** Classification
- We use a *one-hot encoding* $(0, \ldots, 0, 1, 0, \ldots, 0)$, with $y_t \in \{0,1\}^m$.
- The class of the \(t\)-th example is $j$ $\Leftrightarrow$ $y_{t,j} = 1$.
- Equivalently, return $p$ with 
\[
p_i = \sum_{t=1}^k \ind{y_{s_t} = i} / k
\]
#+BEAMER: \pause
**** Regression
- $y_t  \in \Reals^m$, so we need do nothing

** Extensions and parameters
*** The number of neighbours
**** $k=1$
- How does it perform on the training data?
- How might it perform on unseen data?
**** $k = T$
- How does it perform on the training data?
- How might it perform on unseen data?

*** Distance function
**** For data in $\Reals^n$, \(p\)-norm
\[
d(x,y) = \|x - y\|_p
\]
**** Scaled norms
When features having varying scales:
\[
d(x,y) = \|S x - S y\|_p
\]
Or pre-scale the data

**** Complex data
- Manifold distances
- Graph distance

*** Distances 
**** A distance $d(\cdot, \cdot)$:
- Identity $d(x,x) = 0$.
- Positivity $d(x,y) > 0$ if $x \neq y$.
- Symmetry $d(y,x) = d(x,y)$.
- Triangle inequality $d(x,y) \leq d(x,z) + d(z,y)$.
**** For data in $\Reals^n$, $p$-norm
\[
d(x,y) = \|x - y\|_p
\]
*** Norms;
**** A norm $\|\cdot\|$
- Zero element $\|0\| = 0$.
- Homogeneity $\|cx\| = c \|x\|$ for any scalar $a$.
- Triangle inequality $\|x + y\| \leq \|x\| + \|y\|$.
**** $p$-norm
\[
\|z\|_p = \left(\sum_i z_i^p\right)^{1/p}
\]
*** Neighbourhood calculation
If we have $T$ datapoints
**** Sort and top $K$.
- Requires $O(T \ln T)$ time
**** Use the Cover-Tree or KD-Tree algorithm
- Requires $O(c K \ln T)$ time.
- $c$ depends on the data distribution.


*** Making a decision
**** kNN as a *model*
- Given features $x$, we get a vector $p$ of class probabilities:
\[
p_i = P(y = i | x),
\]
where $P(y = i | x)$ is the probability that  $y$ is $i$, given $x$.
#+BEAMER: \pause
**** Decisions to maximise accuracy
At time $t$:
#+ATTR_BEAMER: :overlay <+->
- We observe features $x_t$
- We *predict* label $a_t = \argmax_i P(y_t = i | x_t)$
- We observe the actual label $y_t$.
- We *win* if $y_t = a_t$ and *lose* otherwise
#+BEAMER: \pause
**** The model versus the prediction
- The *model* $P$ tells us the probability of different classes.
- When we *decide* what our prediction should be, we can *use* the model.
- We will use $\pi$ to denote the *decision rule* or *policy*.
*** Decisions versus predictions
- We frequently need to make a *decision*, instead of just a *prediction*.
- Our *utility* function $U(y, a)$ represents our *preferences*.
- The space of *actions* $A$ is /not identical/ to the set of *labels* $Y$.
#+BEAMER: \pause
**** Minimise spam annoyance                                 :B_exampleblock:
	 :PROPERTIES:
	 :BEAMER_env: exampleblock
	 :END:
What utility function would you use for the spam detection problem?
|----------+------+------+-------|
| Utility  | Pass | Flag | Trash |
|----------+------+------+-------|
| Normal   |      |      |       |
| Spam     |      |      |       |
| Virus    |      |      |       |
|----------+------+------+-------|

#+BEAMER: \pause
**** Classification decision to maximise expected utility
- Expected utility of a single decision
\[
\E[U | a, x] = \sum_y P(y | x, a) U(y, a) = \sum_y P(y | x) U(y, a)
\]
- The decision maximising expected utility
\[
a^* = \argmax_a \E[U | a, x] 
\]

  

  
* Activities
*** KNN activity
- Implement nearest neighbours
- Introduction to scikitlearn nearest neighbours

*** Homework:  Measure performance
In this exercise, you will measure utility on a test set, and select actions that potentially maximise utility in expectation:

**** Measure utility
Create a function called /utilityScore(y, actions, U)/.
  
This takes as input the actual labels $y_t$, and actions $a_t$ (e.g. predicted labels) of a classifier. It then returns the average utility: $\sum_{t=1}^T U(a_t, y_t)/ T$.

**** Calculate utility scores
Calculate the /utility_score/ of a basic kNN classifier for various values of $k$.

**** Return highest-utility actions
Create a function /predictUtil(clf, X, U)/
that takes a classifier /clf/, a dataset of features /X/ and a utility
function /U/ as input. It calls /clf.predict__proba()/ and returns a list of actions, one for each row of X.

**** Verification
Verify that using predict_util() givs you a higher utilityScore() than simply using predict()

