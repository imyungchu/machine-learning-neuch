#+TITLE: Generative Modelling
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}
#+OPTIONS:   H:3

* Graphical models
** Graphical model
*** Graphical models
#+ATTR_LATEX: :center 
\begin{center}
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB) to (xD);
      \draw[->] (xD) to (xi);
      \draw[->] (xB) to (xi);
    \end{tikzpicture}
\end{center}
**** Directed acyclic graph
A directed acyclic graph with 
- Nodes $x_1, \ldots, x_n$
- Edges $x_i \to x_j$.
so that there are no cycles in the graph.
**** Conditional independence from graphical models
- Each *node* of the model corresponds to a *random variable*
- The *parent* of a node are the *direct dependencies* of the random variable.

*** Model specification
\begin{center}
    \begin{tikzpicture}
      \node[RV] at (0,0) (x) {$x$};
      \node[RV] at (1,0) (y) {$y$};
      \node[RV] at (2,0) (z) {$z$};
      \draw[->] (x)--(y);
      \draw[->] (y)--(z);
    \end{tikzpicture}
\end{center}
- The graphical model tells us *what depends on what*.
- It does not tell us *how to generate data*.
- We need to specify the *functional relationship* between variables.
**** Col A                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
***** Statistics
    \begin{align*}
      \label{eq:factored-model}
      x &\sim f\\
      y \mid x = a &\sim g(a)\\
      z \mid y = b &\sim h(b)
    \end{align*}
**** Col B                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
***** Python
#+BEGIN_SRC python
  x = f()
  y = g(x)
  z = h(y)
#+END_SRC


*** Graphical models and factorisation
- Graphical models tell us what directly depends on what
- They allow us to simplify the *joint distribution* of the variables
- This is formed into a *product of factors*, one for each variable.
**** Example
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB)--(xD);
      \draw[->] (xD)--(xi);
    \end{tikzpicture}

    This graphical model implies the factorisation 
    \[
    \Pr(x_1, x_2, x_3) = 
    \Pr(x_3 \mid x_2, x_1) 
    \Pr(x_2 \mid x_1) 
    \Pr(x_1) = 
\Pr(x_3 \mid x_2) \Pr(x_2 \mid x_1) \Pr(x_1)
    \]
     Notice that for each factor $\Pr(x_i \mid x_j)$, $x_j$ is the *parent* of $x_i$.

      
*** Conditional independence
- Graphical models tell us what *directly* depends on what
- Consequently, they also specify *conditional independence*
**** Example
***** Col A                                                           :BMCOL:
      :PROPERTIES:
      :BEAMER_col: 0.5
      :END:
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB)--(xD);
      \draw[->] (xD)--(xi);
    \end{tikzpicture}
***** Col B                                                           :BMCOL:
      :PROPERTIES:
      :BEAMER_col: 0.5
      :END:
    \[
    \Pr(x_1, x_2, x_3) = \Pr(x_3 \mid x_2) \Pr(x_2 \mid x_1) \Pr(x_1)
    \]
**** Conditional independence :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
We say that variables $x, y$ are *conditionally* independent given $z$ and write $x \indep y \mid z$ if and only if
\[
\Pr(x, y \mid z) = \Pr(x \mid z) \Pr(y \mid z)
\]
- In the above example, it holds that $x_3 \indep x_1 \mid x_2$.


*** Smoking and lung cancer

      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$S$};
        \node[RV] at (1,1) (x2) {$C$};
        \node[RV] at (2,0) (x3) {$A$};
        \draw[->] (x1)--(x2);
        \draw[->] (x3)--(x2);
      \end{tikzpicture}
      
- Smoking and lung cancer graphical model.
- $S$: Smoking, $C$: cancer, $A$: asbestos exposure.

#+BEAMER: \pause
- In this graph, $A \indep S$, but $A \not \indep S \mid C$
#+BEAMER: \pause
**** XOR example
#+ATTR_BEAMER: :overlay <+->
- C = xor(S, A). 
- If we know C = 1, S = 1, what is $A$?
- C explains away
*** Time of arrival at work
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$x_1$};
        \node[RV] at (1,1) (x2) {$T$};
        \node[RV] at (2,0) (x3) {$x_2$};
        \draw[->] (x2)--(x3);
        \draw[->] (x2)--(x1);
      \end{tikzpicture}
     
Time of arrival at work graphical model where $T$ is a traffic jam and $x_1$ is the time John arrives at the office and $x_2$ is the time Jane arrives at the office.

#+ATTR_BEAMER: :overlay <+->
- Even though $x_1, x_2$ are *not independent*, they become independent once you know $T$, i.e. $x_1 \indep x_2 \mid T$.
- Proof:
- $\Pr(x_1, x_2, T) = \Pr(x_2 \mid T) \Pr(x_1 \mid T) \Pr(T)$ from the graph.
- $\Pr(x_1, x_2, T) / \Pr(T) = \Pr(x_1, x_2 \mid T) = \Pr(x_2 \mid T) \Pr(x_1 \mid T)$. \qed

*** School admission
**** Example

***** Col A                                                           :BMCOL:
      :PROPERTIES:
      :BEAMER_col: 0.5
      :END:
|---------+------+--------|
| School  | Male | Female |
|---------+------+--------|
| A       |   62 |     82 |
| B       |   63 |     68 |
| C       |   37 |     34 |
| D       |   33 |     35 |
| E       |   28 |     24 |
| F       |    6 |      7 |
|---------+------+--------|
| Average |   50 | 27     |
***** Col B                                                           :BMCOL:
      :PROPERTIES:
      :BEAMER_col: 0.5
      :END:

- $z$: gender
- $s$: school applied to
- $a$: admission

**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :END:
        \begin{tikzpicture}
          \node[RV] at (0,0) (z) {$z$};
          \node[RV] at (1,1) (s) {$s$};
          \node[RV] at (2,0) (a) {$a$};
          \draw[->] (z)--(s);
          \draw[->] (z)--(s);
          \draw[->] (s)--(a);
        \end{tikzpicture}

Is admission independent of gender?
**** Col B                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:

        \begin{tikzpicture}
          \node[RV] at (0,0) (z) {$z$};
          \node[RV] at (1,1) (s) {$s$};
          \node[RV] at (2,0) (a) {$a$};
          \draw[->] (z)--(s);
          \draw[->] (z)--(s);
          \draw[->] (s)--(a);
          \draw[->] (z)--(a);
        \end{tikzpicture}

	How about here?
** Exercises
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (b)--(c);
          \draw[->] (c)--(d);
        \end{tikzpicture}
\[
P(a, b, c, d) = \cdots
\]
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (b)--(c);
          \draw[->] (c)--(d);
          \draw[->] (b)--(d);
        \end{tikzpicture}
\[
P(a, b, c, d) = 
\]
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (a)--(c);
        \end{tikzpicture}
\[
P(a, b, c, d) = 
\]
*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (c | b) P(d | b)
\]

*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (d | c) P(c)
\]

*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (c | a) P(d | b, c)
\]




*** Conditional independence (general)
- Consider variables $x_1, \ldots, x_n$.
- Let $B, D$ be subsets of $[n]$, and
- $\bx_B \defn (x_i)_{i \in B}$ be the variables with indices in $B$.
- $\bx_{-j} \defn (x_i)_{i \neq i}$ all the variables apart from $x_j$.
**** Conditional independence :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
  We say $x_i$ is *conditionally independent* of $\bx_B$ given $\bx_D$ and write 
  \[x_i \indep \bx_B \mid \bx_D\]
  if and only if:
  \[
  \Pr(x_i, \bx_B \mid \bx_D)
  =
  \Pr(x_i \mid \bx_D)
  \Pr(\bx_B \mid \bx_D).
  \]
- For this to hold in graphical model, $D$ must separate $i$ from $B$ in the graph.



*** More complex example
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$x_1$};
          \node[RV] at (0,2) (b) {$x_2$};
          \node[RV] at (2,0) (c) {$x_3$};
          \node[RV] at (2,2) (d) {$x_4$};
          \node[RV] at (4,0) (x) {$x_5$};
          \node[RV] at (4,2) (y) {$x_6$};
          \node[RV] at (6,1) (z) {$x_7$};
          \draw[->] (a)--(c);
          \draw[->] (c)--(d);
          \draw[->] (b)--(d);
          \draw[->] (d)--(y);
          \draw[->] (c)--(x);
          \draw[->] (y)--(z);
          \draw[->] (x)--(z);
        \end{tikzpicture}

In this example, we have:
\[
x_7 \indep x_1, x_2 \mid x_3, x_4
\]
and 
\[
x_7 \indep x_3 \mid x_4, x_5
\]

* Classification
** Classification: Generative modelling
   #+TOC: headlines [currentsection,hideothersubsections]
*** Generative modelling
**** General idea
- Data $(x_t,y_t)$.
- Need to model $P(y | x)$.
- Model the *complete* data distribution: $P(x | y)$, $P(x)$, $P(y)$.
- Calculate \(  P(y | x) = \frac{P(x | y) P(y)}{P(x)}. \)
**** Examples
- *Naive Bayes* classifier.
- *Gaussian mixture* model.
- Large language models.
**** Modelling the data distribution in classification
- Need to estimate the density $P(x | y)$ for each class $y$.
- Need to estimate $P(y)$.
*** The basic graphical model

**** A discriminative classification model
Here $P(y|x)$ is given directly.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \draw[->] (x) to (y);
\end{tikzpicture}

**** A generative classification model
Here $P(y | x) = P(x | y) P(y) / P(x)$.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \draw[->] (y) to (x);
\end{tikzpicture}
**** An unsupervised generative  model
Here we just have $P(x)$.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
\end{tikzpicture}


*** Adding parameters to the graphical model
    
**** A Bernoulli RV
Here, $x | \theta \sim \Ber(\theta)$
\begin{tikzpicture}
\node[RV] at (2,0) (x) {$x$};
\node[RV,hidden] at (0,1) (mean) {$\theta$};
\draw[->] (mean) to (x);
\end{tikzpicture}

**** A normally distributed variable
Here $x  | \mu, \sigma \sim \Normal(\mu, \sigma^2)$
\begin{tikzpicture}
\node[RV] at (2,0) (x) {$x$};
\node[RV,hidden] at (0,1) (mean) {$\mu$};
\node[RV,hidden] at (1,1) (variance) {$\sigma$};
\draw[->] (mean) to (x);
\draw[->] (variance) to (x);
\end{tikzpicture}

*** Classification: Naive Bayes Classifier
- Data $(x,y)$
- $x \in X$
- $y \in Y \subset \mathbb{N}$, $N_i$: amount of data from class $i$.
#+BEAMER: \pause
**** Separately model each class
- Assume each class data comes from a different normal distribution
- $x | y = i \sim \Normal(\mu_i, \sigma_i I)$
- For each class, calculate
  - Empirical mean $\hat{\mu}_i = \sum_{t : y_t = i} x_t / N_i$
  - Empirical variance $\hat{\sigma}_i$.
#+BEAMER: \pause
**** Decision rule
Use Bayes's theorem:
\[
P(y | x) = P(x | y) P(y) / P(x),
\]
choosing the $y$ with largest posterior $P(y | x)$.
- $P(x | y = i) \propto \exp(- \|\hat{\mu}_i - x\|^2/\hat{\sigma}_i^2)$
*** Graphical model for the Naive Bayes Classifier
**** When $x \in \Reals$
Assume $k$ classes, then
- $\mu = (\mu_1, \ldots, \mu_k)$
- $\sigma = (\sigma_1, \ldots, \sigma_k)$
- \(\theta = (\theta_1, \ldots, \theta_k)\)
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \node[RV,hidden] at (2,1) (mean) {$\mu$};
      \node[RV,hidden] at (3,1) (variance) {$\sigma$};
      \node[RV,hidden] at (0,1) (choice) {$\theta$};
      \draw[->] (y) to (x);
      \draw[->] (mean) to (x);
      \draw[->] (variance) to (x);
      \draw[->] (choice) to (y);
\end{tikzpicture}
- $y \mid \theta \sim \Mult(\theta)$
- $x \mid y, \mu, \sigma \sim \Normal(\mu_y, \sigma^2_y)$
** Density estimation
*** General idea
**** Parametric models
- Fixed histograms
- Gaussian Mixtures
**** Non-parametric models
- Variable-bin histograms
- Infinite Gaussian Mixture Model
- Kernel methods

*** Histograms
**** Fixed histogram
- Hyper-Parameters: number of bins
- Parameters: Number of points in each bin.
**** Variable histogram
- Hyper-parameters: Rule for constructing bins
- Generally $\sqrt{n}$ points in each bin.

*** Gaussian Mixture Model

**** Hyperparameters:
- Number of Gaussian $k$.
**** Parameters:
- Multinomial distribution $\vparam$ over Gaussians
- For each Gaussian $i$, center $\mu_i$, covariance matrix $\Sigma_i$.
**** Algorithms:
- Expectation Maximisation
- Gradient Ascent
- Variational Bayesian Inference (with appropriate prior)

*** Details of Gaussian mixture models
**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
***** Model. For each point $x_t$:
- $z_t \mid  \theta \sim \Mult(\theta_i)$, $\theta \in \Simplex^k$
- $x_t | z_t = i \sim \Normal(\mu_i, \Sigma_i)$.
- $\Mult(\theta)$ is *multinomial*
\[
\Pr(z_t = i \mid \theta) = \theta_i
\]
- $\Normal(\mu, \Sigma)$ is *multivariate Gaussian*
\[
p(x \mid \mu, \Sigma)
\propto \exp(-\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x-\mu))
\]
- The generating distribution is
\[
p(x | \theta, \mu, \Sigma) = \sum_{z \in [k]} p(x \mid \mu_z, \Sigma_z) P(z \mid \theta).
\]
**** Col B                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:


\begin{tikzpicture}
      \node[RV, hidden] at (2,0) (c) {$z$};
      \node[RV] at (0,0) (x) {$x$};
      \node[RV,hidden] at (0,-1) (mu) {$\mu$};
      \node[RV,hidden] at (0,1) (sigma) {$\Sigma$};
      \node[RV,hidden] at (2,1) (theta) {$\theta$};
      \draw[->] (c) to (x);
      \draw[->] (theta) to (c);
      \draw[->] (sigma) to (x);
      \draw[->] (mu) to (x);
\end{tikzpicture}


* Algorithms for latent variable models     

** Gradient algorithms
*** Gradient ascent
In the following we use $\theta$ for all the parameters of the Gaussian mixture model,
with $x = (x_1, \ldots, x_T)$ and $z = (z_1, \ldots, z_T)$
**** Objective function
One way to estimate $\theta$ is through maximising the likelihood
$L(\theta) = P(x | \theta)$
**** Marginalisation over latent variable
However, we need to marginalise over all values $z$
\[
L(\theta) = \sum_z P(z, x | \theta)
\]
For $T$ data points and $k$ different values of $z_t$, there are $k^T$ vectors $z$ to sum over.
**** Gradient ascent
If we can calculate the gradient of $L$, we can use gradient ascent to update our parameters:
\[
\theta^{(n+1)} = \theta^{(n)} + \alpha \nabla_\theta L(\theta).
\]
*** Gradient calculation
Here we use the *log trick*: $\nabla \ln f(\theta) = \nabla f(\theta) / f(\theta)$.
\begin{align}
\nabla_\theta L(\theta)
& = \sum_z \nabla_\theta P(z, x \mid \theta) 
\\
&= \sum_z  P(z, x \mid \theta) \nabla_\theta \ln P(z, x \mid \theta)
\\
&= \sum_z  P(x \mid z, \theta)P(z \mid \theta) \nabla_\theta \ln P(z, x \mid \theta)
\\
&\approx \frac{1}{m} \sum_{i=1}^m P(x \mid z^{(i)}, \theta) \nabla_\theta \ln P(z^{(i)}, x \mid \theta)
&&z^{(i)} \sim P(z  \mid \theta)
\end{align}
The final approximates the sum with the sample mean, sampling $z^{(i)}$ from the distribution. Hence, we can implement the following algorithm
- For $i = 1, \ldots, m$: $z^{(i)} \sim P(z  \mid \theta^{(n)})$
- $d^{(n)} = \frac{1}{m} \sum_{i=1}^m P(x \mid z^{(i)}, \theta) \nabla_\theta \ln P(z^{(i)}, x \mid \theta^{(n)})$
- $\theta^{(n+1)} = \theta^n + \alpha d^{(n)}$.
** Expectation maximisation
*** A lower bound on the likelihood
For any distribution $G(z)$, and specifically for
$G(z) = P(z | x, \theta^{(k)})$:
\begin{align*}
\ln P(x | \alert{\theta})
& = \sum_z G(z) \ln P(x | \alert{\theta})
 = \sum_z G(z) \ln [P(x, z | \alert{\theta}) / P(z | x, \alert{\theta})]
\\
& = \sum_z G(z) [\ln P(x, z | \alert{\theta}) - \ln P(z | x, \alert{\theta})]
\\
& = \sum_z G(z) \ln P(x, z | \alert{\theta}) - \sum_z G(z) \ln P(z | x, \alert{\theta})
\\
& = \sum_z P(z | x, \theta^{(k)}) \ln P(x, z | \alert{\theta}) - \sum_z P(z | x, \theta^{(k)}) \ln P(z | x, \alert{\theta})
\\
& \geq \sum_z P(z | x, \theta^{(k)}) \ln P(x, z | \alert{\theta}) - \sum_z P(z | x, \theta^{(k)}) \ln P(z | x, \theta^{(k)})
\\
& = Q(\alert{\theta} \mid \theta^{(k)}) +\mathbb{H}(z \mid x, \theta^{(k)}),
\end{align*}
where 
\[
\mathbb{H}(z \mid  x, \theta^{(k)})
= 
\sum_z  P(z \mid  x, \theta^{(k)}) \ln P(z \mid x, \theta^{(k)})
\]
is the entropy of $z$ for a fixed $x, \theta^{(k)}$. As this is not negative, $\ln P(x | \theta) \geq Q(\theta \mid \theta^{(k)})$.
*** Some information theory
Information theory notation can be a bit confusing. Sometimes we talk about random variables $\omega$, and sometimes about probability measures $P$. This is context-dependent.
**** Entropy
For a random variable $\omega$ under distribution $P$, we denote the entropy as
\[
 \mathbb{H}_P(\omega) \equiv \mathbb{H}(P) \equiv \mathbb{H}(\omega) =  \sum_{\omega \in \Omega} P(\omega) \ln P(\omega).
\]
**** KL Divergence
For two probabilities $P, Q$ over random outcomes in the same space $\Omega$, we define
\[
D_{KL}(P \|Q) = \sum_{\omega \in \Omega} P(\omega) \ln \frac{P(\omega)}{Q(\omega)}
\]
**** The Gibbs Inequality
$D_{KL}(P \|Q)  \geq 0$, or $\sum_x \ln P(x) P(x) \geq \sum_x \ln Q(x) P(x)$.
*** EM Algorithm (Dempster et al, 1977)
- Initial parameter $\vparam^{(0)}$, observed data $x$
- For $k=0, 1, \ldots$
-- Expectation step:
\[
Q(\alert{\vparam} \mid  \vparam^{(k)})
 \defn \E_{z \sim P(z | x, \vparam^{(k)})} [\ln P(x, z | \alert{\vparam}) ]
 = \sum_{z} [\ln P(x, z | \alert{\vparam})]  P(z  \mid x, \vparam^{(k)})
\]
-- Maximisation step:
\[
\vparam^{(k+1)} = \argmax_\vparam Q(\vparam, \vparam^{(k)}).
\]

See /Expectation-Maximization as lower bound maximization, Minka, 1998/

*** Minorise-Maximise
EM can be seen as a version of the minorise-maximise algorithm
- $f(\vparam)$: Target function to *maximise*
- $Q(\vparam | \vparam^{(k)})$: surrogate function
**** $Q$ Minorizes $f$
This means surrogate is always a lower bound so that
\[
f(\vparam) \geq Q(\vparam | \vparam^{(k)}),
\qquad
f(\vparam^{(k)}) \geq Q(\vparam^{(k)} | \vparam^{(k)}),
\]

**** Algorithm
- Calculate: $Q(\vparam | \vparam^{(k)})$
- Optimise: $\vparam^{(k+1)} = \argmax_\vparam Q(\vparam | \vparam^{(k)})$.



* Exercises
** Density estimation
*** GMM versus histogram
- Generate some data $x$ from an arbitrary distribution in $\Reals$.
- Fit the data with a histogram for varying numbers of bins
- Fit a GMM with varying numbers of Gaussians
- What is the best fit? How can you measure it?

** Classification
*** GMM Classifier :exercise:
**** Base class: sklearn GaussianMixtureModel
- /fit()/ only works for Density Estimaiton
- /predict()/ only predicts cluster labels
**** Problem
- Create a GMMClassifier class
- /fit()/ should take X, y, arguments
- /predict()/ should predict class labels
- Hint: Use /predict_proba()/ and multiple GMM models


