#+TITLE: Generative Modelling
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}
#+OPTIONS:   H:3

* Graphical models
** Graphical model
*** Graphical models
#+ATTR_LATEX: :center 
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB) to (xD);
      \draw[->] (xD) to (xi);
      \draw[->] (xB) to (xi);
    \end{tikzpicture}
- Variables $x_1, x_2, x_3$
- Arrows denote dependencies between variables.

*** Conditional independence
**** Example
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB)--(xD);
      \draw[->] (xD)--(xi);
    \end{tikzpicture}
    Graphical model for the factorisation $\Pr(x_3 \mid x_2) \Pr(x_2 \mid x_1) \Pr(x_1)$.
**** Definition
    - Consider variables $x_1, \ldots, x_n$.
    - Let $B, D$ be subsets of $[n]$.

    We say $x_i$ is *conditionally independent* of $\bx_B$ given $\bx_D$ and write 
    \[x_i \indep \bx_B \mid \bx_D\]
    if and only if:
    \[
    \Pr(x_i, \bx_B \mid \bx_D)
    =
    \Pr(x_i \mid \bx_D)
    \Pr(\bx_B \mid \bx_D).
    \]
*** Directed graphical model
A collection of $n$ random variables $x_i : \Omega \to X_i$, and let $X \defn \prod_i X_i$, with underlying probability measure $P$ on $\Omega$.
      Let $\bx = (x_i)_{i=1}^n$ and for any subset $B \subset[n]$ let
      \begin{align}
        \bx_B &\defn (x_i)_{i \in B}\\
        \bx_{-j} &\defn (x_i)_{i \neq i}
      \end{align}
*** Model specification
**** Col A                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
***** Statistics
    \begin{align*}
      \label{eq:factored-model}
      x &\sim f\\
      y \mid x = a &\sim g(a)\\
      z \mid y = b &\sim h(b),
    \end{align*}
**** Col B                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
***** Python
#+BEGIN_SRC python
  x = f()
  y = g(x)
  z = h(y)
#+END_SRC
*** Smoking and lung cancer
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$S$};
        \node[RV] at (1,1) (x2) {$C$};
        \node[RV] at (2,0) (x3) {$A$};
        \draw[->] (x1)--(x2);
        \draw[->] (x3)--(x2);
      \end{tikzpicture}
      
Smoking and lung cancer graphical model, where $S$: Smoking, $C$: cancer, $A$: asbestos exposure.

*** Time of arrival at work
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$x_1$};
        \node[RV] at (1,1) (x2) {$T$};
        \node[RV] at (2,0) (x3) {$x_2$};
        \draw[->] (x2)--(x3);
        \draw[->] (x2)--(x1);
      \end{tikzpicture}
     
Time of arrival at work graphical model where $T$ is a traffic jam and $x_1$ is the time John arrives at the office and $x_2$ is the time Jane arrives at the office.

*Conditional independence:
- Even though $x_1, x_2$ are *not independent*, they become independent once you know $T$.

*** School admission
|--------+------+--------|
| School | Male | Female |
|--------+------+--------|
| A      |   62 |     82 |
| B      |   63 |     68 |
| C      |   37 |     34 |
| D      |   33 |     35 |
| E      |   28 |     24 |
| F      |    6 |      7 |
|--------+------+--------|

- $z$: gender
- $s$: school applied to
- $a$: admission

**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
        \begin{tikzpicture}
          \node[RV] at (0,0) (z) {$z$};
          \node[RV] at (1,1) (s) {$s$};
          \node[RV] at (2,0) (a) {$a$};
          \draw[->] (z)--(s);
          \draw[->] (z)--(s);
          \draw[->] (s)--(a);
        \end{tikzpicture}
	Is admission independent of gender?
**** Col B                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:



        \begin{tikzpicture}
          \node[RV] at (0,0) (z) {$z$};
          \node[RV] at (1,1) (s) {$s$};
          \node[RV] at (2,0) (a) {$a$};
          \draw[->] (z)--(s);
          \draw[->] (z)--(s);
          \draw[->] (s)--(a);
          \draw[->] (z)--(a);
        \end{tikzpicture}

	How about here?
** Exercises
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (b)--(c);
          \draw[->] (c)--(d);
        \end{tikzpicture}
\[
P(a, b, c, d) = \cdots
\]
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (b)--(c);
          \draw[->] (c)--(d);
          \draw[->] (b)--(d);
        \end{tikzpicture}
\[
P(a, b, c, d) = 
\]
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (a)--(c);
        \end{tikzpicture}
\[
P(a, b, c, d) = 
\]
*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (c | b) P(d | b)
\]

*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (d | c) P(c)
\]

*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (c | a) P(d | b, c)
\]







* Classification
** Classification: Generative modelling
   #+TOC: headlines [currentsection,hideothersubsections]
*** Generative modelling
**** General idea
- Data $(x_t,y_t)$.
- Need to model $P(y | x)$.
- Model the *complete* data distribution: $P(x | y)$, $P(x)$, $P(y)$.
- Calculate \(  P(y | x) = \frac{P(x | y) P(x)}{P(y)}. \)
**** Examples
- *Naive Bayes* classifier.
- *Gaussian mixture* model.
- Large language models.
**** Modelling the data distribution in classification
- Need to estimate the density $P(x | y)$ for each class $y$.
- Need to estimate $P(y)$.
*** The basic graphical model

**** A discriminative classification model
Here $P(y|x)$ is given directly.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \draw[->] (x) to (y);
\end{tikzpicture}

**** A generative classification model
Here $P(y | x) = P(x | y) P(y) / P(x)$.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \draw[->] (y) to (x);
\end{tikzpicture}
**** An unsupervised generative  model
Here we just have $P(x)$.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
\end{tikzpicture}


*** Adding parameters to the graphical model
    
**** A Bernoulli RV
Here, $x | \theta \sim \Ber(\theta)$
\begin{tikzpicture}
\node[RV] at (2,0) (x) {$x$};
\node[RV,hidden] at (0,1) (mean) {$\theta$};
\draw[->] (mean) to (x);
\end{tikzpicture}

**** A normally distributed variable
Here $x  | \mu, \sigma \sim \Normal(\mu, \sigma^2)$
\begin{tikzpicture}
\node[RV] at (2,0) (x) {$x$};
\node[RV,hidden] at (0,1) (mean) {$\mu$};
\node[RV,hidden] at (1,1) (variance) {$\sigma$};
\draw[->] (mean) to (x);
\draw[->] (variance) to (x);
\end{tikzpicture}

*** Classification: Naive Bayes Classifier
- Data $(x,y)$
- $x \in X$
- $y \in Y \subset \mathbb{N}$, $N_i$: amount of data from class $i$.
  
**** Separately model each class
- Assume each class data comes from a different normal distribution
- $x | y = i \sim \Normal(\mu_i, \sigma_i I)$
- For each class, calculate
  - Empirical mean $\hat{\mu}_i = \sum_{t : y_t = i} x_t / N_i$
  - Empirical variance $\hat{\sigma}_i$.

**** Decision rule
Use Bayes's theorem:
\[
P(y | x) = P(x | y) P(y) / P(x),
\]
choosing the $y$ with largest posterior $P(y | x)$.
- $P(x | y = i) \propto \exp(- \|\hat{\mu}_i - x\|^2/\hat{\sigma}_i^2)$
*** Graphical model for the Naive Bayes Classifier
**** When $x \in \Reals$
Assume $k$ classes, then
- $\mu = (\mu_1, \ldots, \mu_k)$
- $\sigma = (\sigma_1, \ldots, \sigma_k)$
- \(\theta = (\theta_1, \ldots, \theta_k)\)
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \node[RV,hidden] at (2,1) (mean) {$\mu$};
      \node[RV,hidden] at (3,1) (variance) {$\sigma$};
      \node[RV,hidden] at (0,1) (choice) {$\theta$};
      \draw[->] (y) to (x);
      \draw[->] (mean) to (x);
      \draw[->] (variance) to (x);
      \draw[->] (choice) to (y);
\end{tikzpicture}
- $y \mid \theta \sim \Mult(\theta)$
- $x \mid y, \mu, \sigma \sim \Normal(\mu_y, \sigma^2_y)$
** Density estimation
*** General idea
**** Parametric models
- Fixed histograms
- Gaussian Mixtures
**** Non-parametric models
- Variable-bin histograms
- Infinite Gaussian Mixture Model
- Kernel methods

*** Histograms
**** Fixed histogram
- Hyper-Parameters: number of bins
- Parameters: Number of points in each bin.
**** Variable histogram
- Hyper-parameters: Rule for constructing bins
- Generally $\sqrt{n}$ points in each bin.

*** Gaussian Mixture Model

**** Hyperparameters:
- Number of Gaussian $k$.
**** Parameters:
- Multinomial distribution $\vparam$ over Gaussians
- For each Gaussian $i$, center $\mu_i$, covariance matrix $\Sigma_i$.
**** Algorithms:
- Expectation Maximisation
- Gradient Ascent
- Variational Bayesian Inference (with appropriate prior)

*** Details of Gaussian mixture models
**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
***** Model. For each point $x_t$:
- $z_t \mid  \theta \sim \Mult(\theta_i)$, $\theta \in \Simplex^k$
- $x_t | z_t = i \sim \Normal(\mu_i, \Sigma_i)$.
- $\Mult(\theta)$ is *multinomial*
\[
\Pr(z_t = i \mid \theta) = \theta_i
\]
- $\Normal(\mu, \Sigma)$ is *multivariate Gaussian*
\[
p(x \mid \mu, \Sigma)
\propto \exp(-\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x-\mu))
\]
- The generating distribution is
\[
p(x | \theta, \mu, \Sigma) = \sum_{z \in [k]} p(x \mid \mu_z, \Sigma_z) P(z \mid \theta).
\]
**** Col B                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:


\begin{tikzpicture}
      \node[RV, hidden] at (2,0) (c) {$z$};
      \node[RV] at (0,0) (x) {$x$};
      \node[RV,hidden] at (0,-1) (mu) {$\mu$};
      \node[RV,hidden] at (0,1) (sigma) {$\Sigma$};
      \node[RV,hidden] at (2,1) (theta) {$\theta$};
      \draw[->] (c) to (x);
      \draw[->] (theta) to (c);
      \draw[->] (sigma) to (x);
      \draw[->] (mu) to (x);
\end{tikzpicture}


* Algorithms for latent variable models     

** Gradient algorithms
*** Gradient ascent
In the following we use $\theta$ for all the parameters of the Gaussian mixture model,
with $x = (x_1, \ldots, x_T)$ and $z = (z_1, \ldots, z_T)$
**** Objective function
One way to estimate $\theta$ is through maximising the likelihood
$L(\theta) = P(x | \theta)$
**** Marginalisation over latent variable
However, we need to marginalise over all values $z$
\[
L(\theta) = \sum_z P(z, x | \theta)
\]
For $T$ data points and $k$ different values of $z_t$, there are $k^T$ vectors $z$ to sum over.
**** Gradient ascent
If we can calculate the gradient of $L$, we can use gradient ascent to update our parameters:
\[
\theta^{(n+1)} = \theta^{(n)} + \alpha \nabla_\theta L(\theta).
\]
*** Gradient calculation
Here we use the *log trick*: $\nabla \ln f(\theta) = \nabla f(\theta) / f(\theta)$.
\begin{align}
\nabla_\theta L(\theta)
& = \sum_z \nabla_\theta P(z, x \mid \theta) 
\\
&= \sum_z  P(z, x \mid \theta) \nabla_\theta \ln P(z, x \mid \theta)
\\
&= \sum_z  P(x \mid z, \theta)P(z \mid \theta) \nabla_\theta \ln P(z, x \mid \theta)
\\
&\approx \frac{1}{m} \sum_{i=1}^m P(x \mid z^{(i)}, \theta) \nabla_\theta \ln P(z^{(i)}, x \mid \theta)
&&z^{(i)} \sim P(z  \mid \theta)
\end{align}
The final approximates the sum with the sample mean, sampling $z^{(i)}$ from the distribution. Hence, we can implement the following algorithm
- For $i = 1, \ldots, m$: $z^{(i)} \sim P(z  \mid \theta^{(n)})$
- $d^{(n)} = \frac{1}{m} \sum_{i=1}^m P(x \mid z^{(i)}, \theta) \nabla_\theta \ln P(z^{(i)}, x \mid \theta^{(n)})$
- $\theta^{(n+1)} = \theta^n + \alpha d^{(n)}$.
** Expectation maximisation
*** A lower bound on the likelihood
For any distribution $G(z)$, and specifically for
$G(z) = P(z | x, \theta^{(k)})$:
\begin{align*}
\ln P(x | \alert{\theta})
& = \sum_z G(z) \ln P(x | \alert{\theta})
 = \sum_z G(z) \ln [P(x, z | \alert{\theta}) / P(z | x, \alert{\theta})]
\\
& = \sum_z G(z) [\ln P(x, z | \alert{\theta}) - \ln P(z | x, \alert{\theta})]
\\
& = \sum_z G(z) \ln P(x, z | \alert{\theta}) - \sum_z G(z) \ln P(z | x, \alert{\theta})
\\
& = \sum_z P(z | x, \theta^{(k)}) \ln P(x, z | \alert{\theta}) - \sum_z P(z | x, \theta^{(k)}) \ln P(z | x, \alert{\theta})
\\
& \geq \sum_z P(z | x, \theta^{(k)}) \ln P(x, z | \alert{\theta}) - \sum_z P(z | x, \theta^{(k)}) \ln P(z | x, \theta^{(k)})
\\
& = Q(\alert{\theta} \mid \theta^{(k)}) +\mathbb{H}(z \mid x, \theta^{(k)}),
\end{align*}
where 
\[
\mathbb{H}(z \mid  x, \theta^{(k)})
= 
\sum_z  P(z \mid  x, \theta^{(k)}) \ln P(z \mid x, \theta^{(k)})
\]
is the entropy of $z$ for a fixed $x, \theta^{(k)}$. As this is not negative, $\ln P(x | \theta) \geq Q(\theta \mid \theta^{(k)})$.
*** Some information theory
Information theory notation can be a bit confusing. Sometimes we talk about random variables $\omega$, and sometimes about probability measures $P$. This is context-dependent.
**** Entropy
For a random variable $\omega$ under distribution $P$, we denote the entropy as
\[
 \mathbb{H}_P(\omega) \equiv \mathbb{H}(P) \equiv \mathbb{H}(\omega) =  \sum_{\omega \in \Omega} P(\omega) \ln P(\omega).
\]
**** KL Divergence
For two probabilities $P, Q$ over random outcomes in the same space $\Omega$, we define
\[
D_{KL}(P \|Q) = \sum_{\omega \in \Omega} P(\omega) \ln \frac{P(\omega)}{Q(\omega)}
\]
**** The Gibbs Inequality
$D_{KL}(P \|Q)  \geq 0$, or $\sum_x \ln P(x) P(x) \geq \sum_x \ln Q(x) P(x)$.
*** EM Algorithm (Dempster et al, 1977)
- Initial parameter $\vparam^{(0)}$, observed data $x$
- For $k=0, 1, \ldots$
-- Expectation step:
\[
Q(\alert{\vparam} \mid  \vparam^{(k)})
 \defn \E_{z \sim P(z | x, \vparam^{(k)})} [\ln P(x, z | \alert{\vparam}) ]
 = \sum_{z} [\ln P(x, z | \alert{\vparam})]  P(z  \mid x, \vparam^{(k)})
\]
-- Maximisation step:
\[
\vparam^{(k+1)} = \argmax_\vparam Q(\vparam, \vparam^{(k)}).
\]

See /Expectation-Maximization as lower bound maximization, Minka, 1998/

*** Minorise-Maximise
EM can be seen as a version of the minorise-maximise algorithm
- $f(\vparam)$: Target function to *maximise*
- $Q(\vparam | \vparam^{(k)})$: surrogate function
**** $Q$ Minorizes $f$
This means surrogate is always a lower bound so that
\[
f(\vparam) \geq Q(\vparam | \vparam^{(k)}),
\qquad
f(\vparam^{(k)}) \geq Q(\vparam^{(k)} | \vparam^{(k)}),
\]

**** Algorithm
- Calculate: $Q(\vparam | \vparam^{(k)})$
- Optimise: $\vparam^{(k+1)} = \argmax_\vparam Q(\vparam | \vparam^{(k)})$.



* Exercises
** Density estimation
*** GMM versus histogram
- Generate some data $x$ from an arbitrary distribution in $\Reals$.
- Fit the data with a histogram for varying numbers of bins
- Fit a GMM with varying numbers of Gaussians
- What is the best fit? How can you measure it?

** Classification
*** GMM Classifier :exercise:
**** Base class: sklearn GaussianMixtureModel
- /fit()/ only works for Density Estimaiton
- /predict()/ only predicts cluster labels
**** Problem
- Create a GMMClassifier class
- /fit()/ should take X, y, arguments
- /predict()/ should predict class labels
- Hint: Use /predict_proba()/ and multiple GMM models


