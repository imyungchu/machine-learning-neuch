{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session: Tree-Based Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "1. Understand how the Tree-Based Methods works.\n",
    "\n",
    "2. Implement the different algorithms.\n",
    "\n",
    "3. Evaluate the performance of models and compare.\n",
    "\n",
    "\n",
    "**Tree-based methods** are a class of supervised learning algorithms that use a tree like model of decisions. These methods are used for both classification  and regression and based on a divide approach, splitting the dataset into smaller subsets recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Key Idea Behind Tree-Based Methods\n",
    "The idea is to partition the dataset into regions where the response variable is as homogeneous as possible. This is achieved by:\n",
    "\n",
    "1. **Splitting** the data based on conditions (e.g., $ x \\leq t $ where $ t $ is a threshold).\n",
    "\n",
    "2. Repeating the splitting recursively until a stopping condition is met.\n",
    "\n",
    "3. Assigning a prediction (class label or mean value) to the regions created.\n",
    "\n",
    "\n",
    "\n",
    "### How Does a Decision Tree Decide Where to Split?\n",
    "\n",
    "1. Iterate Over All Features:\n",
    "\n",
    "   - The algorithm examines each feature in the dataset.\n",
    "\n",
    "   - For each feature, it evaluates all possible thresholds or categories (for numerical or categorical features, respectively).\n",
    "\n",
    "2. Evaluate Each Split:\n",
    "\n",
    "   - For numerical features: It tries splitting at every unique value of the feature.\n",
    "\n",
    "   - For categorical features: It considers separating groups of categories.\n",
    "\n",
    "3. Compute a Splitting Criterion:\n",
    "   - The algorithm calculates a **metric** to measure how good each split is. Common metrics include:\n",
    "\n",
    "     - **Gini Index** (used in classification):\n",
    "       $$\n",
    "       G = 1 - \\sum_{k=1}^K p_k^2\n",
    "       $$\n",
    "      $ p_k $ is the proportion of samples of class $ k $ in a node.\n",
    "\n",
    "     - **Entropy** (used in classification):\n",
    "      $$\n",
    "       H = -\\sum_{k=1}^K p_k \\log(p_k)\n",
    "       $$\n",
    "       Lower Gini or entropy means a purer split.\n",
    "       \n",
    "     - **Mean Squared Error (MSE)** (used in regression):\n",
    "       $$\n",
    "       MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\bar{y})^2\n",
    "       $$\n",
    "       Minimizing MSE creates better splits for regression.\n",
    "\n",
    "4. Select the Best Split:\n",
    "   - After evaluating all splits, the algorithm chooses the one that maximizes purity (or minimizes error) according to the selected criterion.\n",
    "   \n",
    "   - If multiple splits are equally good, one may be chosen randomly, but this is rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a **recursive split manually** based on **Feature \\(X_1\\)**.\n",
    "\n",
    "\n",
    "| ID  | Feature \\(X_1\\) | Feature \\(X_2\\) | Class \\(y\\) |\n",
    "|:---:|:---------------:|:---------------:|:-----------:|\n",
    "|  1  |        2        |        3        |      A      |\n",
    "|  2  |        4        |        1        |      A      |\n",
    "|  3  |        6        |        5        |      B      |\n",
    "|  4  |        8        |        7        |      B      |\n",
    "|  5  |       10        |        9        |      B      |\n",
    "\n",
    "We will split the data only by **$X_1$**. The values of $X_1$ are $[2, 4, 6, 8, 10]$.\n",
    "\n",
    "The possible split points are midpoints between consecutive values:\n",
    "- $ \\text{Split 1}: X_1 \\leq 3 $\n",
    "- $ \\text{Split 2}: X_1 \\leq 5 $\n",
    "- $ \\text{Split 3}: X_1 \\leq 7 $\n",
    "- $ \\text{Split 4}: X_1 \\leq 9 $\n",
    "\n",
    "We calculate the **Gini Index** for each split to choose the best one.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-Based Methods in Practice\n",
    "Tree-based methods include:\n",
    "\n",
    "1. **Decision Trees**:\n",
    "   - Simple and interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "# TO DO\n",
    "\n",
    "# Fit Classification Tree\n",
    "# Use the DecisionTreeClassifier from sklearn. Its components are: criterion, \n",
    "# max_depht (parameter defines the maximum number of levels the tree can have) and random_state\n",
    "#TO DO\n",
    "\n",
    "# Predictions and Evaluation (Accuracy computation)\n",
    "\n",
    "# Visualize the Tree\n",
    "#replace model by the name of your model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(model, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tasks for classification tree**:\n",
    "\n",
    "1. Modify the tree by using the `entropy` criterion.\n",
    "\n",
    "2. Test the effect of maximum depth on the accuracy of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression tree\n",
    "\n",
    "- Trees split data to minimize the error\n",
    "\n",
    "- The terminal node represents the average value of the responses in that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic regression data of 100 data points with 1 feature and \n",
    "#1 column for target\n",
    "# TO DO\n",
    "\n",
    "# Split data\n",
    "# TO DO\n",
    "\n",
    "# Fit Regression Tree USING DecisionTreeRegressor with max_depth and random_state as components\n",
    "# TO DO\n",
    "\n",
    "# Predictions and Evaluation ( error)\n",
    "# TO DO\n",
    "\n",
    "# Visualize the Tree\n",
    "#replace model by the name of your model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(model, filled=True, feature_names=['Feature'], precision=2)\n",
    "plt.show()\n",
    "\n",
    "# You can Plot predictions using plt.scatter for (X_test, y_test) and \n",
    "# (X_test, y_pred)\n",
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Tasks for regression tree**:\n",
    "\n",
    "1. Try different depths for the tree and observe the effect on MSE.\n",
    "\n",
    "2. Generate new synthetic data (1000 points) and repeat the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Bagging and Random Forests**:\n",
    "\n",
    "   - Combine multiple trees for better predictions.\n",
    "\n",
    "   - introduces randomness by choosing by selecting a random subset of features for splitting.\n",
    "\n",
    "- For classification: Majority vote.\n",
    "\n",
    "- For regression: Average.\n",
    "\n",
    "So think of a random forest as a \"forest\" made up of many \"trees\" (decision trees). Each tree makes its own prediction, and the forest combines these predictions (e.g., by majority voting for classification or averaging for regression).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Loan Approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Income  Loan Approved\n",
       "0   25     Low              0\n",
       "1   30  Medium              0\n",
       "2   35  Medium              1\n",
       "3   40    High              1\n",
       "4   45    High              1\n",
       "5   50     Low              0\n",
       "6   25  Medium              0\n",
       "7   30    High              0\n",
       "8   35    High              1\n",
       "9   40     Low              1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Example Dataset\n",
    "data = {\n",
    "    'Age': [25, 30, 35, 40, 45, 50, 25, 30, 35, 40],\n",
    "    'Income': ['Low', 'Medium', 'Medium', 'High', 'High', 'Low',\n",
    "               'Medium', 'High', 'High', 'Low'],\n",
    "    'Loan Approved': [0, 0, 1, 1, 1, 0, 0, 0, 1, 1]  # 0 = No, 1 = Yes\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "df = pd.get_dummies(df, columns=['Income'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "#TO DO\n",
    "\n",
    "# Train Random Forest using RandomForestClassifier for sklearn with components:\n",
    "# n_estimators(number of trees), max_features='sqrt', and random_state\n",
    "#TO DO\n",
    "\n",
    "# Predict and Evaluate (Accuracy)\n",
    "#TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single tree from the forest\n",
    "#replace model by the name of your model\n",
    "single_tree = model.estimators_[2]  # Get one tree\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(single_tree, feature_names=X.columns, class_names=['No', 'Yes'], filled=True, rounded=True)\n",
    "plt.title(\"Tree from Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tasks for random forest**:\n",
    "\n",
    "1. Compare results with a single decision tree.\n",
    "\n",
    "2. Vary the number of estimators and max features and compare accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train the random forest for regression using the data points you \n",
    "# generate previously. print the MSE\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Boosting for Classification and Regression**\n",
    "\n",
    "   -  Boosting trains sequential models where each model corrects the errors of its predecessor. Final predictions are weighted sums.\n",
    "   \n",
    "    - Weighted error  of a weak learner $m$\n",
    "\n",
    "    $$\n",
    "    e_m = \\sum_{i=1}^N w_i \\cdot \\mathbb{1}(y_i \\neq \\hat{y}_i^{(m)})\n",
    "    $$\n",
    "\n",
    "    - A weak learner is typically a simple model, such as a decision stump (a decision tree with only one split). Each weak learner focuses on improving the performance by correcting the mistakes made by the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor\n",
    "\n",
    "# AdaBoost for Classification\n",
    "# Use the previous dataset to train AdaBoostClassifier and evaluate\n",
    "# TO DO\n",
    "\n",
    "# Gradient Boosting for Regression\n",
    "# Generate a data for regression using GradientBoostingRegressor. Components \n",
    "# are n_estimators, learning_rate, random_state\n",
    "# Print the MSE\n",
    "# TO DO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tasks**:\n",
    "\n",
    "1. Compare AdaBoost with Random Forest.\n",
    "\n",
    "2. Experiment with learning rates in Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Task**:\n",
    "\n",
    "Consider the data you generated for the previous assignment ( data of your project):\n",
    "\n",
    "1. Perform classification or regression using all methods.\n",
    "\n",
    "2. Compare their performance metrics.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
