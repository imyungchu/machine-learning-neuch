{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB session: Features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "1. Understand the need for feature selection.\n",
    "\n",
    "2. Apply various feature selection techniques.\n",
    "\n",
    "3. Evaluate the performance of models with and without feature selection.\n",
    "\n",
    "\n",
    "Feature selection is a crucial step in machine learning that helps improve the performance of models by selecting only the most relevant features (or columns) in a dataset and removing the rest. This reduces noise, speeds up computation, and can improve model accuracy. In our lab session, we'll go through the entire process step-by-step using generated data, covering different techniques for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate a Synthetic Dataset\n",
    "\n",
    "We'll start by generating a dataset with the following characteristics:\n",
    "\n",
    "- 1000 samples (rows)\n",
    "\n",
    "- 15 features (columns), with some of them irrelevant to the target variable\n",
    "\n",
    "To simulate real-world data, we'll include a binary target variable ( 0 or 1, representing two classes) and a mix of relevant and irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.248357</td>\n",
       "      <td>-0.069132</td>\n",
       "      <td>0.323844</td>\n",
       "      <td>0.761515</td>\n",
       "      <td>-0.117077</td>\n",
       "      <td>-0.117068</td>\n",
       "      <td>0.789606</td>\n",
       "      <td>0.383717</td>\n",
       "      <td>-0.234737</td>\n",
       "      <td>0.271280</td>\n",
       "      <td>-0.678495</td>\n",
       "      <td>-0.305499</td>\n",
       "      <td>-0.597381</td>\n",
       "      <td>0.110418</td>\n",
       "      <td>1.197179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.231709</td>\n",
       "      <td>-0.232865</td>\n",
       "      <td>0.120981</td>\n",
       "      <td>-0.956640</td>\n",
       "      <td>-0.862459</td>\n",
       "      <td>-0.281144</td>\n",
       "      <td>-0.506416</td>\n",
       "      <td>0.157124</td>\n",
       "      <td>-0.454012</td>\n",
       "      <td>-0.706152</td>\n",
       "      <td>-0.771042</td>\n",
       "      <td>1.000820</td>\n",
       "      <td>-0.781672</td>\n",
       "      <td>-0.847627</td>\n",
       "      <td>0.818595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.732824</td>\n",
       "      <td>-0.112888</td>\n",
       "      <td>0.033764</td>\n",
       "      <td>-0.712374</td>\n",
       "      <td>-0.272191</td>\n",
       "      <td>0.055461</td>\n",
       "      <td>-0.575497</td>\n",
       "      <td>0.187849</td>\n",
       "      <td>-0.300319</td>\n",
       "      <td>-0.145847</td>\n",
       "      <td>0.921936</td>\n",
       "      <td>0.851410</td>\n",
       "      <td>-1.315797</td>\n",
       "      <td>-0.465951</td>\n",
       "      <td>0.822989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.300853</td>\n",
       "      <td>0.926139</td>\n",
       "      <td>-0.006749</td>\n",
       "      <td>-0.528855</td>\n",
       "      <td>0.411272</td>\n",
       "      <td>-0.610422</td>\n",
       "      <td>0.104432</td>\n",
       "      <td>-0.979835</td>\n",
       "      <td>-0.664093</td>\n",
       "      <td>0.098431</td>\n",
       "      <td>0.041542</td>\n",
       "      <td>-1.073693</td>\n",
       "      <td>0.458318</td>\n",
       "      <td>-0.714807</td>\n",
       "      <td>1.794525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.369233</td>\n",
       "      <td>0.085684</td>\n",
       "      <td>-0.057824</td>\n",
       "      <td>-0.150552</td>\n",
       "      <td>-0.739261</td>\n",
       "      <td>-0.359922</td>\n",
       "      <td>-0.230319</td>\n",
       "      <td>0.528561</td>\n",
       "      <td>0.171809</td>\n",
       "      <td>-0.881520</td>\n",
       "      <td>1.544841</td>\n",
       "      <td>0.604097</td>\n",
       "      <td>1.361007</td>\n",
       "      <td>0.064791</td>\n",
       "      <td>0.765437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.248357  -0.069132   0.323844   0.761515  -0.117077  -0.117068   \n",
       "1  -0.231709  -0.232865   0.120981  -0.956640  -0.862459  -0.281144   \n",
       "2   0.732824  -0.112888   0.033764  -0.712374  -0.272191   0.055461   \n",
       "3  -0.300853   0.926139  -0.006749  -0.528855   0.411272  -0.610422   \n",
       "4   0.369233   0.085684  -0.057824  -0.150552  -0.739261  -0.359922   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  feature_11  feature_12  \\\n",
       "0   0.789606   0.383717  -0.234737    0.271280   -0.678495   -0.305499   \n",
       "1  -0.506416   0.157124  -0.454012   -0.706152   -0.771042    1.000820   \n",
       "2  -0.575497   0.187849  -0.300319   -0.145847    0.921936    0.851410   \n",
       "3   0.104432  -0.979835  -0.664093    0.098431    0.041542   -1.073693   \n",
       "4  -0.230319   0.528561   0.171809   -0.881520    1.544841    0.604097   \n",
       "\n",
       "   feature_13  feature_14  feature_15  target  \n",
       "0   -0.597381    0.110418    1.197179       1  \n",
       "1   -0.781672   -0.847627    0.818595       0  \n",
       "2   -1.315797   -0.465951    0.822989       0  \n",
       "3    0.458318   -0.714807    1.794525       0  \n",
       "4    1.361007    0.064791    0.765437       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 informative features (correlated with the target)\n",
    "X_informative = np.random.randn(1000, 10) * 0.5 \n",
    "# Generate 5 noise features (not correlated with the target)\n",
    "X_noise = np.random.randn(1000, 5)\n",
    "\n",
    "# Generate binary target variable based on informative features\n",
    "target = (X_informative.sum(axis=1) + np.random.randn(1000) * 0.1 > 0.3).astype(int)\n",
    "\n",
    "# Combine informative and noise features\n",
    "X = np.hstack([X_informative, X_noise])\n",
    "feature_names = [f'feature_{i+1}' for i in range(X.shape[1])]\n",
    "data = pd.DataFrame(X, columns=feature_names)\n",
    "data['target'] = target\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    569\n",
      "1    431\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Why Feature Selection?\n",
    "\n",
    "In real datasets, we often have many features, some of which do not contribute to predicting the target variable. Including unnecessary features can:\n",
    "\n",
    "1. Increase model complexity and risk of overfitting.\n",
    "\n",
    "2. Reduce model interpretability.\n",
    "\n",
    "3. Slow down training time and make the model less accurate.\n",
    "\n",
    "By selecting only the most important features, we can create simpler, faster, and often more accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Techniques for Feature Selection\n",
    "\n",
    "We'll explore three main types of feature selection techniques:\n",
    "\n",
    "1. **Filter Methods**\n",
    "\n",
    "2. **Wrapper Methods**\n",
    "\n",
    "3. **Embedded Methods**\n",
    "\n",
    "Each has different strengths, and understanding when to use each is important.\n",
    "\n",
    "\n",
    "### 3.1 Filter Methods\n",
    "\n",
    "Filter methods use statistical measures to assess the importance of each feature independently of the model.\n",
    "\n",
    "#### Example: Correlation-based Feature Selection\n",
    "\n",
    "We can calculate the correlation of each feature with the target variable and remove those with low correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation: target        1.000000\n",
      "feature_8     0.285276\n",
      "feature_9     0.279393\n",
      "feature_2     0.267687\n",
      "feature_5     0.261309\n",
      "feature_3     0.258959\n",
      "feature_4     0.242934\n",
      "feature_10    0.235953\n",
      "feature_6     0.218040\n",
      "feature_1     0.209317\n",
      "feature_7     0.186391\n",
      "feature_15    0.045910\n",
      "feature_14    0.031970\n",
      "feature_11    0.015926\n",
      "feature_13    0.005452\n",
      "feature_12    0.001614\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlations = data.corr()['target'].abs().sort_values(ascending=False)\n",
    "print('correlation:',correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Wrapper Methods\n",
    "\n",
    "Wrapper methods evaluate feature subsets by actually training and validating models on them. This approach is more computationally expensive but often provides better results.\n",
    "\n",
    "#### Example: Recursive Feature Elimination (RFE)\n",
    "\n",
    "RFE is a popular wrapper method that recursively removes the least important features, training the model multiple times to rank the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  rank\n",
      "0    feature_1     1\n",
      "1    feature_2     1\n",
      "2    feature_3     1\n",
      "3    feature_4     1\n",
      "4    feature_5     1\n",
      "5    feature_6     1\n",
      "6    feature_7     1\n",
      "7    feature_8     1\n",
      "8    feature_9     1\n",
      "9   feature_10     1\n",
      "14  feature_15     2\n",
      "10  feature_11     3\n",
      "11  feature_12     4\n",
      "13  feature_14     5\n",
      "12  feature_13     6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Define the model using linear regression\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define RFE with the model and specify the number of features to select\n",
    "rfe = RFE(model, n_features_to_select=10)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "# Fit RFE to the data\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get the ranking of features\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'rank': rfe.ranking_\n",
    "}).sort_values(by='rank')\n",
    "\n",
    "print(feature_ranking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Embedded Methods\n",
    "\n",
    "Embedded methods perform feature selection as part of the model training process itself. These methods are usually faster than wrappers and more accurate than filters.\n",
    "\n",
    "#### Example: Feature Importance from a Tree-based Model\n",
    "\n",
    "Tree-based algorithms like **Random Forest** and **Gradient Boosting** calculate feature importance as part of their training. Here we use  the Random Forest model to compute feature importance by training an ensemble of decision trees and evaluating how much each feature contributes to reducing impurity at each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  importance\n",
      "7    feature_8    0.105836\n",
      "4    feature_5    0.094658\n",
      "1    feature_2    0.092188\n",
      "2    feature_3    0.085131\n",
      "8    feature_9    0.084862\n",
      "3    feature_4    0.082718\n",
      "9   feature_10    0.078795\n",
      "0    feature_1    0.072479\n",
      "5    feature_6    0.067805\n",
      "6    feature_7    0.062819\n",
      "10  feature_11    0.036191\n",
      "14  feature_15    0.036125\n",
      "13  feature_14    0.035001\n",
      "12  feature_13    0.032843\n",
      "11  feature_12    0.032550\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluating Model Performance\n",
    "\n",
    "Finally, we’ll test the performance of a model built with and without feature selection to see the impact. For simplicity, we’ll use a linear regression classifier:\n",
    "\n",
    "1. **Without Feature Selection**: Train a model using all features.\n",
    "\n",
    "2. **With Feature Selection**: Train a model using only the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without feature selection: 0.97\n",
      "Accuracy with feature selection: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Model without feature selection\n",
    "model_all_features = LogisticRegression(max_iter=200)\n",
    "model_all_features.fit(X_train, y_train)\n",
    "predictions_all = model_all_features.predict(X_test)\n",
    "accuracy_all = accuracy_score(y_test, predictions_all)\n",
    "print(f'Accuracy without feature selection: {accuracy_all:.2f}')\n",
    "\n",
    "# Model with selected features (top 5 from RFE)\n",
    "selected_features = feature_ranking[feature_ranking['rank'] == 1]['feature']\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "model_selected_features = LogisticRegression(max_iter=200)\n",
    "model_selected_features.fit(X_train_selected, y_train)\n",
    "predictions_selected = model_selected_features.predict(X_test_selected)\n",
    "accuracy_selected = accuracy_score(y_test, predictions_selected)\n",
    "\n",
    "print(f'Accuracy with feature selection: {accuracy_selected:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     feature_1\n",
       "1     feature_2\n",
       "2     feature_3\n",
       "3     feature_4\n",
       "4     feature_5\n",
       "5     feature_6\n",
       "6     feature_7\n",
       "7     feature_8\n",
       "8     feature_9\n",
       "9    feature_10\n",
       "Name: feature, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Key Takeaways\n",
    "\n",
    "1. **Feature Selection** is essential for building efficient models, especially with large datasets.\n",
    "\n",
    "2. **Filter methods** are fast and easy but don't consider feature interactions.\n",
    "\n",
    "3. **Wrapper methods** provide good results but can be slow.\n",
    "\n",
    "4. **Embedded methods** are a good balance, leveraging the model’s structure to determine feature importance.\n",
    "\n",
    "By selecting features carefully, we can build simpler, faster, and potentially more accurate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: **Combining Feature Selection Methods**\n",
    "\n",
    "1. Generate the synthetic dataset of your project.\n",
    "\n",
    "2. First, use the filter method (correlation-based) to remove irrelevant features.\n",
    "\n",
    "3. Then, apply RFE to further eliminate any unnecessary features.\n",
    "\n",
    "4. Finally, train a Random Forest model and use the feature importance scores to select the top features.\n",
    "\n",
    "5. Compare and evaluate the model performance at each step when (using all features, after filtering, after RFE, after feature importance selection).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Task 2: **Visualizing Feature Selection**\n",
    "\n",
    "1. Use the synthetic dataset you generates in task 1.\n",
    "\n",
    "2. Apply any feature selection technique (e.g., correlation-based, RFE, or Random Forest importance).\n",
    "\n",
    "3. Plot the accuracy of the model with the different numbers of features selected (Create a plot showing the accuracy vs. the number of features used).\n",
    "\n",
    "4. Use bar plots to show how the importance of features changes during the selection process.\n",
    "\n",
    "5. Observe and analyze how feature selection impacts the performance visually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
