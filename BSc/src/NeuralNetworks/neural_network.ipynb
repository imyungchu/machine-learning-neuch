{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b854ad6c",
   "metadata": {},
   "source": [
    "# LAB 4: NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b5ca6d",
   "metadata": {},
   "source": [
    "In this lab, we will explore Neural Networks, which can be trained using the Gradient Descent algorithm.\n",
    "\n",
    "Specifically, we will cover the following topics:\n",
    "\n",
    "- A brief introduction to the PyTorch library for implementing Neural Networks.\n",
    "\n",
    "- Implementation of a neural network using PyTorch.\n",
    "\n",
    "- Discussion on overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a4b766",
   "metadata": {},
   "source": [
    "**A neuron is like a function; it takes a few inputs and calculates an output. Its adjusts its parameters (training), using the gradient descent method to minimise a loss function**.\n",
    "\n",
    "\n",
    "The circle below illustrates an artificial neuron.\n",
    "\n",
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"../../fig/NN1.png\" width=\"800\"/>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df075b",
   "metadata": {},
   "source": [
    "At the far left we see two input values plus a bias value. The input values are 1 and 0 (the green numbers), while the bias holds a value of -2 (the brown number).\n",
    "\n",
    "The two inputs are then multiplied by their  weights, which are 7 and 3 (the blue numbers).\n",
    "\n",
    "Finally we add it up with the bias and end up with a number, in this case: 5 (the red number). This is the input for our artificial neuron.\n",
    "\n",
    "The neuron then performs some kind of computation on this number using activation function ( in our case the Sigmoid function), and then spits out an output. This happens to be 1, as Sigmoid of 5 equals to 1, if we round the number up (more about activation function down).\n",
    "\n",
    "You can find more explanation about the structure [here](https://www.leewayhertz.com/what-are-neural-networks/#What-are-neural-networks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad5639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d631254c",
   "metadata": {},
   "source": [
    "# Single layer network : Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84ac16",
   "metadata": {},
   "source": [
    "The perceptron algorithm was one of the first algorithms used to implement a simple neural network.\n",
    "\n",
    "Perceptrons are supervised learning algorithms and is a type of an ANN\n",
    "\n",
    "Let's remind th components of a Perceptron:\n",
    "    \n",
    "- Inputs: The perceptron takes several inputs $(x_1,x_2,…,x_n)$\n",
    "\n",
    "\n",
    "- Weights: Each input is associated with a weight $(w_1,w_2,…,w_n).$\n",
    "\n",
    "- Bias: A bias term $(b or w_0)$ is added to shift the decision boundary.\n",
    "\n",
    "- Activation Function: The perceptron uses a step function (a simple thresholding function) to determine whether the weighted sum of inputs plus the bias is above or below a certain threshold.\n",
    "\n",
    "- Binary Output: The output of a perceptron is binary (-1 or 1 that can be easily switched to 0 or 1), making it suitable for linearly separable classification problems.\n",
    "\n",
    "### Limitations of Perceptron:\n",
    "\n",
    "- Linear separability: The perceptron can only solve problems that are linearly separable (i.e., it can only classify data that can be separated by a straight line or hyperplane). It cannot solve more complex problems like XOR.\n",
    "\n",
    "- Single-layer model: The original perceptron is a single-layer model and does not have hidden layers, limiting its expressiveness.\n",
    "\n",
    ".\n",
    "\n",
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"../../fig/perceptron.png\" width=\"800\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81738f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc0cc07e",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "Activation functions play an integral role in neural networks by introducing non-linearity.\n",
    "This non-linearity allows neural networks to develop complex representations and functions based on the inputs that would not be possible with a simple linear layers.\n",
    "\n",
    "You can find all types of activation functions [here](https://www.shiksha.com/online-courses/articles/activation-functions-with-real-life-analogy-and-python-code/)\n",
    "\n",
    "Unlike the perceptron, which uses a simple step function for activation, neurons in modern neural networks can use a variety of activation functions, such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7406f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"../../fig/activation_function.png\" width=\"800\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00ddff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "440ff10a",
   "metadata": {},
   "source": [
    "# Multi-layer Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b54d8f",
   "metadata": {},
   "source": [
    "Multi-layer Networks: Neurons are part of more sophisticated architectures called multi-layer perceptrons (MLPs) or deep neural networks, where neurons are organized into layers (input layer, hidden layers, and output layer). Each layer performs computations, and the output of one layer is fed as input to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994be2e3",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"../../fig/MLP.png\" width=\"800\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33f5be",
   "metadata": {},
   "source": [
    "Each hidden/output node is now related to all the nodes of the previous layer: we say that the network is fully connected.\n",
    "\n",
    "\n",
    "###  Inputs to the 2nd Hidden Layer\n",
    "The inputs to a neuron in the 2nd hidden layer come from the outputs of the neurons in the 1st hidden layer.\n",
    "\n",
    "Here we have:\n",
    "- $ h_1^1, h_1^2, h_1^3,h_1^4 $ represent the activations (outputs) of the neurons in the **1st hidden layer**. These are the inputs to the neurons in the 2nd hidden layer.\n",
    ".\n",
    "\n",
    "- Let's assume that $ w_{ij}^{(3)} $ represent the weights connecting neuron $ i $ in the 1st hidden layer to neuron $ j $ in the 2nd hidden layer.\n",
    ".\n",
    "\n",
    "- $ b_j^{2} $ is the bias for neuron $ j $ in the 2nd hidden layer.\n",
    "\n",
    "\n",
    "\n",
    "### Weighted Sum for a Neuron in the 2nd Hidden Layer\n",
    "The value of the weighted sum for neuron $ j $ in the 2nd hidden layer is:\n",
    "\n",
    "$$\n",
    "a_j^{2} = \\sum_{i=1}^{4} w_{ij}^{2} h_i^{1} + b_j^{2}\n",
    "$$\n",
    "\n",
    "\n",
    "After computing the weighted sum $a_j^{2}$, we apply an activation function $ f(a_j^{2}) $ to compute the final output (activation) of the neuron. \n",
    "\n",
    "\n",
    "So the output (activation) of neuron $ j $ in the 2nd hidden layer is:\n",
    "\n",
    "\n",
    "$$\n",
    "h_j^{2} = f\\left(\\sum_{i=1}^{4} w_{ij}^{2} h_i^{1} + b_j^{2} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "If we are using an activation function like ReLU, sigmoid, or tanh, the output of neuron $ j $ in the 2nd hidden layer would be:\n",
    "\n",
    "- **Sigmoid**:\n",
    "  $$\n",
    "  h_j^{2} = \\frac{1}{1 + e^{-a_j^{2}}}\n",
    "  $$\n",
    "- **ReLU**:\n",
    "  $$\n",
    " h_j^{2} = \\max(0, a_j^{2})\n",
    "  $$\n",
    "- **Tanh**:\n",
    "  $$\n",
    " h_j^{2} = \\tanh(a_j^{2}) = \\frac{e^{a_j^{2}} - e^{-a_j^{2}}}{e^{a_j^{2}} + e^{-a_j^{2}}}\n",
    "  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de74114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8823cc36",
   "metadata": {},
   "source": [
    "#  Training the Neural Network\n",
    "\n",
    "To train a NN, we need to follow those process: \n",
    "\n",
    "- Forward pass: Compute the outputs.\n",
    "\n",
    "- Compute loss: Compare outputs with actual labels.\n",
    "\n",
    "- Backward pass: Compute gradients of loss with respect to weights and bias.\n",
    "\n",
    "- Optimize: Update weights using the optimizer (e.g., Adam, SGD).\n",
    "\n",
    "- Repeat for many epochs, using batches of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037561c",
   "metadata": {},
   "source": [
    "### Forward pass \n",
    "\n",
    "The action of obtaining the output y from the input x is called a forward pass.\n",
    "\n",
    "In the forward pass, the input values are multiplied by the weights, biases are added, and the result is passed through an activation function. This process is repeated for every layer, eventually yielding the output of the network.\n",
    "\n",
    "We can implement the forward pass of our multi-layer perceptron in a few lines. We use random weights and bias for the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35482624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sizes = [3, 4, 3, 1]  # number of neurons in each layer\n",
    "num_layers = len(sizes)  # one input layer, 2 hidden layers and one output layer\n",
    "\n",
    "# the first layer does not have bias as it's the input layer\n",
    "biases = [np.random.randn(sizes[i], 1) for i in range(1, num_layers)]\n",
    "\n",
    "# we use the transpose of the weight matrices cause we need the same size as the bias vector\n",
    "weights = [np.random.randn(sizes[i], sizes[i-1]) for i in range(1, num_layers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4cb631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.07763987,  1.3355706 , -1.43261954],\n",
       "        [-0.09855782,  1.2955058 ,  0.72416075],\n",
       "        [ 0.10502936, -1.67961677,  1.40173685],\n",
       "        [-0.04502352,  1.08057925, -1.03064786]]),\n",
       " array([[ 0.04128528,  0.06089792, -0.6319817 ,  1.06822963],\n",
       "        [ 0.15103529, -0.29569492, -1.13376815,  1.00328582],\n",
       "        [ 0.1006686 , -1.52056104,  1.06255129,  1.87065629]]),\n",
       " array([[0.59044125, 2.16752136, 0.10428971]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685374d",
   "metadata": {},
   "source": [
    "Once we have initialized the network size and its parameters, we are ready to implement the forward pass. We can then feed the network with any arbitrary input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd2ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60611854]]\n"
     ]
    }
   ],
   "source": [
    "# Define an activation function\n",
    "def g(z):\n",
    "    \"\"\"Sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0 + np.exp(-z)) \n",
    "\n",
    "# define output/input relation for a MLP\n",
    "def network_forward(h): \n",
    "    \"\"\"Return the output of the network given input h.\"\"\"\n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, h) + b # mat-vect product + bias\n",
    "        h = g(z) # apply activation function\n",
    "    return h\n",
    "\n",
    "# Example input\n",
    "input = np.array([[1.],[2.],[3.]]) # a column vector (1,2,3)^T\n",
    "output = network_forward(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd13b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29bc0457",
   "metadata": {},
   "source": [
    "### Compute the Loss/Cost function:\n",
    "\n",
    "The loss function measures how far off the network’s predictions are from the true labels.\n",
    "Some common loss functions are:\n",
    "\n",
    "- Mean Squared Error (MSE) for regression tasks.\n",
    "\n",
    "- Cross-Entropy Loss for classification tasks (either binary or multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9e0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb9933",
   "metadata": {},
   "source": [
    "### Backward Pass (Backpropagation):\n",
    "\n",
    "The backward pass involves calculating the gradients of the loss with respect to the weights and biases in each layer.\n",
    "\n",
    "The gradients are then used to update the weights and biases to minimize the loss (i.e., make the model’s predictions more accurate).\n",
    "\n",
    "This process is called backpropagation cause it uses the chain rule from calculus to compute the gradients layer by layer, starting from the output layer and moving backward through the network\n",
    "\n",
    ".\n",
    "\n",
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"../../fig/bp.png\" width=\"800\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cc67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4476407",
   "metadata": {},
   "source": [
    "# Implement NN with pytorch\n",
    "\n",
    "PyTorch is one of the most widely used libraries for developing deep learning models. Tensors are one of the fundamental components of PyTorch. You can think of tensors as similar to NumPy arrays.\n",
    "\n",
    "Using tensors, PyTorch can create computational graphs and calculate gradients.\n",
    "\n",
    "We will cover the basic concepts of PyTorch and provide further explanations as we progress through the implementation.\n",
    "\n",
    "you can find the [PyTorch tutorials here](https://pytorch.org/tutorials/beginner/basics/intro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85eae8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 KB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /home/olethros/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.0 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b35a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59271ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_1 shape:  torch.Size([2, 2])\n",
      "tensor_2 shape:  torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "## Transform array to tensor with pytorch\n",
    "\n",
    "tensor_1 = torch.tensor([[1., 2.], \n",
    "                         [3., 3.]])\n",
    "\n",
    "tensor_2 = torch.tensor([[1., 0], \n",
    "                         [-1., 0]])\n",
    "\n",
    "print(\"tensor_1 shape: \", tensor_1.shape)\n",
    "print(\"tensor_2 shape: \", tensor_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c9bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Element Multiplication\n",
    "\n",
    "elem_mul = tensor_1 * tensor_2\n",
    "print(\"Element wise multiplication\")\n",
    "print(\"Result shape: \", elem_mul.size())\n",
    "elem_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ffe8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "matrix_mul = torch.matmul(tensor_1,tensor_2)\n",
    "print(\"Matrix multiplication\")\n",
    "print(\"Result shape: \", matrix_mul.size())\n",
    "torch.matmul(tensor_1,tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tensor as numpy array\n",
    "tensor_1.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers manually\n",
    "input_size = 3\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 3\n",
    "output_size = 2\n",
    "\n",
    "# Create the layers\n",
    "fc1 = nn.Linear(input_size, hidden_size1)  # Input to hidden layer\n",
    "fc2 = nn.Linear(hidden_size1, hidden_size2)  # Hidden layer 1 to hidden layer 2\n",
    "fc3 = nn.Linear(hidden_size2, output_size)  # Hidden layer 2 to output layer\n",
    "\n",
    "# Define activation functions\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD([fc1.weight, fc1.bias, fc2.weight, fc2.bias, fc3.weight, fc3.bias], lr=0.01)\n",
    "\n",
    "# Example input (2 samples, 3 features each)\n",
    "inputs = torch.tensor([[0.1, 0.2, 0.3],\n",
    "                       [0.4, 0.5, 0.6]], dtype=torch.float32)\n",
    "\n",
    "#  true output labels\n",
    "labels = torch.tensor([[0, 1],\n",
    "                       [1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 200\n",
    "loss_values = []  # List to store loss at each epoch\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = fc1(inputs)  # Linear combination in hidden layer 1\n",
    "    a1 = relu(z1)     # Apply ReLU activation\n",
    "    z2 = fc2(a1)  # Linear combination in hidden layer 2\n",
    "    a2 = relu(z2)     # Apply ReLU activation again\n",
    "    z3 = fc3(a2)      # Linear combination in output layer\n",
    "    outputs = sigmoid(z3)  # Apply Sigmoid activation in the output layer\\\n",
    "    \n",
    "    # Convert probabilities to binary predictions (0 or 1)\n",
    "    binary_outputs = torch.round(outputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store the loss value for this epoch\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    # Print loss every 100 epochs for monitoring\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Outputs (Probabilities):\\n\", outputs)\n",
    "print()\n",
    "print(\"Binary Outputs (0 or 1):\\n\", binary_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over epochs\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b2f62",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "- Modify the neural network to include more hidden layers (e.g., add another hidden layer with different size).\n",
    "\n",
    "- Experiment with different activation functions (e.g., Tanh, LeakyReLU, etc.) in the hidden layers and observe the effect on output probabilities.\n",
    "\n",
    "- Experiment with different optimizers such as Adam, RMSprop, and Adagrad in place of SGD, and observe their effect on model performance.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "- How does changing the number of hidden layers affect the model's ability to classify the data?\n",
    "\n",
    "- What happens if you use a different activation function in the hidden layers instead of ReLU (e.g., Tanh or LeakyReLU)? Show the effect on the binary outputs.\n",
    "\n",
    "- How does changing the optimizer affect the convergence of the model?\n",
    "\n",
    "- Which optimizer performed best for this dataset, and why do you think that is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0db663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99337b60",
   "metadata": {},
   "source": [
    "# Let's try with a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f764d5",
   "metadata": {},
   "source": [
    "### Breast Cancer  Dataset\n",
    "\n",
    "The **Breast Cancer  Dataset** is a widely-used dataset for binary classification tasks in machine learning, provided by the `sklearn` library. It contains data from breast cancer, with the goal of classifying tumors as **malignant** (cancerous) or **benign** (non-cancerous).\n",
    "\n",
    "- **Number of samples:** 569\n",
    "\n",
    "- **Number of features:** 30 numeric features\n",
    "\n",
    "- **Target classes:** \n",
    "  - 0 = malignant (212 samples)\n",
    "  - 1 = benign (357 samples)\n",
    "  \n",
    "- **Features:** Each feature represents a characteristic of cell nuclei (e.g., radius, texture, smoothness), calculated from digitized images of breast masses. The features include mean, standard error, and worst (largest) values of each characteristic.\n",
    "\n",
    "You can find more about this dataset [here](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9400d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01160855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the feature data\n",
    "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "\n",
    "# Add the target labels to the DataFrame\n",
    "df['target'] = data.target\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e192e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets (60% train, 20% val, 20% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53836d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset = TensorDataset(X_train, y_train)  \n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape (n_samples, 1)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)      \n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)    \n",
    "\n",
    "# unsqueeze(1) is added because it is often required for compatibility with neural network layers \n",
    "#that expect a 2D input for the target tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f7287",
   "metadata": {},
   "source": [
    "### Batches\n",
    "\n",
    "When we train neural network we usually train our model using batches of training example.\n",
    "\n",
    "\n",
    "Batching method is performed for different reasons. First of all in we have a huge dataset we cannot fit all the data in our memery. Moreover, using batches requires less time to fit the model as in the same time you can perform more update in the weights of the model. \n",
    "\n",
    "So batch size is an additional hyperparameter of our training algorithm.\n",
    "\n",
    "PyTorch provides two main data classes: DataLoader and Dataset to handle your data.\n",
    "\n",
    "Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the batches.\n",
    "\n",
    "you can read more at: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "Let's continue😅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db690c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for mini-batch training\n",
    "batch_size = 32\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)  \n",
    "val_dataset = TensorDataset(X_val, y_val)        \n",
    "test_dataset = TensorDataset(X_test, y_test)     \n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e78901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define the neural network structure\n",
    "input_size = X_train.shape[1]  # Number of features in the dataset\n",
    "hidden_size1 = 16\n",
    "hidden_size2 = 8\n",
    "output_size = 1  # Binary classification (0 or 1)\n",
    "\n",
    "# Create the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size1, hidden_size2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size2, output_size),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train the model\n",
    "epochs = 1000\n",
    "loss_values, train_accuracies, val_accuracies,val_losses = [], [], [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)  \n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    # Validation loss calculation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_X, val_y in val_loader:\n",
    "            val_outputs = model(val_X)\n",
    "            val_loss += criterion(val_outputs, val_y).item()\n",
    "        val_loss /= len(val_loader)  # Average loss for validation\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Accuracy calculations\n",
    "        train_accuracy = (model(X_train).round().eq(y_train).sum() / y_train.size(0)).item()\n",
    "        val_accuracy = (model(X_val).round().eq(y_val).sum() / y_val.size(0)).item()\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_accuracy:.2f}, Val Acc: {val_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_values, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss', linestyle='--')  # Validation loss in dashed line\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Evaluate the model on test data\n",
    "with torch.no_grad():\n",
    "    test_accuracy = (model(X_test).round().eq(y_test).sum() / y_test.size(0)).item()\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27cb73",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6630a",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "- Modify the existing model architecture (e.g., add more layers, change the number of neurons).\n",
    "\n",
    "- Experiment with different activation functions (e.g., Tanh, Leaky ReLU).\n",
    "\n",
    "RETRAIN THE MODEL AND COMMENT THE DIFFERENCE IF IT EXISTS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d5594",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning:\n",
    "\n",
    "- Change the learning rate, batch size, and the number of epochs.\n",
    "\n",
    "- Compare the effects of different optimizers (e.g., Adam, RMSprop) on model performance.\n",
    "\n",
    "Regularization Techniques:\n",
    "\n",
    "- Implement L2 regularization (weight decay) in the optimizer.\n",
    "\n",
    "- Introduce dropout layers to prevent overfitting and discuss their impact on training and validation accuracy.\n",
    "\n",
    "To add L2 regularization or dropout, you can modify the optimizer or the model structure ( dropout is added after the activation function in the model architectiure, Weight decay is added inside the regularization function (SGD or ADAM))\n",
    "\n",
    "SAVE THE MODEL ARCHITECTURE YOU DID BEFORE, RETRAIN THE MODEL AND COMMENT THE DIFFERENCE IF IT EXISTS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8f7f2",
   "metadata": {},
   "source": [
    "### Early Stopping:\n",
    "Early stopping is an optimization technique used to reduce overfitting without compromising on model accuracy. The main idea behind early stopping is to stop training before a model starts to over-fit.\n",
    "\n",
    "- Implement early stopping based on validation loss to avoid overfitting.\n",
    "\n",
    "- Discuss the importance of early stopping in training deep learning models.\n",
    "\n",
    "SAVE ALL YOUR CHANGES AND ADD EARLY STOPPING, RETRAIN THE MODEL AND COMMENT THE DIFFERENCE IF IT EXISTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15159de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2ee63d8",
   "metadata": {},
   "source": [
    "# TASK 2: Build and Evaluate a Regression Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079188b1",
   "metadata": {},
   "source": [
    "**Objective**: Build a regression model using the class dataset to predict the weight. Evaluate the model’s performance using various metrics.\n",
    "\n",
    "### Steps to Follow\n",
    "\n",
    "1. **Data Loading**: \n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - Encode the categorical data\n",
    "   \n",
    "   - Apply oversampling on minority class for data balance\n",
    "   \n",
    "   - Split the dataset into training, validation, and test sets (e.g., 70% training, 15% validation, 15% test).\n",
    "   \n",
    "   - Normalize the features using standard scaling or min-max scaling.\n",
    "   \n",
    "   - Convert the datasets into PyTorch tensors.\n",
    "\n",
    "3. **Model Development**:\n",
    "\n",
    "   - Define a neural network architecture for regression. \n",
    "   \n",
    "   - Use appropriate activation functions and output layers.\n",
    "   \n",
    " \n",
    "4. **Loss Function and Optimizer**:\n",
    "\n",
    "   - Use Mean Squared Error (MSE) loss for regression.\n",
    "   \n",
    "   - Choose an optimizer (e.g., Adam or SGD).\n",
    "   \n",
    "   - Example:\n",
    "     ```python\n",
    "     criterion = nn.MSELoss()\n",
    "     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "     ```\n",
    "\n",
    "5. **Training the Model**:\n",
    "\n",
    "    - No need to use Batches because you don't have a big dataset\n",
    "\n",
    "   - Train the model over a specified number of epochs.\n",
    "   \n",
    "   - Store loss values for visualization.\n",
    "   \n",
    "   - Print the training and validation loss at the end of each epoch.\n",
    "\n",
    "6. **Model Evaluation**:\n",
    "\n",
    "   - Evaluate the model on the test set and calculate the following metrics:\n",
    "   \n",
    "     - Mean Absolute Error (MAE)\n",
    "     - Mean Squared Error (MSE)\n",
    "     \n",
    "   - Example of calculating metrics:\n",
    "     ```python\n",
    "     from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "     with torch.no_grad():\n",
    "         model.eval()\n",
    "         y_pred = model(torch.tensor(X_test, dtype=torch.float32)).numpy()\n",
    "     \n",
    "     mae = mean_absolute_error(y_test, y_pred)\n",
    "     mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "     ```\n",
    "\n",
    "7. **Visualization**:\n",
    "\n",
    "   - Plot the training and validation loss over epochs to visualize training progress.\n",
    "   \n",
    "   - Create scatter plots of predicted vs. actual values to assess model performance visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d347893e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
