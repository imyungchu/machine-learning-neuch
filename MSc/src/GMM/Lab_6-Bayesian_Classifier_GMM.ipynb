{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Bayesian Classifier and Gaussian Mixture Models\n",
    "\n",
    "In the 5th assigment of the class we will study the Bayes Classifier.  \n",
    "\n",
    "We will start with a quick demonstration of Gaussian Mixture Models with sklearn.\n",
    "\n",
    "You have to implement two different variants of the bayes classifier using the assumption that the conditional probability P(x|y):\n",
    "   1. Gaussian Bayes Classifier. P(x|y) ~ Gaussian Distribution (Normal)\n",
    "   2. Gaussian Mixture Bayes Classifier. P(x|y) ~ Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Demonstration Gaussian mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 . Probability density function (pdf) of Multivariate Gaussian Distribution:\n",
    "\n",
    "$${ p(x_{1},\\ldots ,x_{k}; \\mu,\\Sigma)= \\mathcal{N}(\\mathbf {x} | \\mu, \\Sigma)= {\\frac {\\exp \\left(-{\\frac {1}{2}}\\left({\\mathbf {x} }-{\\boldsymbol {\\mu }}\\right)^{\\mathrm {T} }{\\boldsymbol {\\Sigma }}^{-1}\\left({\\mathbf {x} }-{\\boldsymbol {\\mu }}\\right)\\right)}{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma }}|}}}}$$\n",
    "\n",
    "$$\\mathbf {x} \\in \\mathbb{R}^d \\text{: feature vector} $$\n",
    "$$\\Sigma \\in \\mathbb{R}^{d \\times d}  \\text{: is the covariance matrix} $$\n",
    "$$\\mu \\in \\mathbb{R}^d \\text{: mean of the distribution} $$\n",
    "\n",
    "The value of the density at a particular point $\\mathbf {x}$ represent the relative likelihood of observing that particular value in a continuous random variable's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:42:22.603545Z",
     "start_time": "2024-10-31T16:42:22.524453Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal(mean=[1, 1], \n",
    "                                  cov=[[1, 0],\n",
    "                                       [0, 1]],\n",
    "                                  size=200)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlim(-4,6)\n",
    "plt.ylim(-4,6)\n",
    "plt.title(\"Data\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infer the parameter of the distribution we can simply estimate the mean and the covarience from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the multi variate normal distribution with scipy\n",
    "mean_est = np.mean(X,axis=0)\n",
    "cov_est = np.cov(X, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn\n",
    "dist = mvn(mean=mean_est, cov=cov_est)\n",
    "x = [0, 0]\n",
    "print(\"CDF:\", dist.pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. in the case of mixture of gaussian model the final Probability density function is as bellow:\n",
    "\n",
    "$$p_{GMM}(x_{1},\\ldots ,x_{d})= \\sum_{i=1}^{k} w_k p(x_{1},\\ldots ,x_{k};  \\mu_i,\\Sigma_i),s.t  \\sum_{i=1}^{k} w_k  = 1$$\n",
    "\n",
    "\n",
    "The model can be fit using the EM algorithm to find our parameters :\n",
    " 1. $ w_k $ : weight of each gaussian, or the prior probability of the gaussian k, p(k)\n",
    " 1. $ \\mu_k $ : the mean of each the gaussian \n",
    " 1. $ \\Sigma_k $: the covariance of each gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets._samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate some data\n",
    "X, y_true = make_blobs(n_samples=4000, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=4)\n",
    "gmm.fit(X) # find the parameters of each gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights of each Gaussian i.e p(k)\n",
    "w_k = gmm.weights_\n",
    "w_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of each gaussian\n",
    "means = gmm.means_\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of each gaussian\n",
    "cov = gmm.covariances_\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], label=\"mean\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Density Function of Gaussian Mixture Model\n",
    "\n",
    "$$p_{GMM}(x_{1},\\ldots ,x_{k})= \\sum_{i=1}^{k} w_k p(x_{1},\\ldots ,x_{k};  \\mu_i,\\Sigma_i),s.t  \\sum_{i=1}^{k} w_k  = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf of gmm\n",
    "x = (0, 0)\n",
    "\n",
    "\n",
    "def pdf_mixture(x, w, means, cov):\n",
    "    n_componets = len(means)\n",
    "    p_x = 0\n",
    "    for k in range(n_componets):\n",
    "        p_x += w[k] * mvn(mean=means[k], cov=cov[k]).pdf(x)\n",
    "    return p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_mixture(x, w_k, means, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes 1.  if we use covariance_type=\"diag\" the covariance metrix is diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=4, random_state=0, covariance_type=\"diag\")\n",
    "gmm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the cov in a list with the components of the diagonal\n",
    "gmm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "## Bayesian Classifier with Gaussian Model\n",
    "\n",
    "We begin with the implementation of the Gaussian Naive Bayes Classifier.\n",
    "\n",
    "In the Bayesian Classifier, we use Bayes' theorem to calculate \\( p(y|x) \\):\n",
    "\n",
    "$$ \n",
    "p(y|x) = \\frac{p(x|y) \\cdot p(y)}{p(x)} \n",
    "$$\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$ \n",
    "p(y|x) = \\frac{p(x|y) \\cdot p(y)}{\\sum_{y'} p(x|y') p(y')} \n",
    "$$ \n",
    "\n",
    "In the Gaussian Naive Bayes Classifier, we assume that $ p(x|y) $ follows a Gaussian distribution: \n",
    "\n",
    "$$p(x|y) \\sim \\mathcal{N}(\\mathbf {x} | \\mu, \\Sigma) $$\n",
    "\n",
    "with the parameters $\\mu$ and $\\Sigma$ depending on the class $y$. \n",
    "\n",
    "In addition, the Naive Bayes classifier assumes **independence** of the features conditioned on the label, i.e., a **diagonal covariance matrix** for the Gaussian $ p(x|y) $.\n",
    "\n",
    "So the Naive Bayes Classifier, you need to calculate:\n",
    "\n",
    "1. The prior for each class $p(y)$ according to their empirical values.\n",
    "2. The parameters of the Gaussian distribution for each class $y$ to model $ p(x|y) $ according to their empirical values.\n",
    "3. Use Bayes' theorem to predict $ p(y|x) $.\n",
    "\n",
    "The final prediction of our model can be the class with the maximum probability $p(y|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the class below and use it to:   \n",
    "A. fit your model using .fit() method  \n",
    "B. get the propabilities of each class for the different X using .predict_proba() method  \n",
    "C. make the final predictions using the .predict() method and calculate the accuracy of your model  \n",
    "\n",
    "Hints:\n",
    "So to fit the model want:\n",
    "1. Calculate the prior p(y) by calculating the percentage of samples in each class y.\n",
    "2. Find the parameters (mean and covariance matrix) of the gaussian p(x|y) for **each class y**.   \n",
    "We can do this by calculating the empirical mean and covariance matrix of Data X for each class y.\n",
    "3. then use the bayes theorem to calculate p(y|x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    Implementation of Naive Bayes Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # initialization of required variables.\n",
    "        self.models = None  # p(x|y)\n",
    "        self.prior = None  # p(y)\n",
    "        self.classes = None # the classes\n",
    "        self.n_classes = None # number of classes\n",
    "        self.n_features = None # dimension of the features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the parameters of the gaussian models for each class y i.e P(x|y),\n",
    "        as well as the prior class probabilities P(y).\n",
    "        :param X: The input features\n",
    "        :param y: The labels\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        num_of_points= X.shape[0]\n",
    "        \n",
    "        self.classes = list(set(y))\n",
    "        self.n_classes = len(set(y))\n",
    "        \n",
    "        #\n",
    "        self.prior = np.zeros(self.n_classes)\n",
    "        self.models = []\n",
    "        for y_index in range(self.n_classes):\n",
    "            class_y = # fill your code # set the value for y\n",
    "            \n",
    "            # 1. Calculate the prior P(y) for each class\n",
    "            p_y = #fill your code  \n",
    "            self.prior[y_index] =  p_y\n",
    "            \n",
    "            # 2. Estimate the parameters of the gaussian model P(x|y)\n",
    "            # we will find the parameters of the gaussian for each class\n",
    "            data_x_y = #fill your code        # get the data that only in class y\n",
    "            \n",
    "            mean_x_y = #fill your code        # calculate the mean\n",
    "            cov_x_y = #fill your code         # calculate the covariance matrix, Note that you have to set rowvar=False\n",
    "            diag_cov_x_y = #fill your code    # get the diagonal as the naive bayes implies\n",
    "            \n",
    "            gaussian = #fill your code\n",
    "            self.models += [gaussian] # add the model p(x|y) in your model list\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the posterior probability of each class given the features, i.e. P(y|x).\n",
    "        You can calculate this with Bayes's theorem:\n",
    "                    P(y|x) = P(x|y) P(y) / P(x).\n",
    "                    P(y|x): The Posterior probability\n",
    "                    P(x|y): The Likelihood probability -  the model in self.models[y]\n",
    "                    P(y): Prior probability\n",
    "                    P(x): The evidence \\sum_{y'} P(x|y') P(y')\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # get general information\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        # fill your code to calculate P(y|x)\n",
    "        posterior = \n",
    "        return posterior\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predicted class with the maximum posterior P(y|x)\n",
    "        :param X:\n",
    "        :return: the predicted class\n",
    "        hint: use the predict_proba class\n",
    "        \"\"\"\n",
    "        posterior = \n",
    "        prediction = \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate some data to use to train our models\n",
    "centers = [(-10, 5), (0, 0), (0, 10), (-7, 10)]\n",
    "X_y_0, _ = make_blobs(n_samples=4000, centers=centers, cluster_std=1.00, random_state=0)\n",
    "centers = [(-5, 5), (-5, -2)]\n",
    "X_y_1, _ = make_blobs(n_samples=2000, centers=centers, cluster_std=1.00, random_state=0)\n",
    "X = np.concatenate([X_y_0, X_y_1], axis=0)\n",
    "Y = np.concatenate([[0] * 4000, [1] * 2000], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(X[:,0], X[:,1], color =[\"r\" if y == 1 else \"g\" for y in Y], alpha = 0.5)\n",
    "plt.figure()\n",
    "plt.title(\"Scatter plot of our data\")\n",
    "plt.scatter(X_y_0[:, 0], X_y_0[:, 1], alpha=0.5, label=\"x for y=0\")\n",
    "plt.scatter(X_y_1[:, 0], X_y_1[:, 1], alpha=0.5, label=\"x for y=1\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial the classifier object\n",
    "gaussian_naive_bayes = GaussianNaiveBayesClassifier()\n",
    "\n",
    "# fit the model\n",
    "gaussian_naive_bayes.fit(X=X,y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predict propabilities for each class\n",
    "posterior = \n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predict propabilities for each class\n",
    "y_pred = \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true = Y, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_0 = gaussian_naive_bayes.models[0].mean\n",
    "X_y_pred_0 = X[y_pred==0]\n",
    "\n",
    "mean_1 = gaussian_naive_bayes.models[1].mean\n",
    "X_y_pred_1 = X[y_pred==1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Scatter plot of our data\")\n",
    "plt.scatter(X_y_pred_0[:, 0], X_y_pred_0[:, 1], alpha=0.5, label=\"x for y_pred=0\")\n",
    "plt.scatter(mean_0[ 0], mean_0[ 1], label=\"mean for the y_pred=0\", color=\"red\")\n",
    "plt.scatter(X_y_pred_1[:, 0], X_y_pred_1[:, 1], alpha=0.5, label=\"x for y_pred=1\")\n",
    "plt.scatter(mean_1[ 0], mean_1[ 1], label=\"mean for the y_pred=1\", color=\"purple\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.  Bayesian Classifier with Gaussian mixture model\n",
    "\n",
    "Implement the Bayesian Classifier with Gaussian mixture model.\n",
    "The model is as before, but instead of using the Gaussian Distribution to model the P(x|y) for each class, we use a mixture of Gaussian's.\n",
    "\n",
    "So you have to find the parameters of a GMM ($w_k$, $m_k$, $cov_k$) for each different y.  \n",
    "note: To fit the gmm of each class y you can use the sklearn as we demonstrate before\n",
    "\n",
    "Fill the class below and use it to get:  \n",
    "1. fit your model using .fit() method\n",
    "2. the propabilities of each class for the different x using .predict_proba() method\n",
    "3. make the final predictions using the .predict() method and calculate the accuracy of your model\n",
    "4. compare the result with the gaussian bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a class for GMM to have the same API as scipy multivariate gausssian in order to change the minim in our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, mean, cov, w):\n",
    "        self.k = len(means)  # components\n",
    "        self.mean = mean\n",
    "        self.cov = cov\n",
    "        self.w = w\n",
    "\n",
    "    def pdf(self, x):\n",
    "        p_x = 0\n",
    "        for k in range(self.k):\n",
    "            p_x += self.w[k] * mvn(mean=self.mean[k], cov=self.cov[k]).pdf(x)\n",
    "        return p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureNaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    Implementation of Gaussian Mixture Naive Bayes Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # initialization of required variables.\n",
    "        self.models = None  # p(x|y)\n",
    "        self.prior = None  # p(y)\n",
    "        self.classes = None # the classes\n",
    "        self.n_classes = None # number of classes\n",
    "        self.n_features = None # dimension of the features\n",
    "\n",
    "\n",
    "    def fit(self, X, y, n_components):\n",
    "        \"\"\"\n",
    "        Calculate the parameters of the gaussian models for each class i.e P(x|y), as well as the prior class probabilities P(y).\n",
    "        :param X: The input features\n",
    "        :param y: The labels\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        num_of_points= X.shape[0]\n",
    "        \n",
    "        self.classes = list(set(y))\n",
    "        self.n_classes = len(set(y))\n",
    "        \n",
    "        #\n",
    "        self.prior = np.zeros(self.n_classes)\n",
    "        self.models = []\n",
    "        for y_index in range(self.n_classes):\n",
    "            \n",
    "            class_y = # fill your code\n",
    "            # 1. Calculate the prior P(y) for each class\n",
    "            p_y = # fill your code\n",
    "            self.prior[y_index] =  p_y\n",
    "            \n",
    "            # 2. Calculate the parameters model P(x|y)\n",
    "            # we will find the parameters of the gmm for each class\n",
    "            data_x_y = # fill your code        # get the data that only in class y\n",
    "            gmm = # fill your code             # initialise gmm\n",
    "            # fill your code                   # train gmm\n",
    "            \n",
    "            # get the parameters of the model\n",
    "            w_y =# fill your code  \n",
    "            mean_x_y =# fill your code   # calculate the mean\n",
    "            diag_cov_x_y = # fill your code   # calculate the covariance matrix, Note that you have to set rowvar=False\n",
    "            \n",
    "            self.models += [GMM(mean=mean_x_y, cov=diag_cov_x_y, w=w_y)] # add the model p(x|y) in your model list\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the posterior probability of each class given the features, i.e. P(y|x).\n",
    "        You can calculate this with Bayes's theorem:\n",
    "                    P(y|x) = P(x|y) P(y) / P(x).\n",
    "                    P(y|x): The Posterior probability\n",
    "                    P(x|y): The Likelihood probability\n",
    "                    P(y): Prior probability\n",
    "                    P(x): The evidence \\sum_{y'} P(x|y') P(y')\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # get general information\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        posterior = # fill your code\n",
    "        return posterior\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predicted class with the maximum posterior P(y|x)\n",
    "        :param X:\n",
    "        :return: the predicted class\n",
    "        hint: use the predict_proba class\n",
    "        \"\"\"\n",
    "        posterior = # fill your code\n",
    "        prediction = # fill your code\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial the classifier object\n",
    "gaussian_naive_bayes = GaussianMixtureNaiveBayesClassifier()\n",
    "\n",
    "# fit the model\n",
    "gaussian_naive_bayes.fit(X=X,y=Y, n_components= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predict propabilities for each class\n",
    "predicted_proba = \n",
    "predicted_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that sum to 1, note that there may be a small discrepancies due to numerical error\n",
    "predicted_proba.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predict propabilities for each class\n",
    "y_pred = \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true = Y, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mean_0 = gaussian_naive_bayes.models[0].mean\n",
    "X_y_pred_0 = X[y_pred==0]\n",
    "\n",
    "mean_1 = gaussian_naive_bayes.models[1].mean\n",
    "X_y_pred_1 = X[y_pred==1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Scatter plot of our data\")\n",
    "plt.scatter(X_y_pred_0[:, 0], X_y_pred_0[:, 1], alpha=0.5, label=\"x for y_pred=0\")\n",
    "plt.scatter(mean_0[:, 0], mean_0[:, 1], label=\"mean for the y_pred=0\", color=\"red\")\n",
    "plt.scatter(X_y_pred_1[:, 0], X_y_pred_1[:, 1], alpha=0.5, label=\"x for y_pred=1\")\n",
    "plt.scatter(mean_1[:, 0], mean_1[:, 1], label=\"mean for the y_pred=1\", color=\"purple\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Scatter plot of our data\")\n",
    "plt.scatter(X_y_0[:, 0], X_y_0[:, 1], alpha=0.5, label=\"x for y=0\")\n",
    "plt.scatter(X_y_1[:, 0], X_y_1[:, 1], alpha=0.5, label=\"x for y=1\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
