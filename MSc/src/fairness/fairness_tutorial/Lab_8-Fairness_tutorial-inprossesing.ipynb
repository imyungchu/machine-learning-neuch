{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/fair_1.png\" width=\"200\"/>\n",
    "</figure>\n",
    "\n",
    "# Lab 8 - Fairness of Predictive Models\n",
    "\n",
    "In the last lab, we observed that models can exhibit unfairness towards minority groups. We measured Demographic Parity, Equal Opportunity, and Equalized Odds to quantify this unfairness. We also found that simply excluding the sensitive feature from the input features did not solve the problem, as it may be correlated with other features in the dataset. Finally, by excluding these correlated features, we were able to reduce the model's bias with a trade-off in accuracy.\n",
    "\n",
    "Today, we will explore how to train a neural network to enhance model fairness using in-processing methods. More specifically, we will see how we can impose an additional regularization term in the modelâ€™s loss function that accounts for fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Fairness\n",
    "In group fairness, different groups should be treated equally, ensuring fair outcomes across demographic groups.\n",
    "\n",
    "# Correct the Unfair Classifier\n",
    "\n",
    "In summary, there are three general methods to correct an unfair classifier:\n",
    "\n",
    "1. **Pre-Processing**: Make changes to the data before training the model, e.g., removing correlated features.\n",
    "2. **In-Processing**: Make changes to the model to correct fairness, e.g., adding additional loss terms to ensure fairness.\n",
    "3. **Post-Processing**: Make changes after the model's output, e.g., adjusting classification thresholds.\n",
    "\n",
    "For an overview of different methods and fairness criteria, we refer you to the following interesting survey: [https://arxiv.org/pdf/2001.09784.pdf](https://arxiv.org/pdf/2001.09784.pdf).\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"./figures/corect_unfairness.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in /Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: fsspec in /Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages (from torch==2.1.0) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages (from torch==2.1.0) (1.12)\n",
      "Requirement already satisfied: filelock in /Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages (from torch==2.1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages (from torch==2.1.0) (4.4.0)\n",
      "Requirement already satisfied: networkx in /Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages (from torch==2.1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages (from torch==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages (from jinja2->torch==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1. Load Dataset\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T09:06:37.619228Z",
     "start_time": "2024-11-14T09:06:37.606668Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_types = OrderedDict([\n",
    "    (\"age\", \"int\"),\n",
    "    (\"workclass\", \"category\"),\n",
    "    (\"final_weight\", \"int\"),  # originally it was called fnlwgt\n",
    "    (\"education\", \"category\"),\n",
    "    (\"education_num\", \"int\"),\n",
    "    (\"marital_status\", \"category\"),\n",
    "    (\"occupation\", \"category\"),\n",
    "    (\"relationship\", \"category\"),\n",
    "    (\"race\", \"category\"),\n",
    "    (\"sex\", \"category\"),\n",
    "    (\"capital_gain\", \"float\"),  # required because of NaN values\n",
    "    (\"capital_loss\", \"int\"),\n",
    "    (\"hours_per_week\", \"int\"),\n",
    "    (\"native_country\", \"category\"),\n",
    "    (\"income_class\", \"category\"),\n",
    "])\n",
    "target_column = \"income_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        names=data_types,\n",
    "        index_col=None,\n",
    "\n",
    "        comment='|',  # test dataset has comment in it\n",
    "        skipinitialspace=True,  # Skip spaces after delimiter\n",
    "        na_values={\n",
    "            'capital_gain': 99999,\n",
    "            'workclass': '?',\n",
    "            'native_country': '?',\n",
    "            'occupation': '?',\n",
    "        },\n",
    "        dtype=data_types,\n",
    "    )\n",
    "\n",
    "def clean_dataset(data):\n",
    "    # Test dataset has dot at the end, we remove it in order\n",
    "    # to unify names between training and test datasets.\n",
    "    data['income_class'] = data.income_class.str.rstrip('.').astype('category')\n",
    "    \n",
    "    # Remove final weight column since there is no use\n",
    "    # for it during the classification.\n",
    "    data = data.drop('final_weight', axis=1)\n",
    "    \n",
    "    # Duplicates might create biases during the analysis and\n",
    "    # during prediction stage they might give over-optimistic\n",
    "    # (or pessimistic) results.\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    # Binary target variable (>50K == 1 and <=50K == 0)\n",
    "    data[target_column] = (data[target_column] == '>50K').astype(int)\n",
    "    \n",
    "    # Categorical dataset\n",
    "    categorical_features = data.select_dtypes('category').columns\n",
    "    data[categorical_features] = data.select_dtypes('category').apply(lambda x: x.cat.codes)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load & clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (28938, 14)\n",
      "Test dataset shape: (15175, 14)\n"
     ]
    }
   ],
   "source": [
    "# get and clean train dataset\n",
    "TRAIN_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "train_data = clean_dataset(read_dataset(TRAIN_DATA_FILE))\n",
    "train_data = train_data.dropna()\n",
    "print(\"Train dataset shape:\", train_data.shape)\n",
    "\n",
    "# get and clean test dataset\n",
    "TEST_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "test_data = clean_dataset(read_dataset(TEST_DATA_FILE))\n",
    "test_data = test_data.dropna()\n",
    "print(\"Test dataset shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"income_class\"\n",
    "sensitive_feature = \"sex\"\n",
    "features = train_data.columns.difference([target_column, sensitive_feature]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  education  education_num  marital_status  occupation  \\\n",
       "0   39          6          9             13               4           0   \n",
       "1   50          5          9             13               2           3   \n",
       "2   38          3         11              9               0           5   \n",
       "3   53          3          1              7               2           5   \n",
       "4   28          3          9             13               2           9   \n",
       "\n",
       "   relationship  race  sex  capital_gain  capital_loss  hours_per_week  \\\n",
       "0             1     4    1        2174.0             0              40   \n",
       "1             0     4    1           0.0             0              13   \n",
       "2             1     4    1           0.0             0              40   \n",
       "3             0     2    1           0.0             0              40   \n",
       "4             5     2    0           0.0             0              40   \n",
       "\n",
       "   native_country  income_class  \n",
       "0              38             0  \n",
       "1              38             0  \n",
       "2              38             0  \n",
       "3              38             0  \n",
       "4               4             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "fig = sns.countplot(data=train_data[[\"income_class\",\"sex\"]],\n",
    "                    x=\"income_class\",\n",
    "                    hue=\"sex\")\n",
    "fig.set_xticklabels(['income <= 50K','income > 50K'])\n",
    "plt.legend(title='sex', labels=['woman', 'men'])\n",
    "plt.title(\"Num of values in each category\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Get train val test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data[features+[sensitive_feature]], train_data[[target_column]], test_size=0.1, random_state=1234)\n",
    "X_test, y_test = test_data[features], test_data[[target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[features]),index=X_train.index,columns = features)\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val[features]),index=X_val.index,columns = features)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test[features]),index=X_test.index ,columns = features) # note that here we just use the transform method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Get torch itterator\n",
    "To efficiently iterate over the dataset during training, we will use PyTorchâ€™s  `DataLoader` that consumes a `TensorDataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert data into tensors\n",
    "train_x_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float)\n",
    "train_y_tensor = torch.tensor(y_train[[target_column]].values, dtype=torch.float)\n",
    "\n",
    "# use the dataset class wrapper for tensors\n",
    "train_tensor_dataset = torch.utils.data.TensorDataset(train_x_tensor, train_y_tensor)# there are more coplex dataset\n",
    "                                                                                     # i.e images or text data\n",
    "\n",
    "# pass the dataset class into Dataloader to batch and shuffle your data\n",
    "train_dataloader = torch.utils.data.DataLoader(train_tensor_dataset,\n",
    "                                               batch_size=32,\n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert data into tensors\n",
    "val_x_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float)\n",
    "val_y_tensor = torch.tensor(y_val[[target_column]].values, dtype=torch.float)\n",
    "\n",
    "# use the dataset class wrapper for tensors\n",
    "val_tensor_dataset = torch.utils.data.TensorDataset(val_x_tensor, val_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert data into tensors\n",
    "test_x_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float)\n",
    "test_y_tensor = torch.tensor(y_test[[target_column]].values, dtype=torch.float)\n",
    "\n",
    "# use the dataset class wrapper for tensors\n",
    "test_tensor_dataset = torch.utils.data.TensorDataset(test_x_tensor, test_y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite imbalance between a gender, so we expect our model to be unfair.\n",
    "The source of the bias is coming from our societal bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3. Define  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple two layer neural network for regression\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1\n",
    "        layer_1_units = 20\n",
    "        self.layer_1 = torch.nn.Linear(in_features=num_input_features,\n",
    "                                       out_features=layer_1_units)\n",
    "        self.activation_1 = torch.nn.ReLU()\n",
    "        \n",
    "        # layer  2\n",
    "        layer_2_units = 10\n",
    "        self.layer_2 = torch.nn.Linear(in_features=layer_1_units,\n",
    "                                       out_features=layer_2_units)\n",
    "        self.activation_2 = torch.nn.ReLU()\n",
    "        \n",
    "        # layer output layer\n",
    "        self.out_layer = torch.nn.Linear(in_features=layer_2_units,\n",
    "                                         out_features=1)\n",
    "        self.out_activation= torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.activation_1( self.layer_1( x ) )\n",
    "        x_2 = self.activation_2( self.layer_2( x_1 ) )\n",
    "        output = self.out_activation(self.out_layer(x_2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model = NeuralNetwork(num_input_features=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4. Define training prossec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_data, optimizer, loss_fn, lamda):\n",
    "    # reset gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # unfold data\n",
    "    x_batch, y_batch = input_data\n",
    "    \n",
    "    # get predictions\n",
    "    y_pred_propa = model.forward(x_batch)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = loss_fn(y_pred_propa, y_batch)\n",
    "\n",
    "    # compute gradients \n",
    "    loss.backward()\n",
    "    \n",
    "    # optimise network\n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute metrics for monitoring\n",
    "    with torch.no_grad(): \n",
    "        y_pred = (y_pred_propa>0.5) * 1\n",
    "        train_acc = torch.sum(y_pred == y_batch) / y_batch.shape[0]\n",
    "\n",
    "    return loss.data.numpy(), train_acc.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(model, input_data, loss_fn):\n",
    "    # when we use torch.no_grad pytorch didnt store information\n",
    "    # that is required to calculate gradients so is fasterr \n",
    "    with torch.no_grad(): \n",
    "        x_batch, y_batch = val_tensor_dataset.tensors\n",
    "        y_pred_proba = model(x_batch)\n",
    "        loss = loss_fn(y_pred_proba, y_batch)\n",
    "\n",
    "        # compute metrics\n",
    "        y_pred = (y_pred_proba>0.5) * 1\n",
    "        acc = torch.sum(y_pred == y_batch) / y_batch.shape[0]\n",
    "    return loss.data.numpy(), acc.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Train  Loop----------------------------\n",
    "def train_loop(train_dataloaders, val_tensor_dataset, patient, epochs, model, optimizer, loss_fn, lamba, steps_per_epoch = 1000):\n",
    "    best_loss = np.inf\n",
    "    consecutive_epoch = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(epochs): # iterate over epoch    \n",
    "        \n",
    "        # -------------------- Training on each epoch ----------------------------\n",
    "        accumulated_loss = 0 # monitor loss during training\n",
    "        accumulated_accuracy = 0 # monitor  accuracy during training\n",
    "        accuracy_list = []\n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_data = next(iter(train_dataloader))\n",
    "            loss, accuracy = training_step(model,batch_data,optimizer,loss_fn,lamba) # train model using a single batch\n",
    "            accuracy_list += [accuracy]\n",
    "            accumulated_loss = (step * accumulated_loss + loss)/(step+1)\n",
    "            accumulated_accuracy =  (step * accumulated_accuracy + accuracy)/(step+1)\n",
    "\n",
    "        train_history += [{\"loss\":accumulated_loss, \"accuracy\":accumulated_accuracy, \"epoch\": epoch, \"set\":\"train\"}]\n",
    "\n",
    "        # -------------------- Monitor Error Validation set ----------------------------\n",
    "        val_loss, val_accuracy = evaluation_step(model, val_tensor_dataset, loss_fn)\n",
    "        val_history += [{\"loss\":val_loss, \"accuracy\":val_accuracy, \"epoch\": epoch, \"set\":\"val\"}]\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}:  loss:{accumulated_loss:.3f}, accuracy-:{accumulated_accuracy:.3f}, val_loss:{val_loss:.3f}, val_accuracy->{val_accuracy:.3f}\")\n",
    "\n",
    "        # -------------------- Early Stoping ----------------------------\n",
    "        if val_loss > best_loss:\n",
    "            consecutive_epoch += 1\n",
    "        else:\n",
    "            best_loss = val_loss # we have an improvement\n",
    "            consecutive_epoch = 0 # reset counter\n",
    "            best_epoch = epoch\n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "        if consecutive_epoch > patient:\n",
    "            model.load_state_dict(best_weights) # set best weights\n",
    "            break\n",
    "    val_history_df = pd.DataFrame(val_history)\n",
    "    train_history_df = pd.DataFrame(train_history)\n",
    "    return model, val_history_df, train_history_df, best_loss, best_epoch, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2000:  loss:0.651, accuracy-:0.768, val_loss:0.630, val_accuracy->0.758\n",
      "Epoch 10/2000:  loss:0.480, accuracy-:0.758, val_loss:0.480, val_accuracy->0.756\n",
      "Epoch 20/2000:  loss:0.432, accuracy-:0.782, val_loss:0.429, val_accuracy->0.779\n",
      "Epoch 30/2000:  loss:0.417, accuracy-:0.813, val_loss:0.402, val_accuracy->0.811\n",
      "Epoch 40/2000:  loss:0.402, accuracy-:0.815, val_loss:0.388, val_accuracy->0.813\n",
      "Epoch 50/2000:  loss:0.379, accuracy-:0.823, val_loss:0.380, val_accuracy->0.819\n",
      "Epoch 60/2000:  loss:0.375, accuracy-:0.824, val_loss:0.373, val_accuracy->0.821\n",
      "Epoch 70/2000:  loss:0.388, accuracy-:0.821, val_loss:0.366, val_accuracy->0.827\n",
      "Epoch 80/2000:  loss:0.376, accuracy-:0.821, val_loss:0.359, val_accuracy->0.828\n",
      "Epoch 90/2000:  loss:0.368, accuracy-:0.823, val_loss:0.353, val_accuracy->0.829\n",
      "Epoch 100/2000:  loss:0.357, accuracy-:0.839, val_loss:0.347, val_accuracy->0.834\n",
      "Epoch 110/2000:  loss:0.357, accuracy-:0.825, val_loss:0.342, val_accuracy->0.833\n",
      "Epoch 120/2000:  loss:0.355, accuracy-:0.832, val_loss:0.338, val_accuracy->0.838\n",
      "Epoch 130/2000:  loss:0.322, accuracy-:0.852, val_loss:0.335, val_accuracy->0.838\n",
      "Epoch 140/2000:  loss:0.334, accuracy-:0.842, val_loss:0.333, val_accuracy->0.841\n",
      "Epoch 150/2000:  loss:0.323, accuracy-:0.852, val_loss:0.331, val_accuracy->0.840\n",
      "Epoch 160/2000:  loss:0.315, accuracy-:0.849, val_loss:0.331, val_accuracy->0.839\n",
      "Epoch 170/2000:  loss:0.338, accuracy-:0.843, val_loss:0.329, val_accuracy->0.843\n",
      "Epoch 180/2000:  loss:0.321, accuracy-:0.851, val_loss:0.329, val_accuracy->0.842\n",
      "Epoch 190/2000:  loss:0.358, accuracy-:0.829, val_loss:0.328, val_accuracy->0.843\n",
      "Epoch 200/2000:  loss:0.347, accuracy-:0.830, val_loss:0.327, val_accuracy->0.843\n",
      "Epoch 210/2000:  loss:0.339, accuracy-:0.840, val_loss:0.326, val_accuracy->0.842\n",
      "Epoch 220/2000:  loss:0.343, accuracy-:0.836, val_loss:0.326, val_accuracy->0.843\n",
      "Epoch 230/2000:  loss:0.321, accuracy-:0.849, val_loss:0.326, val_accuracy->0.843\n",
      "Epoch 240/2000:  loss:0.341, accuracy-:0.835, val_loss:0.325, val_accuracy->0.845\n",
      "Epoch 250/2000:  loss:0.322, accuracy-:0.844, val_loss:0.326, val_accuracy->0.847\n",
      "Epoch 260/2000:  loss:0.343, accuracy-:0.830, val_loss:0.325, val_accuracy->0.847\n",
      "Epoch 270/2000:  loss:0.329, accuracy-:0.851, val_loss:0.325, val_accuracy->0.846\n",
      "Epoch 280/2000:  loss:0.317, accuracy-:0.844, val_loss:0.324, val_accuracy->0.844\n",
      "Epoch 290/2000:  loss:0.314, accuracy-:0.850, val_loss:0.324, val_accuracy->0.846\n",
      "Epoch 300/2000:  loss:0.330, accuracy-:0.833, val_loss:0.324, val_accuracy->0.847\n",
      "Epoch 310/2000:  loss:0.329, accuracy-:0.846, val_loss:0.325, val_accuracy->0.848\n",
      "Epoch 320/2000:  loss:0.342, accuracy-:0.844, val_loss:0.325, val_accuracy->0.848\n",
      "Epoch 330/2000:  loss:0.322, accuracy-:0.848, val_loss:0.325, val_accuracy->0.846\n",
      "Epoch 340/2000:  loss:0.323, accuracy-:0.851, val_loss:0.324, val_accuracy->0.846\n",
      "Epoch 350/2000:  loss:0.344, accuracy-:0.841, val_loss:0.324, val_accuracy->0.848\n",
      "Epoch 360/2000:  loss:0.321, accuracy-:0.857, val_loss:0.323, val_accuracy->0.848\n",
      "Epoch 370/2000:  loss:0.307, accuracy-:0.860, val_loss:0.324, val_accuracy->0.848\n",
      "Epoch 380/2000:  loss:0.333, accuracy-:0.841, val_loss:0.324, val_accuracy->0.846\n",
      "Epoch 390/2000:  loss:0.324, accuracy-:0.853, val_loss:0.323, val_accuracy->0.849\n",
      "Epoch 400/2000:  loss:0.329, accuracy-:0.842, val_loss:0.322, val_accuracy->0.850\n",
      "Epoch 410/2000:  loss:0.326, accuracy-:0.847, val_loss:0.322, val_accuracy->0.851\n",
      "Epoch 420/2000:  loss:0.326, accuracy-:0.842, val_loss:0.322, val_accuracy->0.848\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(num_input_features=12)\n",
    "\n",
    "lr = 0.005\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.00)\n",
    "loss_fn = torch.nn.BCELoss() # binary cross entropy\n",
    "\n",
    "model, val_history, train_history, best_loss, best_epoch, best_weights = train_loop(train_dataloader,\n",
    "                                                                                    val_tensor_dataset,\n",
    "                                                                                    patient = 50,\n",
    "                                                                                    epochs=2000,\n",
    "                                                                                    model=model,\n",
    "                                                                                    optimizer=optimizer,\n",
    "                                                                                    loss_fn=loss_fn,\n",
    "                                                                                    steps_per_epoch=100,\n",
    "                                                                                    lamba = 0.01)\n",
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_history[\"epoch\"],train_history[\"loss\"], label = \"Training set\")\n",
    "plt.plot(val_history[\"epoch\"],val_history[\"loss\"], label = \"Validation set\")\n",
    "plt.title(\"Learning Curve no Early Stopping\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(test_tensor_dataset.tensors[0]).detach().numpy()\n",
    "predictions = (predictions>0.5)*1\n",
    "accuracy = accuracy_score(y_true=test_tensor_dataset.tensors[1].detach().numpy(), y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy_score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Evaluate Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1.  Demographic Parity\n",
    "\n",
    "**Demographic parity** , also referred to as **statistical parity** , **acceptance rate parity**  and **benchmarking**\n",
    "\n",
    "Demographic Parity states that the positive outcome rates between sensitive group must be the same.\n",
    "\n",
    "---------------\n",
    "> **Definition Demographic Parity :** We say that a predictor $\\hat{Y}$ satisfy **demographic parity** if the predictions $\\hat{Y}$ are independent of the sensitive atribute $S$, $\\hat{Y} \\perp S$.\n",
    "$$ P_r(\\hat{Y}= 1 | S = s ) =  P_r(\\hat{Y} = 1), \\quad  \\forall s\\in S $$\n",
    "\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we mesure the bias of model as follows:\n",
    "$$DP = \\; \\mid P_r(\\hat{Y}= 1 | S = 0 ) -  P_r(\\hat{Y}= 1 | S = 1 ) \\mid $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity_difference(predictions, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of demographic parity difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the propotion of accepted rate.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calcaulte the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the proportion of positive predictions.\n",
    "    # group 0 \n",
    "    group0_mask = sensitive_atribute == 0\n",
    "    predictions_group0 = predictions[group0_mask]\n",
    "    labels_group0 = test_data.loc[group0_mask, target_column]\n",
    "\n",
    "    positive_rate_group0 = np.sum(predictions_group0 == 1)*1/ predictions_group0.shape[0]\n",
    "\n",
    "    # group 1 \n",
    "    group1_mask = sensitive_atribute == 1\n",
    "    predictions_group1 = predictions[group1_mask]\n",
    "    labels_group0 = test_data.loc[group1_mask, target_column]\n",
    "\n",
    "    positive_rate_group1 = np.sum(predictions_group1 == 1)*1 / predictions_group1.shape[0]\n",
    "    \n",
    "    # 3. Calculate the different.\n",
    "    difference = np.abs(positive_rate_group0-positive_rate_group1)\n",
    "    \n",
    "    return difference, [positive_rate_group0, positive_rate_group1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Equalized Opportunities\n",
    "\n",
    "**Equalized Opportunities** , also referred to as **true positive parity**, **sensitivity**.\n",
    "\n",
    "Equalized Opportunities states that each group has equal true positive rates.\n",
    "\n",
    "---------------\n",
    "> **Definition Equalized Opportunities :** We say that a predictor $\\hat{Y}$ satisfy **equalized opportunities** if the predictions $\\hat{Y}$ are independent of the sensitive atribute $S$, conditioned on the positive actual outcome \\( Y = 1 \\), $\\hat{Y} \\perp S \\mid Y = 1$.\n",
    "$$ P_r(\\hat{Y} = 1 | S = s, Y = 1) = P_r(\\hat{Y} = 1 | Y = 1), \\quad \\forall s \\in S $$\n",
    "\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we mesure the bias of model as follows:\n",
    "$$EO = \\; \\mid P_r(\\hat{Y} = 1 | S = 0, Y = 1) -  P_r(\\hat{Y} = 1 | S = 1, Y = 1) \\mid $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_opportunities_difference(predictions, actual, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of equalized opportunities difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the TPR.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calculate the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the proportion of positive predictions.\n",
    "    # group 0 \n",
    "    true_positive_mask = test_data[target_column]==1\n",
    "    group0_mask = sensitive_atribute == 0\n",
    "    predictions_group0 = predictions[np.logical_and(group0_mask, true_positive_mask)]\n",
    "\n",
    "    true_positive_rate_group0 = np.sum(predictions_group0 == 1)*1/ predictions_group0.shape[0]\n",
    "\n",
    "    # group 1 \n",
    "    group1_mask = sensitive_atribute == 1\n",
    "    predictions_group1 = predictions[np.logical_and(group1_mask, true_positive_mask)]\n",
    "    labels_group0 = test_data.loc[group1_mask, target_column]\n",
    "\n",
    "    true_positive_rate_group1 = np.sum(predictions_group1 == 1)*1 / predictions_group1.shape[0]\n",
    "    \n",
    "    # 3. Calculate the different.\n",
    "    difference = np.abs(true_positive_rate_group0-true_positive_rate_group1)\n",
    "    \n",
    "    return difference, [true_positive_rate_group0, true_positive_rate_group1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Equalized Odds\n",
    "\n",
    "**Equalized Odds** , also referred to as\n",
    "\n",
    "Equalized Odds states that the true positive rates (TPR) and false positive rates (FPR) between sensitive group must be the same.\n",
    "\n",
    "---------------\n",
    "> **Definition Equalized Odds :** A classifier $C$ We say that a predictor $\\hat{Y}$ satisfy **equalized Odds** if the predictions $\\hat{Y}$ are independent of the sensitive atribute $S$, conditioned on the actual outcome \\( Y \\), $\\hat{Y} \\perp S \\mid Y$.\n",
    "$$ P_r(\\hat{Y} = 1 | S = s, Y = y) = P_r(\\hat{Y} = 1 | Y = y), \\quad \\forall s \\in S \\quad \\forall y \\in Y$$\n",
    "\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we want to satisfy both:\n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 1 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 1 ) $$ \n",
    "and \n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 0 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 0 ) $$  \n",
    "---------------\n",
    "we mesure the bias of model as follows:\n",
    "$$EOds = \\; \\mid P_r(\\hat{Y} = 1 | S = 0, Y = 1) -  P_r(\\hat{Y} = 1 | S = 1, Y = 1) \\mid + \\mid P_r(\\hat{Y} = 1 | S = 0, Y = 0) -  P_r(\\hat{Y} = 1 | S = 1, Y = 0) \\mid  $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_odds_difference(predictions, data, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of equalized odds difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the TPR.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calcaulte the difference.\n",
    "    \"\"\"\n",
    "    # A. For each group calculate the true positive rate.\n",
    "    # group 0 \n",
    "    true_positive_mask = data[target_column]==1\n",
    "    group0_mask = sensitive_atribute == 0\n",
    "    predictions_group0 = predictions[np.logical_and(group0_mask, true_positive_mask)]\n",
    "    labels_group0 = data.loc[group0_mask, target_column]\n",
    "\n",
    "    true_positive_rate_group0 = np.sum(predictions_group0 == 1)*1/ predictions_group0.shape[0]\n",
    "\n",
    "    # group 1 \n",
    "    group1_mask = sensitive_atribute == 1\n",
    "    predictions_group1 = predictions[np.logical_and(group1_mask, true_positive_mask)]\n",
    "    labels_group0 = data.loc[group1_mask, target_column]\n",
    "\n",
    "    true_positive_rate_group1 = np.sum(predictions_group1 == 1)*1 / predictions_group1.shape[0]\n",
    "    \n",
    "    # Calculate the different.\n",
    "    difference1 = np.abs(true_positive_rate_group0-true_positive_rate_group1)\n",
    "    \n",
    "    # B. For each group calculate the false positive rate.\n",
    "    # group 0 \n",
    "    true_positive_mask = data[target_column]==0\n",
    "    group0_mask = sensitive_atribute == 0\n",
    "    predictions_group0 = predictions[np.logical_and(group0_mask, true_positive_mask)]\n",
    "    labels_group0 = data.loc[group0_mask, target_column]\n",
    "\n",
    "    false_positive_rate_group0 = np.sum(predictions_group0 == 1)/ predictions_group0.shape[0]\n",
    "\n",
    "    # group 1 \n",
    "    group1_mask = sensitive_atribute == 1\n",
    "    predictions_group1 = predictions[np.logical_and(group1_mask, true_positive_mask)]\n",
    "    labels_group0 = data.loc[group1_mask, target_column]\n",
    "\n",
    "    false_positive_rate_group1 = np.sum(predictions_group1 == 1) / predictions_group1.shape[0]\n",
    "    \n",
    "    # Calculate the different.\n",
    "    difference2 = np.abs(false_positive_rate_group0-false_positive_rate_group1)\n",
    "    \n",
    "    # Total difference\n",
    "    difference = difference1 + difference2\n",
    "    return difference, ([true_positive_rate_group0,true_positive_rate_group1], [false_positive_rate_group0,false_positive_rate_group1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the evaluation in a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(y_pred, data, sensitive_feature):\n",
    "    # get all metrics\n",
    "    acc_score = accuracy_score(y_true=data[target_column], y_pred=y_pred)\n",
    "    \n",
    "    demographic_metric, _ = demographic_parity_difference(y_pred,\n",
    "                                                          sensitive_feature)\n",
    "    \n",
    "    equalized_opportunities_metric, _ = equalized_opportunities_difference(y_pred,\n",
    "                                                                           data,\n",
    "                                                                          sensitive_feature)\n",
    "    equalized_odds_metrics, _ = equalized_odds_difference(y_pred,\n",
    "                                                          data,\n",
    "                                                          sensitive_feature)\n",
    "    \n",
    "    return {\n",
    "            \"accuracy\": acc_score,\n",
    "            \"demographic_metric\": demographic_metric,\n",
    "            \"equalized_opportunities\" : equalized_opportunities_metric,\n",
    "            \"equalized_odds\": equalized_odds_metrics\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(test_tensor_dataset.tensors[0]).detach().numpy()\n",
    "predictions = (predictions>0.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_atribute = test_data[sensitive_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metric(y_pred = predictions,\n",
    "                         data = test_data,\n",
    "                         sensitive_feature = sensitive_atribute)\n",
    "results = pd.DataFrame(metrics,\n",
    "                       index = [\"simple model\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Correct the Unfair Classifier\n",
    "\n",
    "Numerous recent papers have proposed mechanisms to enhance fairness in machine learning algorithms.\n",
    "\n",
    "In summary, there are three general methods to correct an unfair classifier:\n",
    "\n",
    "1. **Pre-Processing**: Make changes to the data before training the model, e.g., removing correlated features.\n",
    "2. **In-Processing**: Make changes to the model to correct fairness, e.g., adding additional loss terms to ensure fairness.\n",
    "3. **Post-Processing**: Make changes after the model's output, e.g., adjusting classification thresholds.\n",
    "\n",
    "For an overview of different methods and fairness criteria, we refer you to the following interesting survey: [https://arxiv.org/pdf/2001.09784.pdf](https://arxiv.org/pdf/2001.09784.pdf).\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"./figures/corect_unfairness.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair Regularization in Neural Networks\n",
    "\n",
    "In-processing techniques apply **constraints** or **regularization** during model optimization to enhance fairness.\n",
    "\n",
    "We will explore a regularization technique that imposes an additional term in the loss function to account for the modelâ€™s fairness.\n",
    "\n",
    "The total loss function is defined as follows:\n",
    "\n",
    "$$ L = L_{\\text{perf}} + \\lambda_{1} ||w||^2 + \\lambda_{fair}  L_{\\text{fair}} $$\n",
    "\n",
    "Where:\n",
    "- $ L_{\\text{perf}}$ : A loss term that evaluates the model's performance, such as binary cross-entropy.\n",
    "- $ ||w||^2 $ : A penalty term, L2 regularization, to prevent overfitting by constraining model complexity.\n",
    "- $ L_{\\text{fair}}$ : A penalty term to reduce the modelâ€™s bias. This term is adapted to the specific fairness criterion we aim to impose, such as Demographic Parity, Equal Opportunity, or Equalized Odds.\n",
    "\n",
    "The parameter $\\lambda_fair$ controls the strength of the fairness constraint, balancing model accuracy with fairness objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise Propability Distributions\n",
    "\n",
    "In fairness-aware machine learning, we impose regularization on model probabilities rather than the final output to  control and adjust the model's decision-making process. By focusing on probabilities, we can introduce a continuous, differentiable measure of fairness that is directly aligned with how the model assigns confidence to its predictions. \n",
    "\n",
    "### Demographic Parity\n",
    "\n",
    "Demographic Parity requires that the predicted probabilities are the same across different groups defined by a sensitive attribute, such as gender or race. Formally, for a sensitive attribute \\( A \\) (e.g., gender), Demographic Parity can be defined as:\n",
    "\n",
    "$$\n",
    "DP = \\left| \\mathbb{E}_{x \\sim P(x | s=0)}[p(\\hat{y} = 1 | x)] - \\mathbb{E}_{x \\sim P(x | s=1)}[p(\\hat{y} = 1 | x)] \\right|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ s $ denotes the sensitive attribute (e.g., race, gender).\n",
    "- $ P(x | s=0) $ and $ P(x | s=1)$ are the distributions of the input features $x$ conditioned on the sensitive attribute $s = 0$ and $s = 1$, respectively.\n",
    "-$ p(\\hat{y} = 1 | x)$ is the predicted probability for the outcome $\\hat{y} = 1$, given input $x$, form our model.\n",
    "\n",
    "\n",
    "This means the expected probability of predicting $y = 1$ should be the same for all groups $a$ and $b$ of the sensitive attribute $A$. By regularizing the probabilities, we directly control this balance during training, ensuring that the model does not favor one group over another in terms of predicted outcomes. \n",
    "\n",
    "By applying this fairness constraint to the probabilities, rather than the final binary output $\\hat{y}$, we ensure that fairness is maintained throughout the modelâ€™s decision process, leading to more equitable predictions across different demographic groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_data, optimizer, loss_fn, lamba):\n",
    "    # reset gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # unfold data\n",
    "    (x_batch_g0, y_batch_g0),(x_batch_g1, y_batch_g1) = input_data\n",
    "    \n",
    "    # get predictions\n",
    "    y_pred_propa_g0 = model.forward(x_batch_g0)\n",
    "    y_pred_propa_g1 = model.forward(x_batch_g1)\n",
    "\n",
    "    # calculate loss\n",
    "    loss_g0 = loss_fn(y_pred_propa_g0, y_batch_g0)\n",
    "    loss_g1 = loss_fn(y_pred_propa_g1, y_batch_g1)\n",
    "    total_loss = (loss_g0 + loss_g1)/2\n",
    "    \n",
    "    # regularization\n",
    "    regularization = torch.abs(torch.mean(y_pred_propa_g0) - torch.mean(y_pred_propa_g1))\n",
    "    \n",
    "    # total loss\n",
    "    loss = total_loss + lamba * regularization\n",
    "    \n",
    "    # compute gradients \n",
    "    loss.backward()\n",
    "    \n",
    "    # optimise network\n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute metrics for monitoring\n",
    "    with torch.no_grad(): \n",
    "        y_pred_propa = torch.concat([y_pred_propa_g0, y_pred_propa_g1])\n",
    "        y_true = torch.concat([y_batch_g0, y_batch_g1])\n",
    "        y_pred = (y_pred_propa>0.5) * 1\n",
    "        train_acc = torch.sum(y_pred == y_true) / y_pred.shape[0]\n",
    "\n",
    "    return loss.data.numpy(), train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(model, input_data, loss_fn, lamba):\n",
    "    # when we use torch.no_grad pytorch didnt store information\n",
    "    # that is required to calculate gradients so is fasterr \n",
    "    with torch.no_grad(): \n",
    "        data_g0, data_g1= input_data\n",
    "        (x_batch_g0, y_batch_g0) = data_g0.tensors\n",
    "        (x_batch_g1, y_batch_g1) = data_g1.tensors\n",
    "        y_pred_propa_g0 = model.forward(x_batch_g0)\n",
    "        y_pred_propa_g1 = model.forward(x_batch_g1)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss_g0 = loss_fn(y_pred_propa_g0, y_batch_g0)\n",
    "        loss_g1 = loss_fn(y_pred_propa_g1, y_batch_g1)\n",
    "        total_loss = (loss_g0 + loss_g1)/2\n",
    "    \n",
    "        # regularization\n",
    "        regularization = torch.abs(torch.mean(y_pred_propa_g0) - torch.mean(y_pred_propa_g1))\n",
    "    \n",
    "        # total loss\n",
    "        loss = total_loss + lamba * regularization\n",
    "\n",
    "        # compute metrics\n",
    "        y_pred_propa = torch.concat([y_pred_propa_g0, y_pred_propa_g1])\n",
    "        y_true = torch.concat([y_batch_g0, y_batch_g1])\n",
    "        y_pred = (y_pred_propa>0.5) * 1\n",
    "        acc = torch.sum(y_pred == y_true) / y_pred.shape[0]\n",
    "        \n",
    "    return loss.data.numpy(), acc.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Train  Loop----------------------------\n",
    "def train_loop(train_dataloaders, val_tensor_dataset, patient, epochs, model, optimizer, loss_fn, lamba, steps_per_epoch = 1000):\n",
    "    best_loss = np.inf\n",
    "    consecutive_epoch = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(epochs): # iterate over epoch    \n",
    "        \n",
    "        # -------------------- Training on each epoch ----------------------------\n",
    "        accumulated_loss = 0 # monitor loss during training\n",
    "        accumulated_accuracy = 0 # monitor  accuracy during training\n",
    "        accuracy_list = []\n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_data = []\n",
    "            d = map(iter,train_dataloaders)\n",
    "            for data in d:\n",
    "                batch_data += [next(data)]\n",
    "            loss, accuracy = training_step(model,batch_data,optimizer,loss_fn,lamba) # train model using a single batch\n",
    "            accuracy_list += [accuracy]\n",
    "            accumulated_loss = (step * accumulated_loss + loss)/(step+1)\n",
    "            accumulated_accuracy =  (step * accumulated_accuracy + accuracy)/(step+1)\n",
    "\n",
    "        train_history += [{\"loss\":accumulated_loss, \"accuracy\":accumulated_accuracy, \"epoch\": epoch, \"set\":\"train\"}]\n",
    "\n",
    "        # -------------------- Monitor Error Validation set ----------------------------\n",
    "        val_loss, val_accuracy = evaluation_step(model, val_tensor_dataset, loss_fn, lamba)\n",
    "        val_history += [{\"loss\":val_loss, \"accuracy\":val_accuracy, \"epoch\": epoch, \"set\":\"val\"}]\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}:  loss:{accumulated_loss:.3f}, accuracy-:{accumulated_accuracy:.3f}, val_loss:{val_loss:.3f}, val_accuracy->{val_accuracy:.3f}\")\n",
    "\n",
    "        # -------------------- Early Stoping ----------------------------\n",
    "        if val_loss > best_loss:\n",
    "            consecutive_epoch += 1\n",
    "        else:\n",
    "            best_loss = val_loss # we have an improvement\n",
    "            consecutive_epoch = 0 # reset counter\n",
    "            best_epoch = epoch\n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "        if consecutive_epoch > patient:\n",
    "            model.load_state_dict(best_weights) # set best weights\n",
    "            break\n",
    "    val_history_df = pd.DataFrame(val_history)\n",
    "    train_history_df = pd.DataFrame(train_history)\n",
    "    return model, val_history_df, train_history_df, best_loss, best_epoch, best_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_group_0 = X_train[sensitive_feature] == 0\n",
    "mask_group_1 = np.logical_not(mask_group_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_g0 = X_train_scaled[features].loc[mask_group_0.values]\n",
    "y_train_g0 = y_train[[target_column]].loc[mask_group_0.values]\n",
    "train_x_tensor_g0 = torch.tensor(x_train_g0.values, dtype=torch.float)\n",
    "train_y_tensor_g0 = torch.tensor(y_train_g0[[target_column]].values, dtype=torch.float)\n",
    "train_dataset_g0 = torch.utils.data.TensorDataset(train_x_tensor_g0, train_y_tensor_g0)\n",
    "train_dataloader_g0 = torch.utils.data.DataLoader(train_dataset_g0, batch_size=32,shuffle=True)\n",
    "\n",
    "x_train_g1 = X_train_scaled[features].loc[mask_group_1.values]\n",
    "y_train_g1 = y_train[[target_column]].loc[mask_group_1.values]\n",
    "train_x_tensor_g1 = torch.tensor(x_train_g1.values, dtype=torch.float)\n",
    "train_y_tensor_g1 = torch.tensor(y_train_g1[[target_column]].values, dtype=torch.float)\n",
    "train_dataset_g1 = torch.utils.data.TensorDataset(train_x_tensor_g1, train_y_tensor_g1)\n",
    "train_dataloader_g1 = torch.utils.data.DataLoader(train_dataset_g1, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_group_0 = X_val[sensitive_feature] == 0\n",
    "mask_group_1 = np.logical_not(mask_group_0)\n",
    "\n",
    "x_val_g0 = X_val_scaled[features].loc[mask_group_0.values]\n",
    "y_val_g0 = y_val[[target_column]].loc[mask_group_0.values]\n",
    "val_x_tensor_g0 = torch.tensor(x_val_g0.values, dtype=torch.float)\n",
    "val_y_tensor_g0 = torch.tensor(y_val_g0[[target_column]].values, dtype=torch.float)\n",
    "val_dataset_g0 = torch.utils.data.TensorDataset(val_x_tensor_g0, val_y_tensor_g0)\n",
    "\n",
    "x_val_g1 = X_val_scaled[features].loc[mask_group_1.values]\n",
    "y_val_g1 = y_val[[target_column]].loc[mask_group_1.values]\n",
    "val_x_tensor_g1 = torch.tensor(x_val_g1.values, dtype=torch.float)\n",
    "val_y_tensor_g1 = torch.tensor(y_val_g1[[target_column]].values, dtype=torch.float)\n",
    "val_dataset_g1 = torch.utils.data.TensorDataset(val_x_tensor_g1, val_y_tensor_g1)\n",
    "\n",
    "val_dataset = [val_dataset_g0,val_dataset_g1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_input_features=12)\n",
    "\n",
    "lr = 0.005\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.00)\n",
    "loss_fn = torch.nn.BCELoss() # binary cross entropy\n",
    "\n",
    "results_training = train_loop(train_dataloaders = (train_dataloader_g0, train_dataloader_g1),\n",
    "                     val_tensor_dataset= val_dataset,\n",
    "                     patient = 50,\n",
    "                     epochs=2000,\n",
    "                     model=model,\n",
    "                     optimizer=optimizer,\n",
    "                     loss_fn=loss_fn,\n",
    "                     lamba = 2.5,\n",
    "                     steps_per_epoch=100)\n",
    "model, val_history, train_history, best_loss, best_epoch, best_weights = results_training    \n",
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(test_tensor_dataset.tensors[0]).detach().numpy()\n",
    "predictions = (predictions>0.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_atribute = test_data[sensitive_feature]\n",
    "metrics = compute_metric(y_pred = predictions,\n",
    "                         data = test_data,\n",
    "                         sensitive_feature = sensitive_atribute)\n",
    "results2 = pd.DataFrame(metrics,\n",
    "                       index = [\"Reg DP model\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([results, results2],axis=1)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3. Equalized Opportunities\n",
    "\n",
    "Similarly, we can regularize the model to preserve Equalized Opportunities. Equalized Opportunities requires that the model has equal true positive rates (TPR) across different sensitive groups, meaning that the probability of a correct positive prediction (i.e., $ \\hat{y} = 1 $) given the true label $ y = 1 $ should be the same for all sensitive groups.\n",
    "\n",
    "The Equalized Opportunities difference (EO) can be defined as:\n",
    "\n",
    "$$ \\text{EO} = \\left| \\mathbb{E}_{x \\sim P(x | y = 1, s = 0)}[p(\\hat{y} = 1 | x)] - \\mathbb{E}_{x \\sim P(x | y = 1, s = 1)}[p(\\hat{y} = 1 | x)] \\right| $$\n",
    "\n",
    "Where:\n",
    "- $ p(\\hat{y} = 1 | x) $ is the predicted probability of $ y = 1 $ given input $ x $,\n",
    "- $ y = 1 $ denotes the positive class,\n",
    "- $ s $ denotes the sensitive attribute (e.g., gender, race),\n",
    "- The expectations are taken over the input data conditioned on $ y = 1 $ and the sensitive attribute $ s $.\n",
    "\n",
    "By regularizing the model according to this criterion, we ensure that the model maintains equal true positive rates across the sensitive groups, thus promoting fairness in the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_g0 = X_train[sensitive_feature] == 0\n",
    "mask_g1 = np.logical_not(mask_g0)\n",
    "\n",
    "mask_y0 = y_train[target_column] == 0\n",
    "mask_y1 = np.logical_not(mask_y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_g0y0 = np.logical_and(mask_g0, mask_y0)\n",
    "x_train_g0y0 = X_train_scaled[features].loc[mask_g0y0]\n",
    "y_train_g0y0 = y_train[[target_column]].loc[mask_g0y0]\n",
    "train_x_tensor_g0y0 = torch.tensor(x_train_g0y0.values, dtype=torch.float)\n",
    "train_y_tensor_g0y0 = torch.tensor(y_train_g0y0[[target_column]].values, dtype=torch.float)\n",
    "train_dataset_g0y0 = torch.utils.data.TensorDataset(train_x_tensor_g0y0, train_y_tensor_g0y0)\n",
    "train_dataloader_g0y0 = torch.utils.data.DataLoader(train_dataset_g0y0, batch_size=32,shuffle=True)\n",
    "\n",
    "mask_g0y1 = np.logical_and(mask_g0, mask_y1)\n",
    "x_train_g0y1 = X_train_scaled[features].loc[mask_g0y1]\n",
    "y_train_g0y1 = y_train[[target_column]].loc[mask_g0y1]\n",
    "train_x_tensor_g0y1 = torch.tensor(x_train_g0y1.values, dtype=torch.float)\n",
    "train_y_tensor_g0y1 = torch.tensor(y_train_g0y1[[target_column]].values, dtype=torch.float)\n",
    "train_dataset_g0y1 = torch.utils.data.TensorDataset(train_x_tensor_g0y1, train_y_tensor_g0y1)\n",
    "train_dataloader_g0y1 = torch.utils.data.DataLoader(train_dataset_g0y1, batch_size=32,shuffle=True)\n",
    "\n",
    "mask_g1y0 = np.logical_and(mask_g1, mask_y0)\n",
    "x_train_g1y0 = X_train_scaled[features].loc[mask_g1y0]\n",
    "y_train_g1y0 = y_train[[target_column]].loc[mask_g1y0]\n",
    "train_x_tensor_g1y0 = torch.tensor(x_train_g1y0.values, dtype=torch.float)\n",
    "train_y_tensor_g1y0 = torch.tensor(y_train_g1y0[[target_column]].values, dtype=torch.float)\n",
    "train_dataset_g1y0 = torch.utils.data.TensorDataset(train_x_tensor_g1y0, train_y_tensor_g1y0)\n",
    "train_dataloader_g1y0 = torch.utils.data.DataLoader(train_dataset_g1y0, batch_size=32,shuffle=True)\n",
    "\n",
    "mask_g1y1 = np.logical_and(mask_g1, mask_y1)\n",
    "x_train_g1y1 = X_train_scaled[features].loc[mask_g1y1]\n",
    "y_train_g1y1 = y_train[[target_column]].loc[mask_g1y1]\n",
    "train_x_tensor_g1y1 = torch.tensor(x_train_g1y1.values, dtype=torch.float)\n",
    "train_y_tensor_g1y1 = torch.tensor(y_train_g1y1[[target_column]].values, dtype=torch.float)\n",
    "train_dataset_g1y1 = torch.utils.data.TensorDataset(train_x_tensor_g1y1, train_y_tensor_g1y1)\n",
    "train_dataloader_g1y1 = torch.utils.data.DataLoader(train_dataset_g1y1, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_g0 = X_val[sensitive_feature] == 0\n",
    "mask_g1 = np.logical_not(mask_g0)\n",
    "\n",
    "mask_y0 = y_val[target_column] == 0\n",
    "mask_y1 = np.logical_not(mask_y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_g0y0 = np.logical_and(mask_g0, mask_y0)\n",
    "x_val_g0y0 = X_val_scaled[features].loc[mask_g0y0]\n",
    "y_val_g0y0 = y_val[[target_column]].loc[mask_g0y0]\n",
    "val_x_tensor_g0y0 = torch.tensor(x_val_g0y0.values, dtype=torch.float)\n",
    "val_y_tensor_g0y0 = torch.tensor(y_val_g0y0[[target_column]].values, dtype=torch.float)\n",
    "val_dataset_g0y0 = torch.utils.data.TensorDataset(val_x_tensor_g0y0, val_y_tensor_g0y0)\n",
    "\n",
    "mask_g0y1 = np.logical_and(mask_g0, mask_y1)\n",
    "x_val_g0y1 = X_val_scaled[features].loc[mask_g0y1]\n",
    "y_val_g0y1 = y_val[[target_column]].loc[mask_g0y1]\n",
    "val_x_tensor_g0y1 = torch.tensor(x_val_g0y1.values, dtype=torch.float)\n",
    "val_y_tensor_g0y1 = torch.tensor(y_val_g0y1[[target_column]].values, dtype=torch.float)\n",
    "val_dataset_g0y1 = torch.utils.data.TensorDataset(val_x_tensor_g0y1, val_y_tensor_g0y1)\n",
    "\n",
    "mask_g1y0 = np.logical_and(mask_g1, mask_y0)\n",
    "x_val_g1y0 = X_val_scaled[features].loc[mask_g1y0]\n",
    "y_val_g1y0 = y_val[[target_column]].loc[mask_g1y0]\n",
    "val_x_tensor_g1y0 = torch.tensor(x_val_g1y0.values, dtype=torch.float)\n",
    "val_y_tensor_g1y0 = torch.tensor(y_val_g1y0[[target_column]].values, dtype=torch.float)\n",
    "val_dataset_g1y0 = torch.utils.data.TensorDataset(val_x_tensor_g1y0, val_y_tensor_g1y0)\n",
    "\n",
    "mask_g1y1 = np.logical_and(mask_g1, mask_y1)\n",
    "x_val_g1y1 = X_val_scaled[features].loc[mask_g1y1]\n",
    "y_val_g1y1 = y_val[[target_column]].loc[mask_g1y1]\n",
    "val_x_tensor_g1y1 = torch.tensor(x_val_g1y1.values, dtype=torch.float)\n",
    "val_y_tensor_g1y1 = torch.tensor(y_val_g1y1[[target_column]].values, dtype=torch.float)\n",
    "val_dataset_g1y1 = torch.utils.data.TensorDataset(val_x_tensor_g1y1, val_y_tensor_g1y1)\n",
    "val_dataset = [val_dataset_g0y0,val_dataset_g0y1,val_dataset_g1y0,val_dataset_g1y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_data, optimizer, loss_fn, lamba):\n",
    "    # reset gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # unfold data\n",
    "    data_g0y0, data_g0y1, data_g1y0, data_g1y1 = input_data\n",
    "    x_batch_g0y0, y_batch_g0y0 = data_g0y0\n",
    "    x_batch_g0y1, y_batch_g0y1 = data_g0y1\n",
    "    x_batch_g1y0, y_batch_g1y0 = data_g1y0\n",
    "    x_batch_g1y1, y_batch_g1y1 = data_g1y1\n",
    "    \n",
    "    # get predictions\n",
    "    y_pred_propa_g0y0 = model.forward(x_batch_g0y0)\n",
    "    y_pred_propa_g0y1 = model.forward(x_batch_g0y1)\n",
    "    y_pred_propa_g1y0 = model.forward(x_batch_g1y0)\n",
    "    y_pred_propa_g1y1 = model.forward(x_batch_g1y1)\n",
    "\n",
    "    # calculate loss\n",
    "    loss_g0y0 = loss_fn(y_pred_propa_g0y0, y_batch_g0y0)\n",
    "    loss_g0y1 = loss_fn(y_pred_propa_g0y1, y_batch_g0y1)\n",
    "    loss_g1y0 = loss_fn(y_pred_propa_g1y0, y_batch_g1y0)\n",
    "    loss_g1y1 = loss_fn(y_pred_propa_g1y1, y_batch_g1y1)\n",
    "    total_loss = (loss_g0y0 + loss_g0y1 + loss_g1y0 + loss_g1y1)/4\n",
    "    \n",
    "    # regularization\n",
    "    regularization = torch.abs(torch.mean(y_pred_propa_g0y1) - torch.mean(y_pred_propa_g1y1))\n",
    "    \n",
    "    # total loss\n",
    "    loss = total_loss + lamba * regularization\n",
    "    \n",
    "    # compute gradients \n",
    "    loss.backward()\n",
    "    \n",
    "    # optimise network\n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute metrics for monitoring\n",
    "    with torch.no_grad(): \n",
    "        y_pred_propa = torch.concat([y_pred_propa_g0y0, y_pred_propa_g0y1,y_pred_propa_g1y0,y_pred_propa_g1y1])\n",
    "        y_true = torch.concat([y_batch_g0y0, y_batch_g0y1, y_batch_g1y0, y_batch_g1y1])\n",
    "        y_pred = (y_pred_propa>0.5) * 1\n",
    "        train_acc = torch.sum(y_pred == y_true) / y_pred.shape[0]\n",
    "\n",
    "    return loss.data.numpy(), train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(model, input_data, loss_fn, lamba):\n",
    "    # when we use torch.no_grad pytorch didnt store information\n",
    "    # that is required to calculate gradients so is fasterr \n",
    "    with torch.no_grad(): \n",
    "        data_g0y0, data_g0y1, data_g1y0, data_g1y1 = input_data\n",
    "        x_batch_g0y0, y_batch_g0y0 = data_g0y0.tensors\n",
    "        x_batch_g0y1, y_batch_g0y1 = data_g0y1.tensors\n",
    "        x_batch_g1y0, y_batch_g1y0 = data_g1y0.tensors\n",
    "        x_batch_g1y1, y_batch_g1y1 = data_g1y1.tensors\n",
    "\n",
    "        # get predictions\n",
    "        y_pred_propa_g0y0 = model(x_batch_g0y0)\n",
    "        y_pred_propa_g0y1 = model(x_batch_g0y1)\n",
    "        y_pred_propa_g1y0 = model(x_batch_g1y0)\n",
    "        y_pred_propa_g1y1 = model(x_batch_g1y1)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_g0y0 = loss_fn(y_pred_propa_g0y0, y_batch_g0y0)\n",
    "        loss_g0y1 = loss_fn(y_pred_propa_g0y1, y_batch_g0y1)\n",
    "        loss_g1y0 = loss_fn(y_pred_propa_g1y0, y_batch_g1y0)\n",
    "        loss_g1y1 = loss_fn(y_pred_propa_g1y1, y_batch_g1y1)\n",
    "        total_loss = (loss_g0y0 + loss_g0y1 + loss_g1y0 + loss_g1y1)/4\n",
    "\n",
    "        # regularization\n",
    "        regularization = torch.abs(torch.mean(y_pred_propa_g0y1) - torch.mean(y_pred_propa_g1y1))\n",
    "        \n",
    "        loss = total_loss + lamba* regularization\n",
    "\n",
    "        # compute metrics\n",
    "        y_pred_propa = torch.concat([y_pred_propa_g0y0, y_pred_propa_g0y1,y_pred_propa_g1y0,y_pred_propa_g1y1])\n",
    "        y_true = torch.concat([y_batch_g0y0, y_batch_g0y1, y_batch_g1y0, y_batch_g1y1])\n",
    "        y_pred = (y_pred_propa>0.5) * 1\n",
    "        acc = torch.sum(y_pred == y_true) / y_pred.shape[0]\n",
    "\n",
    "        \n",
    "    return loss.data.numpy(), acc.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_input_features=12)\n",
    "\n",
    "lr = 0.005\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.00)\n",
    "loss_fn = torch.nn.BCELoss() # binary cross entropy\n",
    "\n",
    "results_training = train_loop(train_dataloaders = (train_dataloader_g0y0, train_dataloader_g0y1, train_dataloader_g1y0, train_dataloader_g1y1),\n",
    "                     val_tensor_dataset= val_dataset,\n",
    "                     patient = 50,\n",
    "                     epochs=2000,\n",
    "                     model=model,\n",
    "                     optimizer=optimizer,\n",
    "                     loss_fn=loss_fn,\n",
    "                     lamba = 2.5,\n",
    "                     steps_per_epoch=100)\n",
    "model, val_history, train_history, best_loss, best_epoch, best_weights = results_training    \n",
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(test_tensor_dataset.tensors[0]).detach().numpy()\n",
    "predictions = (predictions>0.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_atribute = test_data[sensitive_feature]\n",
    "metrics = compute_metric(y_pred = predictions,\n",
    "                         data = test_data,\n",
    "                         sensitive_feature = sensitive_atribute)\n",
    "results3 = pd.DataFrame(metrics,\n",
    "                       index = [\"Reg EO model\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([all_results, results3],axis=1)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3. Equalized Odds\n",
    "\n",
    "Similarly, we can regularize the model to preserve Equalized Odds. Equalized Odds requires that the model has equal true positive rates (TPR) and equal false positive rates (FPR) across different sensitive groups, meaning that the probabilities of both a correct positive prediction and an incorrect positive prediction (i.e., $ \\hat{y} = 1 $) given the true label $ y = 1 $ or $ y = 0 $ should be the same for all sensitive groups.\n",
    "\n",
    "Formally, Equalized Odds can be expressed as:\n",
    "\n",
    "$$ \\mathbb{E}_{x \\sim P(x | y = 1, s = 0)}[p(\\hat{y} = 1 | x)] = \\mathbb{E}_{x \\sim P(x | y = 1, s = 1)}[p(\\hat{y} = 1 | x)] $$\n",
    "\n",
    "$$ \\mathbb{E}_{x \\sim P(x | y = 0, s = 0)}[p(\\hat{y} = 1 | x)] = \\mathbb{E}_{x \\sim P(x | y = 0, s = 1)}[p(\\hat{y} = 1 | x)] $$\n",
    "\n",
    "The Equalized Odds difference (EO) can be defined as:\n",
    "\n",
    "$$ R1 = \\left| \\mathbb{E}_{x \\sim P(x | y = 1, s = 0)}[p(\\hat{y} = 1 | x)] - \\mathbb{E}_{x \\sim P(x | y = 1, s = 1)}[p(\\hat{y} = 1 | x)] \\right| $$\n",
    "\n",
    "\n",
    "$$ R1 =  \\left| \\mathbb{E}_{x \\sim P(x | y = 0, s = 0)}[p(\\hat{y} = 1 | x)] - \\mathbb{E}_{x \\sim P(x | y = 0, s = 1)}[p(\\hat{y} = 1 | x)] \\right| $$\n",
    "\n",
    "$$\n",
    "\\text{EO} =  R1 + R2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ p(\\hat{y} = 1 | x) $ is the predicted probability of $ y = 1 $ given input $ x $,\n",
    "- $ y = 1 $ and $ y = 0 $ denote the positive and negative classes, respectively,\n",
    "- $ s $ denotes the sensitive attribute (e.g., gender, race),\n",
    "- The expectations are taken over the input data conditioned on the true label $ y $ and the sensitive attribute $ s $.\n",
    "\n",
    "By regularizing the model according to this criterion, we ensure that the model maintains equal true positive and false positive rates across the sensitive groups, thus promoting fairness in the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_data, optimizer, loss_fn, lamba):\n",
    "    # reset gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # unfold data\n",
    "    data_g0y0, data_g0y1, data_g1y0, data_g1y1 = input_data\n",
    "    x_batch_g0y0, y_batch_g0y0 = data_g0y0\n",
    "    x_batch_g0y1, y_batch_g0y1 = data_g0y1\n",
    "    x_batch_g1y0, y_batch_g1y0 = data_g1y0\n",
    "    x_batch_g1y1, y_batch_g1y1 = data_g1y1\n",
    "    \n",
    "    # get predictions\n",
    "   \n",
    "    # calculate loss\n",
    "    loss_g0y0 = \n",
    "    loss_g0y1 = \n",
    "    loss_g1y0 = \n",
    "    loss_g1y1 = \n",
    "    total_loss = (loss_g0y0 + loss_g0y1 + loss_g1y0 + loss_g1y1)/4\n",
    "    \n",
    "    # regularization\n",
    "    regularization1 =  # R1 True posive rates\n",
    "    regularization2 =  # R2 False posive rates\n",
    "    regularization = (regularization1 + regularization2)/2\n",
    "    \n",
    "    # total loss\n",
    "    loss = \n",
    "    \n",
    "    # compute gradients \n",
    "    loss.backward()\n",
    "    \n",
    "    # optimise network\n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute metrics for monitoring\n",
    "    with torch.no_grad(): \n",
    "        ### accuracy\n",
    "        train_acc = \n",
    "\n",
    "    return loss.data.numpy(), train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(model, input_data, loss_fn, lamba):\n",
    "    # when we use torch.no_grad pytorch didnt store information\n",
    "    # that is required to calculate gradients so is fasterr \n",
    "    with torch.no_grad(): \n",
    "        data_g0y0, data_g0y1, data_g1y0, data_g1y1 = input_data\n",
    "        x_batch_g0y0, y_batch_g0y0 = data_g0y0.tensors\n",
    "        x_batch_g0y1, y_batch_g0y1 = data_g0y1.tensors\n",
    "        x_batch_g1y0, y_batch_g1y0 = data_g1y0.tensors\n",
    "        x_batch_g1y1, y_batch_g1y1 = data_g1y1.tensors\n",
    "\n",
    "        # get predictions\n",
    "        y_pred_propa_g0y0 =\n",
    "        y_pred_propa_g0y1 = \n",
    "        y_pred_propa_g1y0 = \n",
    "        y_pred_propa_g1y1 = \n",
    "\n",
    "        # calculate loss\n",
    "        loss_g0y0 = \n",
    "        loss_g0y1 = \n",
    "        loss_g1y0 =\n",
    "        loss_g1y1 = \n",
    "        total_loss = (loss_g0y0 + loss_g0y1 + loss_g1y0 + loss_g1y1)/4\n",
    "\n",
    "        # regularization\n",
    "        regularization1 = \n",
    "        regularization2 = \n",
    "        regularization = (regularization1 + regularization2)/2\n",
    "        \n",
    "        # total loss\n",
    "        loss = total_loss + lamba * regularization\n",
    "\n",
    "        # compute metrics\n",
    "        acc = \n",
    "\n",
    "        \n",
    "    return loss.data.numpy(), acc.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_input_features=12)\n",
    "\n",
    "lr = 0.005\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.00)\n",
    "loss_fn = torch.nn.BCELoss() # binary cross entropy\n",
    "\n",
    "results_training = train_loop(train_dataloaders = (train_dataloader_g0y0, train_dataloader_g0y1, train_dataloader_g1y0, train_dataloader_g1y1),\n",
    "                     val_tensor_dataset= val_dataset,\n",
    "                     patient = 50,\n",
    "                     epochs=2000,\n",
    "                     model=model,\n",
    "                     optimizer=optimizer,\n",
    "                     loss_fn=loss_fn,\n",
    "                     lamba = 2.50,\n",
    "                     steps_per_epoch=100)\n",
    "model, val_history, train_history, best_loss, best_epoch, best_weights = results_training    \n",
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(test_tensor_dataset.tensors[0]).detach().numpy()\n",
    "predictions = (predictions>0.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_atribute = test_data[sensitive_feature]\n",
    "metrics = compute_metric(y_pred = predictions,\n",
    "                         data = test_data,\n",
    "                         sensitive_feature = sensitive_atribute)\n",
    "results3 = pd.DataFrame(metrics,\n",
    "                       index = [\"Reg Eoods model\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([all_results, results3],axis=1)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples \n",
    "### 1. Hiring - **Demographic Parity (DP) **\n",
    "\n",
    "**Scenario**: A hiring model predicts whether a candidate will be hired. The sensitive attribute is gender.\n",
    "\n",
    "- **Goal**: Ensure that the predicted probability of being hired is the same for both men and women.\n",
    "- **Intuition**: If the model predicts a higher chance of hiring for men than women, it may be biased. Demographic Parity ensures equal hiring chances for both genders.\n",
    "- **Why We Need It**: Demographic Parity helps to avoid systemic discrimination based on gender. In hiring, if one group is consistently predicted to have a higher chance of being hired, it perpetuates historical biases, which could lead to unfair practices and potential legal violations. Ensuring equal predicted hiring probabilities is key to maintaining fairness.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.  Health (Cancer Detection) - **Equalized Opportunities (EO) **\n",
    "\n",
    "**Scenario**: A cancer detection system predicts whether a patient has cancer. The sensitive attribute is race.\n",
    "\n",
    "- **Goal**: Ensure that the model has the same True Positive Rate (TPR) for both Black and White patients.\n",
    "- **Intuition**: If the model detects cancer in Black patients at a lower rate than White patients, itâ€™s unfair. Equalized Opportunities ensures both groups have equal chances of correctly identifying cancer when itâ€™s present.\n",
    "- **Why We Need It**: In cancer detection, the main focus is on detecting the disease accurately (True Positive Rate). **We care less about false positives** in this context, as a false positive (indicating cancer when there is none) might lead to further testing but doesn't necessarily harm the patient as much as missing a cancer diagnosis would. Therefore, Equalized Opportunities ensures that both groups have an equal chance of being correctly diagnosed with cancer, without prioritizing the False Positive Rate, which could be less critical in this context.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.Loan Approval -  **Equalized Odds (Eodds) **\n",
    "\n",
    "**Scenario**: A loan approval model predicts whether a person will default on a loan. The sensitive attribute is age.\n",
    "\n",
    "- **Goal**: Ensure that the model has equal True Positive Rates (TPR) and False Positive Rates (FPR) for both younger and older applicants.\n",
    "- **Intuition**: If the model denies credit to younger applicants more often than older ones, or incorrectly approves more older applicants, Equalized Odds ensures both groups are treated fairly in terms of correct and incorrect predictions.\n",
    "- **Why We Need It**: In loan approval, **we care about both True Positive and False Positive Rates**. A False Positive (approving a loan to someone who will default) can lead to financial loss for the lender, while a True Positive (denying credit to someone who will default) protects the lender. Equalized Odds ensures that both younger and older applicants are treated equally by the model, reducing the chance of either group being unfairly burdened by mistakes (such as false denials or unjust approvals).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Emphasis:\n",
    "\n",
    "- **Demographic Parity** focuses on **equal predicted probabilities** (e.g., being hired) across groups and helps eliminate systematic bias, particularly in sensitive areas like hiring.\n",
    "- **Equalized Opportunities** focuses on ensuring **equal true positive rates** across groups (e.g., correctly diagnosing cancer) and is crucial when missing an important outcome (like a cancer diagnosis) can have a severe impact.\n",
    "- **Equalized Odds** focuses on both **True Positive Rates and False Positive Rates**, making it particularly important in contexts like loan approval where both types of prediction errors (underestimating risk or falsely denying credit) have significant consequences.\n",
    "\n",
    "By focusing on the right fairness metric in the right context, we can ensure that the model aligns with the most important ethical considerations and practical needs of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
