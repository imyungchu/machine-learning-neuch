{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/fair_1.png\" width=\"200\"/>\n",
    "</figure>\n",
    "\n",
    "# Lab 7 - Fairness of Predictive Models\n",
    "\n",
    "Today, we will discuss different notions of fairness in the predictive setting.\n",
    "\n",
    "More specifically, we will go through a running example using the Adult dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning (ML) usage is increasing every day. Companies and organisations build ML models with the purpose of minimising human effort or improving performance in various tasks. So today Machine learning arguably affects our lives.\n",
    "\n",
    "Examples:\n",
    "1. Recommendation system\n",
    "2. Face recognition\n",
    "3. Self-driving cars\n",
    "4. Candidate selection e.x college admissions, CV screening\n",
    "5. Loan admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Notions\n",
    "\n",
    "When machine learning models are being used to make decisions, they cannot be separated from the social and ethical context in which they are applied, and those developing and deploying these models must take care to do so in a manner that accounts for both performance and fairness. \n",
    "\n",
    "So in the last decade Fairness has become one of the most active research areas in ML.\n",
    "\n",
    "The most popular notions are **Individual Fairness** and **Group Fairness**.\n",
    "\n",
    "### Individual Fairness \n",
    "Individual fairness is a notion of fairness, meaning that similar individuals should be treated similarly.\n",
    "\n",
    "### Group Fairness\n",
    "In group fairness, different groups should be treated equally, ensuring fair outcomes across demographic groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPAS Dataset and Fairness Issues\n",
    "There are a lot of examples that researchers demonstrate inadvertently discriminating against several population groups.\n",
    "\n",
    "The most know is [ProPublica's](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis) research in COMPAS Dataset.\n",
    "\n",
    "The **COMPAS** (Correctional Offender Management Profiling for Alternative Sanctions) tool was used in Florida, to predict the likelihood of defendants reoffending. The model assigns a risk score based on factors like criminal history, age, and demographic data. Judges used these scores to inform decisions about bail, sentencing, and parole.\n",
    "\n",
    "However, investigations revealed significant **racial bias** in the model’s predictions. \n",
    "Black defendants were more likely to be assigned higher risk scores, even when they did not reoffend.\n",
    "Black defendants had twice \"false positive rates\" from white defendants.\n",
    "\n",
    "These disparities led to concerns about **Algorithmic Fairness**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Sources of Bias.\n",
    "\n",
    "Bias may be introduced into a machine learning project at any step along the way, and it is important to carefully think through each potential source and how it may affect your results.\n",
    "\n",
    "\n",
    "Source of bias:\n",
    "- Data\n",
    "    1. Historical injustice.\n",
    "    2. Sample bias - Collection Bias.\n",
    "    3. Limited features.\n",
    "    4. Unbalanced dataset.\n",
    "    5. Proxy Variables.\n",
    "- Modelling\n",
    "    1. Data preprocessing\n",
    "    2. Model assumptions \n",
    "- Feedback loops.\n",
    "    1. decisions based on biassed models lead to biassed dataset.  \n",
    "\n",
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/simple_pipeline.png\" width=\"600\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3. Load Dataset\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_types = OrderedDict([\n",
    "    (\"age\", \"int\"),\n",
    "    (\"workclass\", \"category\"),\n",
    "    (\"final_weight\", \"int\"),  # originally it was called fnlwgt\n",
    "    (\"education\", \"category\"),\n",
    "    (\"education_num\", \"int\"),\n",
    "    (\"marital_status\", \"category\"),\n",
    "    (\"occupation\", \"category\"),\n",
    "    (\"relationship\", \"category\"),\n",
    "    (\"race\", \"category\"),\n",
    "    (\"sex\", \"category\"),\n",
    "    (\"capital_gain\", \"float\"),  # required because of NaN values\n",
    "    (\"capital_loss\", \"int\"),\n",
    "    (\"hours_per_week\", \"int\"),\n",
    "    (\"native_country\", \"category\"),\n",
    "    (\"income_class\", \"category\"),\n",
    "])\n",
    "target_column = \"income_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        names=data_types,\n",
    "        index_col=None,\n",
    "\n",
    "        comment='|',  # test dataset has comment in it\n",
    "        skipinitialspace=True,  # Skip spaces after delimiter\n",
    "        na_values={\n",
    "            'capital_gain': 99999,\n",
    "            'workclass': '?',\n",
    "            'native_country': '?',\n",
    "            'occupation': '?',\n",
    "        },\n",
    "        dtype=data_types,\n",
    "    )\n",
    "\n",
    "def clean_dataset(data):\n",
    "    # Test dataset has dot at the end, we remove it in order\n",
    "    # to unify names between training and test datasets.\n",
    "    data['income_class'] = data.income_class.str.rstrip('.').astype('category')\n",
    "    \n",
    "    # Remove final weight column since there is no use\n",
    "    # for it during the classification.\n",
    "    data = data.drop('final_weight', axis=1)\n",
    "    \n",
    "    # Duplicates might create biases during the analysis and\n",
    "    # during prediction stage they might give over-optimistic\n",
    "    # (or pessimistic) results.\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    # Binary target variable (>50K == 1 and <=50K == 0)\n",
    "    data[target_column] = (data[target_column] == '>50K').astype(int)\n",
    "    \n",
    "    # Categorical dataset\n",
    "    categorical_features = data.select_dtypes('category').columns\n",
    "    data[categorical_features] = data.select_dtypes('category').apply(lambda x: x.cat.codes)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load & clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and clean train dataset\n",
    "TRAIN_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "train_data = clean_dataset(read_dataset(TRAIN_DATA_FILE))\n",
    "train_data = train_data.dropna()\n",
    "print(\"Train dataset shape:\", train_data.shape)\n",
    "\n",
    "# get and clean test dataset\n",
    "TEST_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "test_data = clean_dataset(read_dataset(TEST_DATA_FILE))\n",
    "test_data = test_data.dropna()\n",
    "print(\"Test dataset shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"income_class\"\n",
    "sensitive_feature = \"sex\"\n",
    "features = train_data.columns.difference([target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig = sns.countplot(data=train_data[[\"income_class\",\"sex\"]],\n",
    "                    x=\"income_class\",\n",
    "                    hue=\"sex\")\n",
    "fig.set_xticklabels(['income <= 50K','income > 50K'])\n",
    "plt.legend(title='sex', labels=['woman', 'men'])\n",
    "plt.title(\"Num of values in each category\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(train_data[features]),columns = features)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(test_data[features]),columns = features) # note that here we just use the transform method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite imbalance between a gender, so we expect our model to be unfair.\n",
    "The source of the bias is coming from our societal bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4. Train a Model\n",
    "let's train our model and analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "model.fit(X = X_train[features],\n",
    "          y = train_data[target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test[features])\n",
    "accuracy = accuracy_score(y_true=test_data[target_column], y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(y_true = test_data[target_column],\n",
    "                           y_pred = model.predict(X_test[features]))\n",
    "print(\"Accuracy_score:\", acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5. Analysis of model fairness\n",
    "\n",
    "\n",
    "Unfortunately, there is no single definition of fairness. Many competing approaches exist to measure whether a model is statistically “fair.” It is essential to start with the social and policy goals for equity and fairness, and then map those goals to the statistical properties we seek in our models to help achieve them. Most definitions involve dividing the population into groups and comparing specific metrics across these groups.\n",
    "\n",
    "Different definitions prioritize different fairness aspects, so the appropriate choice of fairness definition depends on the application. This typically requires a careful consideration of the project’s goals and a thorough discussion between data scientists, decision-makers, and those impacted by the model's use.\n",
    "\n",
    "Below, we define some of the most widely recognized fairness definitions:\n",
    "\n",
    "1. **Demographic Parity** - equal positive outcome rates across groups.\n",
    "2. **Equalized Opportunities** - equal true positive rates across groups.\n",
    "3. **Equalized Odds** - equal true positive and false positive rates across groups.\n",
    "4. **Predictive Parity** - equal positive predictive values (precision) across groups.  \n",
    "5. **Equal Accuracy** - Equal overall accuracy rates across groups.  \n",
    ".  \n",
    ".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test[features])\n",
    "predictions = pd.Series(predictions,index = test_data.index)\n",
    "sensitive_atribute = test_data[sensitive_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_mask = sensitive_atribute == 0\n",
    "predictions_woman = predictions.loc[woman_mask]\n",
    "labels_woman = test_data.loc[woman_mask, target_column]\n",
    "\n",
    "acc_score1 = accuracy_score(y_true = labels_woman,\n",
    "                            y_pred = predictions_woman)\n",
    "print(\"Accuracy_score:\", acc_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_mask = sensitive_atribute == 1\n",
    "predictions_man = predictions.loc[man_mask]\n",
    "labels_man = test_data.loc[man_mask, target_column]\n",
    "\n",
    "acc_score2 = accuracy_score(y_true = labels_man,\n",
    "                            y_pred = predictions_man)\n",
    "print(\"Accuracy_score:\", acc_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diffrence_on_accuracy = np.abs(acc_score1 - acc_score2)\n",
    "Diffrence_on_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1.  Demographic Parity\n",
    "\n",
    "**Demographic parity** , also referred to as **statistical parity** , **acceptance rate parity**  and **benchmarking**\n",
    "\n",
    "Demographic Parity states that the positive outcome rates between sensitive group must be the same.\n",
    "\n",
    "---------------\n",
    "> **Definition Demographic Parity :** We say that a predictor $\\hat{Y}$ satisfy **demographic parity** if the predictions $\\hat{Y}$ are independent of the sensitive atribute $S$, $\\hat{Y} \\perp S$.\n",
    "$$ P_r(\\hat{Y}= 1 | S = s ) =  P_r(\\hat{Y} = 1), \\quad  \\forall s\\in S $$\n",
    "\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we mesure the bias of model as follows:\n",
    "$$DP = \\; \\mid P_r(\\hat{Y}= 1 | S = 0 ) -  P_r(\\hat{Y}= 1 | S = 1 ) \\mid $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity_difference(predictions, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of demographic parity difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the propotion of positive predictions.\n",
    "    2. Calcaulte the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the proportion of positive predictions.\n",
    "    # group 0 \n",
    "\n",
    "\n",
    "    positive_rate_group0 = \n",
    "\n",
    "    # group 1 \n",
    "    \n",
    "    positive_rate_group1 = \n",
    "    \n",
    "    # 3. Calculate the different.\n",
    "    difference = np.abs(positive_rate_group0-positive_rate_group1)\n",
    "    \n",
    "    return difference, [positive_rate_group0, positive_rate_group1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test[features])\n",
    "sensitive_attribute = test_data[\"sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_metric, rates = demographic_parity_difference(predictions, sensitive_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Demographic parity difference is: {round(demographic_metric,5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The rate of different groups are: {rates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "**When to use demographic parity?** \n",
    "1. We are aware of **historical biases** that may have affected the quality of our data. E.x not presents of specific minority in a specific work.\n",
    "2. We want to **change the state of our current world to improve it**. For example, we want to have equal admitions of different group in a specific project.\n",
    "\n",
    "\n",
    "**Potential issues?** \n",
    "1. **Laziness**: We can satisfy demographic parity in we accept random people in group S=0 but qualified people in group  b unless we have the same proportion of positive outcome. We avoid lasyness because classfication is usually perfomred by optimizing an perfomance metric.\n",
    "2. **Not optimality compatible**: A classifier that satisfy demographic parity is suboptimal, if the dataset demographic parity is not hold.\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2. Equalized Opportunities\n",
    "\n",
    "**Equalized Opportunities** , also referred to as **true positive parity**, **sensitivity**.\n",
    "\n",
    "Equalized Opportunities states that each group has equal true positive rates.\n",
    "\n",
    "---------------\n",
    "> **Definition Equalized Opportunities :** We say that a predictor $\\hat{Y}$ satisfy **equalized opportunities** if the predictions $\\hat{Y}$ are independent of the sensitive atribute $S$, conditioned on the positive actual outcome \\( Y = 1 \\), $\\hat{Y} \\perp S \\mid Y = 1$.\n",
    "$$ P_r(\\hat{Y} = 1 | S = s, Y = 1) = P_r(\\hat{Y} = 1 | Y = 1), \\quad \\forall s \\in S $$\n",
    "\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we mesure the bias of model as follows:\n",
    "$$EO = \\; \\mid P_r(\\hat{Y} = 1 | S = 0, Y = 1) -  P_r(\\hat{Y} = 1 | S = 1, Y = 1) \\mid $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_opportunities_difference(predictions, actual, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of equalized opportunities difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the TPR.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calculate the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the true positive rates.\n",
    "    # group 0 \n",
    "   \n",
    "    true_positive_rate_group0 = \n",
    "\n",
    "    # group 1 \n",
    "\n",
    "    true_positive_rate_group1 = \n",
    "    \n",
    "    # 3. Calculate the different.\n",
    "    difference = np.abs(true_positive_rate_group0-true_positive_rate_group1)\n",
    "    \n",
    "    return difference, [true_positive_rate_group0, true_positive_rate_group1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test[features])\n",
    "sensitive_attribute = test_data[\"sex\"]\n",
    "actual = test_data[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "equalized_opportunities_metric, _ = equalized_opportunities_difference(predictions, actual, sensitive_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"equalized opportunities difference is: {round(equalized_opportunities_metric,5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "**When to use equalized opportunities?**\n",
    "1. When is a strong emphasis on predicting the positive outcome correctly. e.x college admissions\n",
    "2. When False Positives are not costly for the stakeholder. e.x spam detection\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3. Equalized Odds\n",
    "\n",
    "**Equalized Odds** , also referred to as\n",
    "\n",
    "Equalized Odds states that the true positive rates (TPR) and false positive rates (FPR) between sensitive group must be the same.\n",
    "\n",
    "---------------\n",
    "> **Definition Equalized Odds :** A classifier $C$ We say that a predictor $\\hat{Y}$ satisfy **equalized Odds** if the predictions $\\hat{Y}$ are independent of the sensitive atribute $S$, conditioned on the actual outcome \\( Y \\), $\\hat{Y} \\perp S \\mid Y$.\n",
    "$$ P_r(\\hat{Y} = 1 | S = s, Y = y) = P_r(\\hat{Y} = 1 | Y = y), \\quad \\forall s \\in S \\quad \\forall y \\in Y$$\n",
    "\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we want to satisfy both:\n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 1 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 1 ) $$ \n",
    "and \n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 0 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 0 ) $$  \n",
    "---------------\n",
    "we mesure the bias of model as follows:\n",
    "$$EOds = \\; \\mid P_r(\\hat{Y} = 1 | S = 0, Y = 1) -  P_r(\\hat{Y} = 1 | S = 1, Y = 1) \\mid + \\mid P_r(\\hat{Y} = 1 | S = 0, Y = 0) -  P_r(\\hat{Y} = 1 | S = 1, Y = 0) \\mid  $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_odds_difference(predictions, data, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of equalized odds difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the TPR.\n",
    "    2. Calcaulte the difference.\n",
    "    3. For each group calculate the TPR.\n",
    "    4. Calcaulte the difference.\n",
    "    5. sum the Calcaulte the differences\n",
    "    \"\"\"\n",
    "    # A. For each group calculate the true positive rate.\n",
    "    # group 0 \n",
    "\n",
    "    true_positive_rate_group0 = \n",
    "\n",
    "    # group 1 \n",
    "    true_positive_rate_group1 = \n",
    "    \n",
    "    # Calculate the different.\n",
    "    difference1 = np.abs(true_positive_rate_group0-true_positive_rate_group1)\n",
    "    \n",
    "    # B. For each group calculate the false positive rate.\n",
    "    # group 0 \n",
    "  \n",
    "    false_positive_rate_group0 = \n",
    "    # group 1 \n",
    "    \n",
    "    false_positive_rate_group1 = \n",
    "    \n",
    "    # Calculate the different.\n",
    "    difference2 = np.abs(false_positive_rate_group0-false_positive_rate_group1)\n",
    "    \n",
    "    # Total difference\n",
    "    difference = \n",
    "    return difference, ([true_positive_rate_group0,true_positive_rate_group1], [false_positive_rate_group0,false_positive_rate_group1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_metrics, _ = equalized_odds_difference(predictions=predictions,\n",
    "                                                      data=test_data,\n",
    "                                                      sensitive_attribute=test_data[\"sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"equalized odds difference is: {round(equalized_odds_metrics,5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([acc_score,\n",
    "                        demographic_metric,\n",
    "                        equalized_opportunities_metric,\n",
    "                        equalized_odds_metrics], \n",
    "                       index = [\"accuracy\",\n",
    "                                \"demographic_metric\",\n",
    "                                \"equalized_opportunities\",\n",
    "                                \"equalized_odds\"],\n",
    "                       columns = [\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your project you have explained why you choose the specific fairness criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(y_pred, data, sensitive_feature):\n",
    "    # get all metrics\n",
    "    acc_score = accuracy_score(y_true=data[target_column], y_pred=y_pred)\n",
    "    \n",
    "    demographic_metric, _ = demographic_parity_difference(y_pred,\n",
    "                                                          sensitive_feature)\n",
    "    \n",
    "    equalized_opportunities_metric, _ = equalized_opportunities_difference(y_pred,\n",
    "                                                                           data,\n",
    "                                                                          sensitive_feature)\n",
    "    equalized_odds_metrics, _ = equalized_odds_difference(y_pred,\n",
    "                                                          data,\n",
    "                                                          sensitive_feature)\n",
    "    \n",
    "    return {\n",
    "            \"accuracy\": acc_score,\n",
    "            \"demographic_metric\": demographic_metric,\n",
    "            \"equalized_opportunities\" : equalized_opportunities_metric,\n",
    "            \"equalized_odds\": equalized_odds_metrics\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Correct the Unfair Classifier\n",
    "\n",
    "Numerous recent papers have proposed mechanisms to enhance fairness in machine learning algorithms.\n",
    "\n",
    "In summary, there are three general methods to correct an unfair classifier:\n",
    "\n",
    "1. **Pre-Processing**: Make changes to the data before training the model, e.g., removing correlated features.\n",
    "2. **In-Processing**: Make changes to the model to correct fairness, e.g., adding additional loss terms to ensure fairness.\n",
    "3. **Post-Processing**: Make changes after the model's output, e.g., adjusting classification thresholds.\n",
    "\n",
    "For an overview of different methods and fairness criteria, we refer you to the following interesting survey: [https://arxiv.org/pdf/2001.09784.pdf](https://arxiv.org/pdf/2001.09784.pdf).\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"./figures/corect_unfairness.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-Processing\n",
    "\n",
    "Unfair models can often result from bias in the data. Pre-processing methods aim to remove bias from the data before it is used to train models. The data is transformed, and then the algorithm is trained as it would be on the original dataset.\n",
    "\n",
    "Pre-processing techniques work by transforming the data into a different representation to remove bias. Common approaches include re-balancing techniques, removing sensitive features and their proxy variables, or using machine learning techniques, such as representation learning, to transform the data into a fair representation.\n",
    "\n",
    "**Pros:**\n",
    "1. Can be applied to any downstream task.\n",
    "2. Independent of the final classification model.\n",
    "3. Most approaches do not require access to sensitive features after training.\n",
    "\n",
    "**Cons:**\n",
    "1. May reduce performance (e.g., lower accuracy).\n",
    "2. Cannot address all fairness criteria, as some depend on the classifier. For example, ensuring equalized odds requires access to the classifier's outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. In-Processing\n",
    "\n",
    "In-Processing techniques incorporate fairness into the optimization process of a classifier. In general, this approach introduces constraints or regularization terms in the objective function during training. Different methods have been proposed, either as general frameworks or solutions based on specific classification algorithms.\n",
    "\n",
    "**Pros:**\n",
    "1. Good performance on both accuracy and fairness measures.\n",
    "2. Flexibility in balancing the trade-off between performance and fairness.\n",
    "3. No need for access to sensitive features after training.\n",
    "\n",
    "**Cons:**\n",
    "1. Certain constraints may be difficult to apply.\n",
    "2. Limited to specific classifiers.\n",
    "3. Requires modifying the classifier to include constraints during retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Post-Processing\n",
    "\n",
    "These methods post-process the output score of a classifier to remove bias. The most common techniques use different classification thresholds for each group to satisfy fairness criteria.\n",
    "\n",
    "**Pros:**\n",
    "1. Can be applied after any classifier.\n",
    "2. Relatively good performance, especially on fairness measures.\n",
    "3. No need to modify the classifier itself.\n",
    "\n",
    "**Cons:**\n",
    "1. Often requires test-time access to the protected attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1 Unawareness\n",
    "A very intuitive but **wrong solution** is to assume that ignoring the sensitive feature we can build a fair classifier.\n",
    "This approach is wrong because other feature may be correlated with our sensitive features.\n",
    "\n",
    "We will build a **Unawareness** classifier and the compare the results with our previous classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([results, unaware_results],axis=1)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Our conclusion is that we are not able to vanish unfairness by excluding sensitive features.  \n",
    "The reason in that some other feature leak information about our sensitive attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2 Remove correlated features\n",
    "From the above results we expect that our sensitive feature are correlate with non-sensitive feature in our analysis.\n",
    "We will try to find and exclude also those feature in order to improve our fairness properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = {}\n",
    "for f in non_sensitive_features:\n",
    "    corr[f] =np.corrcoef(test_data[sensitive_feature], test_data[f])[0,1]\n",
    "corr = np.abs(pd.DataFrame(data=corr, index=[\"corr\"]).T).sort_values(by=\"corr\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "corr.plot(kind=\"bar\")\n",
    "plt.title(\"Correlation of sensitive attribute\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteratively Exclude Correlated Features\n",
    "\n",
    "Let's build a classifier while progressively removing the most correlated features in each step. This helps us see how removing correlated features affects model accuracy.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Get Feature Correlations**  \n",
    "   Calculate the correlation matrix and identify the most correlated features.\n",
    "\n",
    "2. **Iterate Over Top Correlated Features**  \n",
    "   For each step \\( k \\):\n",
    "   - Exclude the top \\( k \\) most correlated features.\n",
    "   - Train a classifier with the remaining features.\n",
    "   - Measure fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "k_most_correlated = corr.iloc[0:k].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k_most_correlated)):\n",
    "    print(f\"===========iteration{i+1}===========\")\n",
    "    \n",
    "    # 1. train\n",
    "   \n",
    "    # 2. compute matrix\n",
    "   \n",
    "    # 3. append result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to Remember\n",
    "\n",
    "1. **ML models can be biased due to various sources,** with data-dependent biases being the most common.\n",
    "2. **Selecting the appropriate fairness criterion** is essential and should align with the specific requirements of our problem.\n",
    "3. **Fairness metrics can be calculated** using different rates for different groups.\n",
    "4. **Removing sensitive features does not guarantee fairness** because proxy variables can still introduce bias. In many cases, removing these features does not significantly influence the algorithm.\n",
    "5. **Biases can be addressed through pre-processing, in-processing, or post-processing methods.**\n",
    "6. **Different methods involve trade-offs** between fairness and model performance, so choosing the right approach depends on the acceptable balance between these two factors.\n",
    "\n",
    "For an overview of different methods and fairness critiriaa we refer you to the following survey https://arxiv.org/pdf/2001.09784.pdf.\n",
    "\n",
    "Fairness python libraries:\n",
    "https://aif360.readthedocs.io/en/stable/modules/algorithms.html\n",
    "https://fairlearn.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-society",
   "language": "python",
   "name": "ml-society"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
