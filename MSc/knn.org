#+TITLE: Nearest Neighbour Algorithms
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_CLASS_OPTIONS: [10pt]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}
* Introduction
** The hidden secret of machine learning
*** Fill-in class data

#+CAPTION: Spreadsheet link
#+ATTR_LATEX: :width 0.5\textwidth
[[./fig/class_data_QR.png]]




*** (Batch) learning from data

- $x_t \in \CX$, the *input* variables (or *features*)
- Usually $\CX = \Reals^n$: the n-dimensional Euclidean space
#+BEAMER: \pause
 
**** Unsupervised learning:
- Training examples $(x_1, \ldots  x_T)$,
- Predict $x_{t}$ for $t > T$.

#+BEAMER: \pause
**** Supervised learning:  
- Given labelled training examples $(x_1, y_1), \ldots (x_T, y_T)$
- $y_t \in \CY$, the *output* variables (or *labels*)
- For new $x_t$, $t > T$, predict $y_t$.
#+BEAMER: \pause
**** Classification vs Regression
- Classification: $\CY = \{1, \ldots, m\}$ are *discrete* labels
- Regression: $\CY = \Reals^m$ are *continuous* values



*** The kNN algorithm idea

- Assume an unknown example is similar to its neighbours
- Smoothness allows us to make predictions

/Discriminatory analysis-nonparametric discrimination: consistency properties/, Evelyn Fix and Joseph L.  Hodges Jr, 1951.

**** Evelyn :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+CAPTION: Evelyn Fix
#+ATTR_LATEX: :width 0.5\textwidth
[[../fig/fix_evelyn2.jpg]]
**** Joseph :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+CAPTION: Joseph Hodges
#+ATTR_LATEX: :width 0.5\textwidth
[[../fig/Hodges.jpg]]





*** Performance of KNN on image classification
[[../fig/knn-image-performance.png]]

- Really simple!
- Can outperform really complex models!

* The algorithm
** $k$ Nearest Neighbours

*** The Nearest Neighbour algorithm
**** Pseudocode
- Input: Data $(x_t, y_t)_{t=1}^T$, test point $x$, distance $d$ 
- $t^* = \argmin_t d(x_t, x)$ /// How do we implement this?
- Return $\hat{y}_t = y_{t^*}$

**** Classification
     $\hat{y}_t  \in [m] \equiv \{1, \ldots, m\}$
     
**** Regression
$\hat{y}_t  \in \Reals^m$

*** The k-Nearest Neighbour algorithm

**** Pseudocode
- Input: Data $(x_t, y_t)_{t=1}^T$, test point $x$, distance $d$, neighbours \(k\)
- Calculate $h_t = d(x_t, x)$ for all $t$.
- Get sorted indices $s = \texttt{argsort}(h)$ so that $d(x_{s_i}, x) \leq d(x_{s_{i+1}}, x)$ for all $i$. (How?)
- Return $\sum_{i=1}^k y_{s_i} / k$.

**** Classification
- It is not convenient to work with discrete labels.
- We use a *one-hot encoding* $(0, \ldots, 0, 1, 0, \ldots, 0)$.
- $y_t \in \{0,1\}^m$ with $\|y_t\|_1 = 1$, so that the class of the \(t\)-th example is $j$ iff $y_{t,j} = 1$.

**** Regression
- $y_t  \in \Reals^m$, so we need do nothing

*** Making a decision
**** kNN: A *model* of the conditional distribution $P(y | x)$
- Given features $x$, we get a vector
- $p_i = \hat{\Pr}(y = i | x)$.
**** The optimal decision rule $\pi$ derived from kNN
- Classification decision $a_t \sim \pi(a | x_t)$
- $a_t \in \CA$ but $\CA \neq \CY$, e.g. can include "Do not Know", or "Alert" etc.
- Actual label $y_t$
- $U(a_t, y_t)$: utility function depending on the application.
**** Decision rule maximising accuracy
- $a_t = \argmax_i \hat{\Pr}(y = i | x)$.

** Extensions and parameters
*** The number of neighbours
**** $k=1$
- How does it perform on the training data?
- How might it perform on unseen data?
**** $k = T$
- How does it perform on the training data?
- How might it perform on unseen data?

*** Distance function
**** For data in $\Reals^n$, \(p\)-norm
\[
d(x,y) = \|x - y\|_p
\]
**** Scaled norms
When features having varying scales:
\[
d(x,y) = \|S x - S y\|_p
\]
Or pre-scale the data

**** Complex data
- Manifold distances
- Graph distance

*** Distances 
**** A distance $d(\cdot, \cdot)$:
- Identity $d(x,x) = 0$.
- Positivity $d(x,y) > 0$ if $x \neq y$.
- Symmetry $d(y,x) = d(x,y)$.
- Triangle inequality $d(x,y) \leq d(x,z) + d(z,y)$.
**** For data in $\Reals^n$, \(p\)-norm
\[
d(x,y) = \|x - y\|_p
\]
*** Norms;
**** A norm $\|\cdot\|$
- Zero element $\|0\| = 0$.
- Homogeneity $\|cx\| = c \|x\|$ for any scalar $a$.
- Triangle inequality $\|x + y\| \leq \|x\| + \|y\|$.
**** $p$-norm
\[
\|z\|_p = \left(\sum_i z_i^p\right)^{1/p}
\]
*** Neighbourhood calculation
If we have $T$ datapoints
**** Sort and top $K$.
- Requires $O(T \ln T)$ time
**** Use the Cover-Tree or KD-Tree algorithm
- Requires $O(c K \ln T)$ time.
- $c$ depends on the data distribution.


* Activities


*** KNN activity
- Implement nearest neighbours
[[./src/KNearestNeighbours/NearestNeighbourClassifier.py]]
- Introduction to scikitlearn nearest neighbours
[[./src/KNearestNeighbours/KNN-scikit.ipynb]]
- Introduction to generalisation errors
